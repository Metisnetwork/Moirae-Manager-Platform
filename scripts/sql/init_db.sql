use db_moirae;
-- ----------------------------
-- init t_project_temp
-- ----------------------------
TRUNCATE t_project_temp;
INSERT INTO `t_project_temp` VALUES (1, '空白', 'Blank', '空白模板', 'Blank template project', 1, now(), now());
INSERT INTO `t_project_temp` VALUES (2, '协同营销', 'Collaborative Marketing', '协同营销模板', 'Collaborative marketing template', 1, now(), now());
INSERT INTO `t_project_temp` VALUES (3, '线性训练', 'Linear training', '线性训练模板', 'Linear training template', 0, now(), now());
INSERT INTO `t_project_temp` VALUES (4, 'DNN算法', 'DNN algorithm', 'DNN算法模板', 'DNN algorithm template', 0, now(), now());
INSERT INTO `t_project_temp` VALUES (5, 'XGBoost算法', 'XGBoost algorithm', 'XGBoost算法模板', 'XGBoost algorithm template', 0, now(), now());

-- ----------------------------
-- init t_workflow_temp
-- ----------------------------
TRUNCATE t_workflow_temp;
INSERT INTO `t_workflow_temp` VALUES (1, 2, '协同营销对应工作流', 'Collaborative Marketing workflow', '协同营销对应工作流', 'Collaborative Marketing workflow', 1, 0, 1, now(), now());
INSERT INTO `t_workflow_temp` VALUES (2, 3, '线性训练对应工作流', null, '线性训练对应工作流', null, 1, 0, 1, now(), now());
INSERT INTO `t_workflow_temp` VALUES (3, 4, 'DNN算法对应工作流', null, 'DNN算法对应工作流', null, 1, 0, 1, now(), now());
INSERT INTO `t_workflow_temp` VALUES (4, 5, 'XGBoost算法对应工作流', null, 'XGBoost算法对应工作流', null, 1, 0, 1, now(), now());

-- ----------------------------
-- init t_workflow_node_temp
-- ----------------------------
TRUNCATE t_workflow_node_temp;
INSERT INTO `t_workflow_node_temp` VALUES (1, 1, 1, '逻辑回归训练',  'Logistic Regression Training', '1', '2', '0', '1', '2021-12-01 03:44:28', '2021-12-01 03:44:28');
INSERT INTO `t_workflow_node_temp` VALUES (2, 1, 2, '逻辑回归预测', 'Logistic Regression Prediction', '2', null, '0', '1', '2021-12-01 03:44:28', '2021-12-01 03:44:28');
INSERT INTO `t_workflow_node_temp` VALUES (3, 2, 6, '线性回归训练', 'Linear Regression Training', '1', '2', '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:19');
INSERT INTO `t_workflow_node_temp` VALUES (4, 2, 7, '线性回归预测', 'Linear Regression Prediction', '2', null, '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:27');
INSERT INTO `t_workflow_node_temp` VALUES (5, 3, 8, 'DNN训练', 'DNN Training', '1', '2', '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:30');
INSERT INTO `t_workflow_node_temp` VALUES (6, 3, 9, 'DNN预测', 'DNN Prediction', '2', null, '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:34');
INSERT INTO `t_workflow_node_temp` VALUES (7, 4, 10, 'XGBoost训练', 'XGBoost Training', '1', '2', '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:37');
INSERT INTO `t_workflow_node_temp` VALUES (8, 4, 11, 'XGBoost预测', 'XGBoost Prediction', '2', null, '0', '1', '2021-12-01 03:44:29', '2021-12-01 08:20:40');

-- ----------------------------
-- init t_algorithm
-- ----------------------------
TRUNCATE t_algorithm;
INSERT INTO `t_algorithm` VALUES (1, '逻辑回归训练',  'Logistic Regression Training', '用于跨组织逻辑回归训练', 'Used for cross-organization logistic regression training', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '1', '2021-10-25 15:16:02', '2021-11-12 10:03:52');
INSERT INTO `t_algorithm` VALUES (2, '逻辑回归预测', 'Logistic Regression Prediction', '用于跨组织逻辑回归预测', 'Used for cross-organization logistic regression prediction', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '1', '1', '1', '0', '1', '1', '2021-10-25 15:16:02', '2021-11-12 10:03:46');
INSERT INTO `t_algorithm` VALUES (3, '隐私集合求交（PSI）', 'Privacy Set Intersection (PSI)', '用于跨组织的数据交集查询', 'Used for cross-organization data intersection query', 'Rosetta', '3', '2', 'SQL,Python', 'window,linux,mac', '1', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '0', '2021-10-25 16:47:14', '2021-11-12 10:02:38');
INSERT INTO `t_algorithm` VALUES (4, '隐私求和', 'Privacy Summation', '用于多方参与的隐私数据求和', 'Summation of private data for multi-party participation', 'Rosetta', '3', '2', 'SQL,Python', 'window,linux,mac', '1', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '0', '2021-10-25 17:01:40', '2021-11-12 10:02:59');
INSERT INTO `t_algorithm` VALUES (5, '缺失值处理', 'Missing value processing', '用于缺失值处理', 'Used for missing value processing', 'Rosetta', '3', '2', 'SQL,Python', 'window,linux,mac', '2', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '0', '2021-10-25 17:03:34', '2021-11-12 10:03:05');
INSERT INTO `t_algorithm` VALUES (6, '线性回归训练', 'Linear Regression Training', '用于跨组织线性回归训练', 'Used for cross-organization linear regression training', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '1', '2021-11-17 15:00:00', '2021-11-17 13:00:00');
INSERT INTO `t_algorithm` VALUES (7, '线性回归预测', 'Linear Regression Prediction', '用于跨组织线性回归的预测', 'Used for cross-organization linear regression prediction', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '1', '1', '1', '0', '1', '1', '2021-11-17 15:00:00', '2021-11-17 13:00:00');
INSERT INTO `t_algorithm` VALUES (8, 'DNN训练', 'DNN Training', '用于跨组织DNN训练', 'Used for cross-organization DNN training', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '300000', '0', '1', '1', '0', '1', '1', '2021-11-22 11:00:40', '2021-11-22 11:00:44');
INSERT INTO `t_algorithm` VALUES (9, 'DNN预测', 'DNN Prediction', '用于跨组织DNN预测算法', 'Used for cross-organization DNN prediction', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '300000', '1', '1', '1', '0', '1', '1', '2021-11-22 11:05:49', '2021-11-22 11:05:52');
INSERT INTO `t_algorithm` VALUES (10, 'XGBoost训练', 'XGBoost Training', '用于跨组织XGBoost训练', 'Used for cross-organization XGBoost training', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '0', '1', '1', '0', '1', '1', '2021-11-26 14:50:39', '2021-11-26 15:15:36');
INSERT INTO `t_algorithm` VALUES (11, 'XGBoost预测', 'XGBoost Prediction', '用于跨组织XGBoost预测', 'Used for cross-organization XGBoost prediction', 'Rosetta', '3', '2', 'Python', 'window,linux,mac', '3', '1073741824', '1', '2', '3145728', '180000', '1', '1', '1', '0', '1', '1', '2021-11-26 14:51:58', '2021-11-26 14:52:59');

-- ----------------------------
-- init t_algorithm_code
-- ----------------------------
INSERT INTO `t_algorithm_code` VALUES (1, 1, 2, '# coding:utf-8\n\nimport sys\nsys.path.append(\"..\")\nimport os\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLRTrain(object):\n    \'\'\'\n    Privacy logistic regression train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.001)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert self.epochs > 0, \"epochs must be greater 0\"\n        assert self.batch_size > 0, \"batch size must be greater 0\"\n        assert self.learning_rate > 0, \"learning rate must be greater 0\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        Logistic regression training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num])\n            Y = tf.placeholder(tf.float64, [None, 1])\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n            logits = tf.matmul(X, W) + b\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)\n            loss = tf.reduce_mean(loss)\n            # optimizer\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            \n            pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\n            reveal_Y = rtt.SecureReveal(pred_Y)\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                \n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n            Y_pred_prob = Y_pred.astype(\"float\").reshape([-1, ])\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n            auc_score = roc_auc_score(Y_true, Y_pred_prob)\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            Y_pred_class = (Y_pred_prob > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n            accuracy = accuracy_score(Y_true, Y_pred_class)\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            f1_score = f1_score(Y_true, Y_pred_class)\n            precision = precision_score(Y_true, Y_pred_class)\n            recall = recall_score(Y_true, Y_pred_class)\n            log.info(\"********************\")\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n            log.info(f\"PRECISION: {round(precision, 6)}\")\n            log.info(f\"RECALL: {round(recall, 6)}\")\n            log.info(\"********************\")\n        log.info(\"train finish.\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    y_data.iloc[:split_point].to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        y_data.iloc[split_point:].to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = os.path.dirname(self.output_file)\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_lr = PrivacyLRTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_lr.train()\n', NULL, 1, '2021-10-12 15:56:27', '2021-11-01 11:36:42');
INSERT INTO `t_algorithm_code` VALUES (2, 2, 2, '# coding:utf-8\n\nimport sys\nsys.path.append(\"..\")\nimport os\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLRPredict(object):\n    \'\'\'\n    Privacy logistic regression predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        Logistic regression predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        X = tf.placeholder(tf.float64, [None, column_total_num])\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n        init = tf.global_variables_initializer()\n        # predict\n        pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n        with tf.Session() as sess:\n            log.info(\"session init.\")\n            sess.run(init)\n            log.info(\"start restore model.\")\n            if self.party_id == self.model_restore_party:\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                    log.info(f\"model restore from: {self.model_file}.\")\n                    saver.restore(sess, self.model_file)\n                else:\n                    raise Exception(\"model not found or model damaged\")\n            else:\n                log.info(\"restore model...\")\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                with open(temp_file, \"w\") as f:\n                    pass\n                saver.restore(sess, temp_file)\n            log.info(\"finish restore model.\")\n            \n            # predict\n            log.info(\"predict start.\")\n            predict_start_time = time.time()\n            Y_pred_prob = sess.run(reveal_Y, feed_dict={X: shard_x})\n            log.debug(f\"Y_pred_prob:\\n {Y_pred_prob[:10]}\")\n            predict_use_time = round(time.time() - predict_start_time, 3)\n            log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred_prob = Y_pred_prob.astype(\"float\")\n            Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\n            Y_class = (Y_pred_prob > self.predict_threshold) * 1\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_lr = PrivacyLRPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_lr.predict()\n', NULL, 1, '2021-10-12 15:56:27', '2021-10-26 10:26:51');
INSERT INTO `t_algorithm_code` VALUES (3, 3, 2, '', NULL, 1, '2021-10-25 16:49:33', '2021-10-26 10:23:52');
INSERT INTO `t_algorithm_code` VALUES (4, 4, 2, '', NULL, 1, '2021-10-25 17:04:39', '2021-10-26 10:23:52');
INSERT INTO `t_algorithm_code` VALUES (5, 5, 2, '', NULL, 1, '2021-10-25 17:04:57', '2021-10-26 10:23:52');
INSERT INTO `t_algorithm_code` VALUES (6, 6, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLinearRegTrain(object):\n    \'\'\'\n    Privacy linear regression train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.001)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert self.epochs > 0, \"epochs must be greater 0\"\n        assert self.batch_size > 0, \"batch size must be greater 0\"\n        assert self.learning_rate > 0, \"learning rate must be greater 0\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        Linear regression training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num])\n            Y = tf.placeholder(tf.float64, [None, 1])\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n            pred_Y = tf.matmul(X, W) + b\n            loss = tf.square(Y - pred_Y)\n            loss = tf.reduce_mean(loss)\n            # optimizer\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            \n            reveal_Y = rtt.SecureReveal(pred_Y)\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                \n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            from sklearn.metrics import r2_score, mean_squared_error\n            Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n            r2 = r2_score(Y_true, Y_pred)\n            rmse = np.sqrt(mean_squared_error(Y_true, Y_pred))\n            log.info(\"********************\")\n            log.info(f\"R Squared: {round(r2, 6)}\")\n            log.info(f\"RMSE: {round(rmse, 6)}\")\n            log.info(\"********************\")\n        log.info(\"train finish.\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = os.path.dirname(self.output_file)\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_linear_reg = PrivacyLinearRegTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_linear_reg.train()', NULL, 1, '2021-11-17 13:52:26', '2021-11-17 13:52:26');
INSERT INTO `t_algorithm_code` VALUES (7, 7, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLinearRegPredict(object):\n    \'\'\'\n    Privacy linear regression predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        Linear regression predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        X = tf.placeholder(tf.float64, [None, column_total_num])\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n        init = tf.global_variables_initializer()\n        # predict\n        pred_Y = tf.matmul(X, W) + b\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n        with tf.Session() as sess:\n            log.info(\"session init.\")\n            sess.run(init)\n            log.info(\"start restore model.\")\n            if self.party_id == self.model_restore_party:\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                    log.info(f\"model restore from: {self.model_file}.\")\n                    saver.restore(sess, self.model_file)\n                else:\n                    raise Exception(\"model not found or model damaged\")\n            else:\n                log.info(\"restore model...\")\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                with open(temp_file, \"w\") as f:\n                    pass\n                saver.restore(sess, temp_file)\n            log.info(\"finish restore model.\")\n            \n            # predict\n            log.info(\"predict start.\")\n            predict_start_time = time.time()\n            Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\n            log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\n            predict_use_time = round(time.time() - predict_start_time, 3)\n            log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred = Y_pred.astype(\"float\")\n            Y_result = pd.DataFrame(Y_pred, columns=[\"result\"])\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_linear_reg = PrivacyLinearRegPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_linear_reg.predict()\n', NULL, 1, '2021-11-17 13:53:06', '2021-11-17 13:53:06');
INSERT INTO `t_algorithm_code` VALUES (8, 8, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport copy\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyDnnTrain(object):\n    \'\'\'\n    Privacy DNN train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"label_owner\": \"p1\",\n                \"label_column\": \"Y\",\n                \"algorithm_parameter\": {\n                    \"epochs\": 50,\n                    \"batch_size\": 256,\n                    \"learning_rate\": 0.1,\n                    \"layer_units\": [32, 128, 32, 1],\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"],\n                    \"init_method\": \"random_normal\",\n                    \"use_intercept\": true,\n                    \"optimizer\": \"sgd\",\n                    \"use_validation_set\": True,\n                    \"validation_set_rate\": 0.2,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.results_dir = results_dir\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 50)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.1)\n        self.layer_units = algorithm_parameter.get(\"layer_units\", [32, 128, 32, 1])\n        self.layer_activation = algorithm_parameter.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"])\n        self.init_method = algorithm_parameter.get(\"init_method\", \"random_normal\")  # \'random_normal\', \'random_uniform\', \'zeros\', \'ones\'\n        self.use_intercept = algorithm_parameter.get(\"use_intercept\", True)  # True: use b, False: not use b\n        self.optimizer = algorithm_parameter.get(\"optimizer\", \"sgd\")\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.output_file = os.path.join(self.results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\n        for i in self.layer_units:\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\n        for i in self.layer_activation:\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\n        if self.layer_activation[-1] == \'sigmoid\':\n            if self.layer_units[-1] != 1:\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\n        \n        assert isinstance(self.init_method, str), \"init_method must be type(str)\"\n        if self.init_method == \'random_normal\':\n            self.init_method = tf.random_normal\n        elif self.init_method == \'random_uniform\':\n            self.init_method = tf.random_uniform\n        elif self.init_method == \'zeros\':  # if len(self.layer_units) != 1, init_method not use zeros, because it can not work well.\n            self.init_method = tf.zeros\n        elif self.init_method == \'ones\':\n            self.init_method = tf.ones\n        else:\n            raise Exception(f\"init_method only can be random_normal/random_uniform/zeros/ones], not {self.init_method}\")\n        assert isinstance(self.optimizer, str), \"optimizer must be type(str)\"\n        if self.optimizer == \'sgd\':\n            self.optimizer = tf.train.GradientDescentOptimizer\n        else:\n            raise Exception(f\"optimizer only can be sgd, not {self.optimizer}\")\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"\n        assert isinstance(self.use_validation_set, bool), \"use_validation_set must be type(bool), true or false\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, \n                                              label_owner=self.label_owner)\\\n                                .load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, \n                                                          label_owner=self.label_owner)\\\n                                            .load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\n            Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'Y\')\n            val_Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'val_Y\')\n                        \n            output = self.dnn(X, column_total_num)\n            \n            output_layer_activation = self.layer_activation[-1]\n            with tf.name_scope(\'output\'):\n                if not output_layer_activation:\n                    pred_Y = output\n                elif output_layer_activation == \'sigmoid\':\n                    pred_Y = tf.sigmoid(output)\n                elif output_layer_activation == \'relu\':\n                    pred_Y = tf.nn.relu(output)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            with tf.name_scope(\'loss\'):\n                if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n                    loss = tf.square(Y - pred_Y)\n                    loss = tf.reduce_mean(loss)\n                elif output_layer_activation == \'sigmoid\':\n                    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\n                    loss = tf.reduce_mean(loss)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            \n            # optimizer\n            with tf.name_scope(\'optimizer\'):\n                optimizer = self.optimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'saver\')\n                        \n            reveal_loss = rtt.SecureReveal(loss) # only reveal to the result party\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to the result party\n            reveal_val_Y = rtt.SecureReveal(val_Y) # only reveal to the result party\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                summary_writer = tf.summary.FileWriter(self.get_temp_dir(), sess.graph)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                loss_history_train, loss_history_val = [], []\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                    train_loss = sess.run(reveal_loss, feed_dict={X: shard_x, Y: shard_y})\n                    # collect loss\n                    loss_history_train.append(float(train_loss))\n                    if self.use_validation_set:\n                        val_loss = sess.run(reveal_loss, feed_dict={X: shard_x_val, Y: shard_y_val})\n                        loss_history_val.append(float(val_loss))\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_val_Y, feed_dict={val_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if self.party_id in self.result_party:\n            log.info(f\"result_party evaluation the model.\")\n            if self.use_validation_set:\n                log.info(\"result_party evaluate model.\")\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n                self.model_evaluation(Y_true, Y_pred, output_layer_activation)\n            # self.show_train_history(loss_history_train, loss_history_val, self.epochs)\n            # log.info(f\"result_party show train history finish.\")\n        log.info(\"train finish.\")\n    \n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\n        with tf.name_scope(layer_name):\n            W = tf.Variable(self.init_method([input_dim, output_dim], dtype=tf.float64), name=\'W\')\n            if self.use_intercept:\n                b = tf.Variable(self.init_method([output_dim], dtype=tf.float64), name=\'b\')\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W) + b\n            else:\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W)\n            if not activation:\n                one_layer = logits\n            elif activation == \'sigmoid\':\n                one_layer = tf.sigmoid(logits)\n            elif activation == \'relu\':\n                one_layer = tf.nn.relu(logits)\n            else:\n                raise Exception(f\'not support {activation} activation.\')\n            return one_layer\n    \n    def dnn(self, input_X, input_dim):\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\n        layer_activation.append(\"\")\n        for i in range(len(self.layer_units)):\n            if i == 0:\n                input_units = input_dim\n                previous_output = input_X\n            else:\n                input_units = self.layer_units[i-1]\n                previous_output = output\n            output = self.layer(previous_output, \n                                input_units, \n                                self.layer_units[i], \n                                layer_activation[i], \n                                layer_name=f\"Dense_{i}\")\n        return output\n    \n    def model_evaluation(self, Y_true, Y_pred, output_layer_activation):\n        if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n            r2 = r2_score(Y_true, Y_pred)\n            rmse = mean_squared_error(Y_true, Y_pred, squared=False)\n            mse = mean_squared_error(Y_true, Y_pred, squared=True)\n            mae = mean_absolute_error(Y_true, Y_pred)\n            log.info(\"********************\")\n            log.info(f\"R Squared: {round(r2, 6)}\")\n            log.info(f\"RMSE: {round(rmse, 6)}\")\n            log.info(f\"MSE: {round(mse, 6)}\")\n            log.info(f\"MAE: {round(mae, 6)}\")\n            log.info(\"********************\")\n        elif output_layer_activation == \'sigmoid\':\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n            auc_score = roc_auc_score(Y_true, Y_pred)\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n            accuracy = accuracy_score(Y_true, Y_pred_class)\n            f1_score = f1_score(Y_true, Y_pred_class)\n            precision = precision_score(Y_true, Y_pred_class)\n            recall = recall_score(Y_true, Y_pred_class)\n            log.info(\"********************\")\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n            log.info(f\"PRECISION: {round(precision, 6)}\")\n            log.info(f\"RECALL: {round(recall, 6)}\")\n            log.info(\"********************\")\n    \n    def show_train_history(self, train_history, val_history, epochs, name=\'loss\'):\n        log.info(\"start show_train_history\")\n        assert all([isinstance(ele, float) for ele in train_history]), \'element of train_history must be float.\'\n        import matplotlib.pyplot as plt\n        plt.figure()\n        y_min = min(train_history)\n        y_max = max(train_history)\n        y_ticks = np.linspace(y_min, y_max, 10)\n        plt.scatter(list(range(1, epochs+1)), train_history, label=\'train\')\n        if self.use_validation_set:\n            plt.scatter(list(range(1, epochs+1)), val_history, label=\'val\')\n        plt.xlabel(\'epochs\') \n        plt.ylabel(name)\n        plt.yticks(y_ticks)\n        plt.title(f\'{name} with epochs\')\n        plt.legend()\n        figure_path = os.path.join(self.results_dir, f\'{name}.jpg\')\n        plt.savefig(figure_path)\n        \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_class_num = train_y_data.unique().shape[0]\n                    assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_class_num = val_y_data.unique().shape[0]\n                        assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        path = self.results_dir\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_dnn = PrivacyDnnTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_dnn.train()\n', NULL, 1, '2021-11-22 03:01:38', '2021-11-22 10:19:18');
INSERT INTO `t_algorithm_code` VALUES (9, 9, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport copy\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyDnnPredict(object):\n    \'\'\'\n    Privacy Dnn predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"model_restore_party\": \"p3\",\n                \"model_path\": \"/absoulte_path/to/model_dir\",\n                \"algorithm_parameter\": {\n                    \"layer_units\": [32, 128, 32, 1],\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"],\n                    \"use_intercept\": true,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.layer_units = algorithm_parameter.get(\"layer_units\", [32, 128, 32, 1])\n        self.layer_activation = algorithm_parameter.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"])\n        self.use_intercept = algorithm_parameter.get(\"use_intercept\", True)  # True: use b, False: not use b        \n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.results_dir = results_dir\n        self.output_file = os.path.join(self.results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\n        for i in self.layer_units:\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\n        for i in self.layer_activation:\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\n        if self.layer_activation[-1] == \'sigmoid\':\n            if self.layer_units[-1] != 1:\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"     \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\n            output = self.dnn(X, column_total_num)\n            output_layer_activation = self.layer_activation[-1]\n            with tf.name_scope(\'output\'):\n                if not output_layer_activation:\n                    pred_Y = output\n                elif output_layer_activation == \'sigmoid\':\n                    pred_Y = tf.sigmoid(output)\n                elif output_layer_activation == \'relu\':\n                    pred_Y = tf.nn.relu(output)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            init = tf.global_variables_initializer()\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                log.info(\"start restore model.\")\n                if self.party_id == self.model_restore_party:\n                    if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                        log.info(f\"model restore from: {self.model_file}.\")\n                        saver.restore(sess, self.model_file)\n                    else:\n                        raise Exception(\"model not found or model damaged\")\n                else:\n                    log.info(\"restore model...\")\n                    temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                    with open(temp_file, \"w\") as f:\n                        pass\n                    saver.restore(sess, temp_file)\n                log.info(\"finish restore model.\")\n                \n                # predict\n                log.info(\"predict start.\")\n                predict_start_time = time.time()\n                Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\n                log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\n                predict_use_time = round(time.time() - predict_start_time, 3)\n                log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred = Y_pred.astype(\"float\")\n            if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n                Y_result = pd.DataFrame(Y_pred, columns=[\"Y_pred\"])\n            elif output_layer_activation == \'sigmoid\':\n                Y_prob = pd.DataFrame(Y_pred, columns=[\"Y_prob\"])\n                Y_class = (Y_pred > self.predict_threshold) * 1\n                Y_class = pd.DataFrame(Y_class, columns=[f\"Y_class(>{self.predict_threshold})\"])\n                Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\n        with tf.name_scope(layer_name):\n            W = tf.Variable(tf.random_normal([input_dim, output_dim], dtype=tf.float64), name=\'W\')\n            if self.use_intercept:\n                b = tf.Variable(tf.random_normal([output_dim], dtype=tf.float64), name=\'b\')\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W) + b\n            else:\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W)\n            if not activation:\n                one_layer = logits\n            elif activation == \'sigmoid\':\n                one_layer = tf.sigmoid(logits)\n            elif activation == \'relu\':\n                one_layer = tf.nn.relu(logits)\n            else:\n                raise Exception(f\'not support {activation} activation.\')\n            return one_layer\n    \n    def dnn(self, input_X, input_dim):\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\n        layer_activation.append(\"\")\n        for i in range(len(self.layer_units)):\n            if i == 0:\n                input_units = input_dim\n                previous_output = input_X\n            else:\n                input_units = self.layer_units[i-1]\n                previous_output = output\n            output = self.layer(previous_output, \n                                input_units, \n                                self.layer_units[i], \n                                layer_activation[i], \n                                layer_name=f\"Dense_{i}\")\n        return output\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_dnn = PrivacyDnnPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_dnn.predict()\n', NULL, 1, '2021-11-22 03:06:38', '2021-11-22 03:06:57');
INSERT INTO `t_algorithm_code` VALUES (10, 10, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyXgbTrain(object):\n    \'\'\'\n    Privacy XGBoost train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"label_owner\": \"p1\",\n                \"label_column\": \"Y\",\n                \"algorithm_parameter\": {\n                    \"epochs\": 10,\n                    \"batch_size\": 256,\n                    \"learning_rate\": 0.01,\n                    \"num_trees\": 3,   # num of trees\n                    \"max_depth\": 4,   # max depth of per tree\n                    \"num_bins\": 5,    # num of bins of feature\n                    \"num_class\": 2,   # num of class of label\n                    \"lambd\": 1.0,     # L2 regular coefficient, [0, +∞)\n                    \"gamma\": 0.0,     # Gamma, also known as \"complexity control\", is an important parameter we use to prevent over fitting\n                    \"use_validation_set\": True,\n                    \"validation_set_rate\": 0.2,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.results_dir = results_dir\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.1)\n        self.num_trees = algorithm_parameter.get(\"num_trees\", 3)\n        self.max_depth = algorithm_parameter.get(\"max_depth\", 4)\n        self.num_bins = algorithm_parameter.get(\"num_bins\", 5)\n        self.num_class = algorithm_parameter.get(\"num_class\", 2)\n        self.lambd = algorithm_parameter.get(\"lambd\", 1.0)\n        self.gamma = algorithm_parameter.get(\"gamma\", 0.0)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(self.results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"       \n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx\\\n            = rtt.PrivateDatasetEx(data_owner=self.data_party, \n                                    label_owner=self.label_owner,\n                                    dataset_type=rtt.DatasetType.SampleAligned,\n                                    num_classes=self.num_class)\\\n                    .load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val\\\n                = rtt.PrivateDataset(data_owner=self.data_party,\n                                     label_owner=self.label_owner,\n                                    dataset_type=rtt.DatasetType.SampleAligned,\n                                    num_classes=self.num_class)\\\n                    .load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:\n            log.info(\"start build SecureXGBClassifier.\")\n            xgb = rtt.SecureXGBClassifier(epochs=self.epochs,\n                                        batch_size=self.batch_size,\n                                        learning_rate=self.learning_rate,\n                                        max_depth=self.max_depth,\n                                        num_trees=self.num_trees,\n                                        num_class=self.num_class,\n                                        num_bins=self.num_bins,\n                                        lambd=self.lambd,\n                                        gamma=self.gamma)\n            log.info(\"start train XGBoost.\")\n            xgb.FitEx(shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx)\n            log.info(\"start save model.\")\n            xgb.SaveModel(self.output_file)\n            log.info(\"save model success.\")    \n            if self.use_validation_set:\n                # predict Y\n                rv_pred = xgb.Reveal(xgb.Predict(shard_x_val), self.result_party)\n                y_shape = rv_pred.shape\n                log.info(f\"y_shape: {y_shape}, rv_pred: \\n {rv_pred[:10]}\")\n                pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\n                log.info(f\"pred_y: \\n {pred_y[:10]}\")\n                pred_y = np.array(pred_y)\n                pred_y.reshape(y_shape)\n                Y_pred = np.squeeze(pred_y, 1)\n                log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                # actual Y\n                Y_actual = xgb.Reveal(shard_y_val, self.result_party)\n                log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate success.\")\n             \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            Y_pred = np.squeeze(Y_pred.astype(\"float\"))\n            Y_true = np.squeeze(Y_actual.astype(\"float\"))\n            self.model_evaluation(Y_true, Y_pred)\n        \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        log.info(\"train success.\")\n    \n    def model_evaluation(self, Y_true, Y_pred):\n        from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n        if self.num_class == 2:\n            average = \'binary\'\n            multi_class = \'raise\'\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n        else:\n            average = \'weighted\'\n            multi_class = \'ovr\'\n            Y_pred_class = np.argmax(Y_pred, axis=1)\n        auc_score = roc_auc_score(Y_true, Y_pred, multi_class=multi_class)\n        accuracy = accuracy_score(Y_true, Y_pred_class)\n        f1_score = f1_score(Y_true, Y_pred_class, average=average)\n        precision = precision_score(Y_true, Y_pred_class, average=average)\n        recall = recall_score(Y_true, Y_pred_class, average=average)\n        log.info(\"********************\")\n        log.info(f\"AUC: {round(auc_score, 6)}\")\n        log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n        log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n        log.info(f\"PRECISION: {round(precision, 6)}\")\n        log.info(f\"RECALL: {round(recall, 6)}\")\n        log.info(\"********************\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = self.results_dir\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_xgb = PrivacyXgbTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_xgb.train()\n', NULL, 1, '2021-11-26 14:56:41', '2021-11-26 16:54:34');
INSERT INTO `t_algorithm_code` VALUES (11, 11, 2, '# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyXgbPredict(object):\n    \'\'\'\n    Privacy XGBoost predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"model_restore_party\": \"p3\",\n                \"model_path\": \"/absoulte_path/to/model_dir\",\n                \"algorithm_parameter\": {\n                    \"num_trees\": 3,\n                    \"max_depth\": 4,\n                    \"num_bins\": 5,\n                    \"num_class\": 2,\n                    \"lambd\": 1.0,\n                    \"gamma\": 0.0,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.num_trees = algorithm_parameter.get(\"num_trees\", 3)\n        self.max_depth = algorithm_parameter.get(\"max_depth\", 4)\n        self.num_bins = algorithm_parameter.get(\"num_bins\", 5)\n        self.num_class = algorithm_parameter.get(\"num_class\", 2)\n        self.lambd = algorithm_parameter.get(\"lambd\", 1.0)\n        self.gamma = algorithm_parameter.get(\"gamma\", 0.0)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.model_file = os.path.join(self.model_path, \"model\")        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")      \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\" \n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party,\n                                     dataset_type=rtt.DatasetType.SampleAligned,\n                                     num_classes=self.num_class)\\\n                        .load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n\n        xgb = rtt.SecureXGBClassifier(max_depth=self.max_depth, \n                                      num_trees=self.num_trees, \n                                      num_class=self.num_class, \n                                      num_bins=self.num_bins,\n                                      lambd=self.lambd,\n                                      gamma=self.gamma,\n                                      epochs=10,\n                                      batch_size=256,\n                                      learning_rate=0.01)\n        \n        log.info(\"start restore model.\")\n        if self.party_id == self.model_restore_party:\n            log.info(f\"model restore from: {self.model_file}.\")\n            xgb.LoadModel(self.model_file)\n        else:\n            log.info(\"restore model...\")\n            xgb.LoadModel(\"\")\n        log.info(\"finish restore model.\")\n                \n        # predict\n        predict_start_time = time.time()\n        rv_pred = xgb.Reveal(xgb.Predict(shard_x), self.result_party)\n        predict_use_time = round(time.time() - predict_start_time, 3)\n        log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        y_shape = rv_pred.shape\n        pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\n        pred_y = np.array(pred_y)\n        pred_y.reshape(y_shape)\n        pred_y = np.squeeze(pred_y, 1)\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred_prob = pred_y.astype(\"float\")\n            if self.num_class == 2:\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\n                Y_class = (Y_pred_prob > self.predict_threshold) * 1\n            else:\n                columns = [f\"Y_prob_{i}\" for i in range(Y_pred_prob.shape[1])]\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=columns)\n                Y_class = np.argmax(Y_prob, axis=1)\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_xgb = PrivacyXgbPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_xgb.predict()\n', NULL, 1, '2021-11-26 14:57:14', '2021-11-26 16:54:37');

-- ----------------------------
-- init t_algorithm_variable_struct
-- ----------------------------
TRUNCATE t_algorithm_variable_struct;
INSERT INTO `t_algorithm_variable_struct` VALUES (1, 1, '{\"label_owner\":\"p0\",\"label_column\":\"Y\",\"algorithm_parameter\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}', '逻辑回归训练变量参数', 1, '2021-10-26 14:05:13', '2021-10-26 17:08:32');
INSERT INTO `t_algorithm_variable_struct` VALUES (2, 2, '{\"model_restore_party\":\"p0\",\"model_path\":\"file_path\",\"predict_threshold\":0.5}', '逻辑回归预测变量参数', 1, '2021-10-26 14:05:13', '2021-10-26 14:05:13');
INSERT INTO `t_algorithm_variable_struct` VALUES (3, 6, '{\"label_owner\":\"p0\",\"label_column\":\"Y\",\"algorithm_parameter\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}', '线性回归训练变量参数', 1, '2021-11-18 14:16:25', '2021-11-18 14:19:34');
INSERT INTO `t_algorithm_variable_struct` VALUES (4, 7, '{\"model_restore_party\":\"p0\",\"model_path\":\"file_path\",\"predict_threshold\":0.5}', '线性回归预测变量参数', 1, '2021-11-18 14:16:38', '2021-11-18 14:20:12');
INSERT INTO `t_algorithm_variable_struct` VALUES (5, 8, '{\"label_owner\":\"p0\",\"label_column\":\"Y\",\"algorithm_parameter\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}', 'DNN训练变量参数', 1, '2021-11-22 03:04:35', '2021-11-22 03:04:44');
INSERT INTO `t_algorithm_variable_struct` VALUES (6, 9, '{"model_restore_party":"p0","model_path":"file_path","algorithm_parameter":{"layer_units":[32,128,32,1],"layer_activation":["sigmoid","sigmoid","sigmoid","sigmoid"],"use_intercept":true,"predict_threshold":0.5}}', 'DNN预测变量参数', 1, '2021-11-22 03:06:10', '2021-11-22 03:06:26');
INSERT INTO `t_algorithm_variable_struct` VALUES (7, 10, '{\"label_owner\":\"p0\",\"label_column\":\"Y\",\"algorithm_parameter\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.01,\"num_trees\":3,\"max_depth\":4,\"num_bins\":5,\"num_class\":2,\"lambd\":1.0,\"gamma\":0.0,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}', 'XGBoost训练变量参数', 1, '2021-11-26 14:59:36', '2021-11-26 15:27:32');
INSERT INTO `t_algorithm_variable_struct` VALUES (8, 11, '{\"model_restore_party\":\"p0\",\"model_path\":\"file_path\",\"algorithm_parameter\":{\"num_trees\":3,\"max_depth\":4,\"num_bins\":5,\"num_class\":2,\"lambd\":1.0,\"gamma\":0.0,\"predict_threshold\":0.5}}', 'XGBoost预测变量参数', 1, '2021-11-26 15:01:32', '2021-11-26 15:01:46');

-- ----------------------------
-- init t_algorithm_type
-- ----------------------------
TRUNCATE t_algorithm_type;
INSERT INTO `t_algorithm_type` VALUES (1, '统计分析（开发中）', 'Statistics(developing)', '统计分析', 'Statistical Analysis', 1, 1, now(), now());
INSERT INTO `t_algorithm_type` VALUES (2, '特征工程（开发中）', 'Feature Engineering(developing)', '特征工程', 'Feature engineering', 2, 1, now(), now());
INSERT INTO `t_algorithm_type` VALUES (3, '机器学习', 'Machine Learning', '机器学习', 'Machine learning', 3, 1, now(), now());

-- ----------------------------
-- init t_organization
-- ----------------------------
INSERT INTO `t_organization` VALUES ('1', 'companyA', 'd48404c82bd93a578d3ea7e805bd6f8cd5a05cf05203ba15c532e9f45eeb09b8d391950a45485ac36df9d566b21f2200ccf51571a65e5cef7148791fcdcd8e1f', 'identity_e46bdcbae03643d489ecacdcbb33225b', '39.98.124.39', '8801', '1', '1', '2021-12-21 17:18:12', '2021-12-21 17:19:14');
INSERT INTO `t_organization` VALUES ('2', 'companyB', '8d8f472b4cc53febd96c7c0e4a078fc93d19a61acb20cf224d53c0fdc8c4515ab3ac04a238d46e9810ab2aa0c6c734051404a5c5b9d10c9321736ddbbed17f9e', 'identity_75904f86cc1747a785abd680dd563878', '39.101.141.143', '8801', '1', '1', '2021-12-21 17:18:12', '2021-12-21 17:19:19');
INSERT INTO `t_organization` VALUES ('3', 'companyC', 'f1e41d70dbb79ae80ae555e8a3cfad663ae800010876eff92a449247f147f3024859b6ba2fbbf923d3a2228e971d9c71d236014704677cc36ea8617a0ae089c2', 'identity_e9a18a3a70ac402689b3ab666cd0de12', '39.101.171.59', '8801', '1', '1', '2021-12-21 17:18:12', '2021-12-21 17:19:24');
INSERT INTO `t_organization` VALUES ('4', 'companyD', 'f228f49dfea0476249e928c274f7779b026e97b6d17b3bd5cb7d89bfd73f7efcbb080456174464df4710216d70397357fe9bdd64be2cb392dfd810b4fb85fd44', 'identity_3a1158e5d28e4e7da142605554115ae7', '39.98.117.220', '8801', '1', '1', '2021-12-21 17:18:12', '2021-12-21 17:19:28');
INSERT INTO `t_organization` VALUES ('5', 'companyE', 'a1154e811f4d4e5feb875c1450901e7c977aece468510d3017e6835d80fdbb4773d86b9d0ed3e39f35b458d1a6e960f71f545c1d47013b84e238b20c5a8cc351', 'identity_1ddd067a254c4168819341f8b83c3786', '39.103.144.123', '8801', '1', '1', '2021-12-21 17:18:12', '2021-12-21 17:19:36');

-- ----------------------------
-- init t_data_sync
-- ----------------------------
INSERT INTO t_data_sync(data_type, latest_synced) VALUES('data_auth_req', '0');
INSERT INTO t_data_sync(data_type, latest_synced) VALUES('local_meta_data', '0');
INSERT INTO t_data_sync(data_type, latest_synced) VALUES('local_task', '0');