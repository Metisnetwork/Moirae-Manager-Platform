CREATE DATABASE IF NOT EXISTS `db_moirae` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

USE `db_moirae`;

/*Table structure for table `dc_meta_data` */

DROP TABLE IF EXISTS `dc_meta_data`;

CREATE TABLE `dc_meta_data` (
    `meta_data_id` varchar(200)  NOT NULL COMMENT '元数据ID,hash',
    `meta_data_type` tinyint NOT NULL DEFAULT '1' COMMENT '表示该元数据是 `普通数据` 还是 `模型数据` 的元数据 (0: 未定义; 1: 普通数据元数据; 2: 模型数据元数据)',
    `file_name` varchar(100)  NOT NULL COMMENT '文件名称',
    `identity_id` varchar(200)  NOT NULL COMMENT '组织身份ID',
    `file_type` int NOT NULL COMMENT '文件后缀/类型, 0:未知; 1:csv',
    `industry` varchar(100)  DEFAULT NULL COMMENT '行业名称',
    `published_at` datetime(3) NOT NULL COMMENT '发布时间，精确到毫秒',
    `remarks` varchar(100)  DEFAULT NULL COMMENT '数据描述',
    `status` int DEFAULT NULL COMMENT '元数据的状态 (0: 未知; 1: 还未发布的新表; 2: 已发布的表; 3: 已撤销的表)',
    `token_address` varchar(100)  DEFAULT NULL COMMENT '对应合约的地址',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `location_type` tinyint NOT NULL DEFAULT '1' COMMENT '源数据的存储位置类型 (组织本地服务器、远端服务器、云等)：0-未知，1-存储在组织本地服务器上，2-存储在远端服务器上',
    `nonce` int DEFAULT '0' COMMENT '元数据的 nonce (用来标识该元数据在所属组织中的元数据的序号, 从 0 开始递增)',
    `allow_expose` tinyint(1) DEFAULT '0' COMMENT '是否可以被曝光 (1: 可以; 0: 不可以; 如 数据原始内容可以被下载或者支持外域查看时则为1, 默认为0)',
    `origin_id` varchar(200)  NOT NULL COMMENT '数据文件ID,hash',
    `file_path` varchar(100)  NOT NULL COMMENT '文件存储路径',
    `rows` int NOT NULL DEFAULT '0' COMMENT '数据行数(不算title)',
    `size` bigint NOT NULL DEFAULT '0' COMMENT '文件大小(字节)',
    `columns` int NOT NULL DEFAULT '0' COMMENT '数据列数',
    `has_title` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否带标题',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='数据文件信息';

/*Data for the table `dc_meta_data` */

/*Table structure for table `dc_meta_data_column` */

DROP TABLE IF EXISTS `dc_meta_data_column`;

CREATE TABLE `dc_meta_data_column` (
    `meta_data_id` varchar(200)  NOT NULL COMMENT '元数据ID,hash',
    `column_idx` int NOT NULL COMMENT '字段索引序号',
    `column_name` varchar(100)  NOT NULL COMMENT '字段名称',
    `column_type` varchar(100)  NOT NULL COMMENT '字段类型',
    `column_size` int NOT NULL DEFAULT '0' COMMENT '字段大小',
    `remarks` varchar(100)  DEFAULT NULL COMMENT '字段描述',
    PRIMARY KEY (`meta_data_id`,`column_idx`)
) ENGINE=InnoDB  COMMENT='数据文件元数据信息';

/*Data for the table `dc_meta_data_column` */

/*Table structure for table `dc_org` */

DROP TABLE IF EXISTS `dc_org`;

CREATE TABLE `dc_org` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `node_name` varchar(100)  DEFAULT NULL COMMENT '组织身份名称',
    `node_id` varchar(200)  NOT NULL COMMENT '组织节点ID',
    `image_url` varchar(256)  DEFAULT NULL COMMENT '组织机构图像url',
    `details` varchar(256)  DEFAULT NULL COMMENT '组织机构简介',
    `status` int NOT NULL DEFAULT '1' COMMENT '状态,1-Normal; 2-NonNormal',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='数据中心组织信息';

/*Data for the table `dc_org` */

/*Table structure for table `dc_power_server` */

DROP TABLE IF EXISTS `dc_power_server`;

CREATE TABLE `dc_power_server` (
    `id` varchar(200)  NOT NULL COMMENT '计算服务主机ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '组织身份ID',
    `memory` bigint NOT NULL DEFAULT '0' COMMENT '计算服务内存, 字节',
    `core` int NOT NULL DEFAULT '0' COMMENT '计算服务core',
    `bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '计算服务带宽, bps',
    `used_memory` bigint DEFAULT '0' COMMENT '使用的内存, 字节',
    `used_core` int DEFAULT '0' COMMENT '使用的core',
    `used_bandwidth` bigint DEFAULT '0' COMMENT '使用的带宽, bps',
    `published` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否发布，true/false',
    `published_at` datetime(3) NOT NULL COMMENT '发布时间，精确到毫秒',
    `status` int DEFAULT NULL COMMENT '算力的状态 (0: 未知; 1: 还未发布的算力; 2: 已发布的算力(算力未被占用); 3: 已发布的算力(算力正在被占用); 4: 已撤销的算力)',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='计算服务信息';

/*Data for the table `dc_power_server` */

/*Table structure for table `dc_task` */

DROP TABLE IF EXISTS `dc_task`;

CREATE TABLE `dc_task` (
    `id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `task_name` varchar(100)  NOT NULL COMMENT '任务名称',
    `user_id` varchar(200)  NOT NULL COMMENT '发起任务的用户的信息 (task是属于用户的)',
    `user_type` int NOT NULL COMMENT '用户类型 (0: 未定义; 1: 以太坊地址; 2: Alaya地址; 3: PlatON地址',
    `required_memory` bigint NOT NULL DEFAULT '0' COMMENT '需要的内存, 字节',
    `required_core` int NOT NULL DEFAULT '0' COMMENT '需要的core',
    `required_bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '需要的带宽, bps',
    `required_duration` bigint NOT NULL DEFAULT '0' COMMENT '需要的时间, milli seconds',
    `owner_identity_id` varchar(200)  NOT NULL COMMENT '任务创建者组织身份ID',
    `owner_party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `create_at` datetime(3) NOT NULL COMMENT '任务创建时间，精确到毫秒',
    `start_at` datetime(3) DEFAULT NULL COMMENT '任务开始执行时间，精确到毫秒',
    `end_at` datetime(3) DEFAULT NULL COMMENT '任务结束时间，精确到毫秒',
    `used_memory` bigint NOT NULL DEFAULT '0' COMMENT '使用的内存, 字节',
    `used_core` int NOT NULL DEFAULT '0' COMMENT '使用的core',
    `used_bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '使用的带宽, bps',
    `used_file_size` bigint DEFAULT '0' COMMENT '使用的所有数据大小，字节',
    `status` int DEFAULT NULL COMMENT '任务状态, 0:未知;1:等待中;2:计算中,3:失败;4:成功',
    `status_desc` varchar(255)  DEFAULT NULL COMMENT '任务状态说明',
    `remarks` varchar(255)  DEFAULT NULL COMMENT '任务描述',
    `task_sign` varchar(1024)  DEFAULT NULL COMMENT '任务签名',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    KEY `end_at` (`end_at`)
) ENGINE=InnoDB  COMMENT='任务';

/*Data for the table `dc_task` */

/*Table structure for table `dc_task_algo_provider` */

DROP TABLE IF EXISTS `dc_task_algo_provider`;

CREATE TABLE `dc_task_algo_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '算法提供者组织身份ID',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    PRIMARY KEY (`task_id`)
) ENGINE=InnoDB  COMMENT='任务算法提供者';

/*Data for the table `dc_task_algo_provider` */

/*Table structure for table `dc_task_data_provider` */

DROP TABLE IF EXISTS `dc_task_data_provider`;

CREATE TABLE `dc_task_data_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `meta_data_id` varchar(200)  NOT NULL COMMENT '参与任务的元数据ID',
    `meta_data_name` varchar(1024)  DEFAULT NULL COMMENT '元数据名称',
    `policy_type` int NOT NULL COMMENT '存储的策略类型',
    `input_type` int NOT NULL COMMENT '输入数据的类型. 0: unknown, 1: origin_data, 2: model',
    `identity_id` varchar(200)  NOT NULL COMMENT '(冗余)参与任务的元数据的所属组织的identity_id',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `key_column_idx` int DEFAULT NULL COMMENT '元数据id',
    `selected_columns` varchar(200)  DEFAULT NULL COMMENT '元数据选择列',
    PRIMARY KEY (`task_id`,`meta_data_id`)
) ENGINE=InnoDB  COMMENT='任务数据提供者（数据和模型）';

/*Data for the table `dc_task_data_provider` */

/*Table structure for table `dc_task_event` */

DROP TABLE IF EXISTS `dc_task_event`;

CREATE TABLE `dc_task_event` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'id',
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `event_type` varchar(20)  NOT NULL COMMENT '事件类型',
    `identity_id` varchar(200)  NOT NULL COMMENT '产生事件的组织身份ID',
    `party_id` varchar(200)  NOT NULL COMMENT '产生事件的partyId (单个组织可以担任任务的多个party, 区分是哪一方产生的event)',
    `event_at` datetime(3) NOT NULL COMMENT '产生事件的时间，精确到毫秒',
    `event_content` varchar(1024)  NOT NULL COMMENT '事件内容',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='任务事件';

/*Data for the table `dc_task_event` */

/*Table structure for table `dc_task_meta_data_column` */

DROP TABLE IF EXISTS `dc_task_meta_data_column`;

CREATE TABLE `dc_task_meta_data_column` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `meta_data_id` varchar(200)  NOT NULL COMMENT '参与任务的元数据ID',
    `selected_column_idx` int NOT NULL COMMENT '元数据在此次任务中的参与计算的字段索引序号',
    PRIMARY KEY (`task_id`,`meta_data_id`,`selected_column_idx`)
) ENGINE=InnoDB  COMMENT='任务数据metadata明细';

/*Data for the table `dc_task_meta_data_column` */

/*Table structure for table `dc_task_power_provider` */

DROP TABLE IF EXISTS `dc_task_power_provider`;

CREATE TABLE `dc_task_power_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '算力提供者组织身份ID',
    `policy_type` int NOT NULL COMMENT '存储的策略类型',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `provider_party_id` varchar(200)  DEFAULT NULL COMMENT '算力依附的数据角色',
    `used_memory` bigint DEFAULT '0' COMMENT '任务使用的内存, 字节',
    `used_core` int DEFAULT '0' COMMENT '任务使用的core',
    `used_bandwidth` bigint DEFAULT '0' COMMENT '任务使用的带宽, bps',
    PRIMARY KEY (`task_id`,`identity_id`)
) ENGINE=InnoDB  COMMENT='任务算力提供者';

/*Data for the table `dc_task_power_provider` */

/*Table structure for table `dc_task_result_consumer` */

DROP TABLE IF EXISTS `dc_task_result_consumer`;

CREATE TABLE `dc_task_result_consumer` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `consumer_identity_id` varchar(200)  NOT NULL COMMENT '结果消费者组织身份ID',
    `consumer_party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `producer_identity_id` varchar(200)  DEFAULT NULL COMMENT '结果产生者的组织身份ID',
    `producer_party_id` varchar(200)  DEFAULT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    PRIMARY KEY (`task_id`,`consumer_identity_id`)
) ENGINE=InnoDB  COMMENT='任务结果接收者';

/*Data for the table `dc_task_result_consumer` */

/*Table structure for table `mo_algorithm` */

DROP TABLE IF EXISTS `mo_algorithm`;

CREATE TABLE `mo_algorithm` (
    `algorithm_id` bigint NOT NULL COMMENT '算法表ID',
    `algorithm_desc` varchar(200)  DEFAULT NULL COMMENT '中文算法描述',
    `algorithm_desc_en` varchar(200)  DEFAULT NULL COMMENT '英文算法描述',
    `author` varchar(30)  DEFAULT NULL COMMENT '算法作者',
    `max_numbers` bigint DEFAULT NULL COMMENT '支持协同方最大数量',
    `min_numbers` bigint DEFAULT NULL COMMENT '支持协同方最小数量',
    `support_language` varchar(64)  DEFAULT NULL COMMENT '支持语言,多个以","进行分隔',
    `support_os_system` varchar(64)  DEFAULT NULL COMMENT '支持操作系统,多个以","进行分隔',
    `cost_mem` bigint DEFAULT NULL COMMENT '所需的内存 (单位: byte)',
    `cost_cpu` int DEFAULT NULL COMMENT '所需的核数 (单位: 个)',
    `cost_gpu` int DEFAULT NULL COMMENT 'GPU核数(单位：核)',
    `cost_bandwidth` bigint DEFAULT '0' COMMENT '所需的带宽 (单位: bps)',
    `run_time` bigint NOT NULL DEFAULT '3600000' COMMENT '所需的运行时长,默认1小时 (单位: ms)',
    `input_model` tinyint NOT NULL DEFAULT '0' COMMENT '是否需要输入模型: 0-否，1:是',
    `output_model` tinyint NOT NULL DEFAULT '0' COMMENT '是否产生模型: 0-否，1:是',
    `support_default_psi` tinyint NOT NULL DEFAULT '0' COMMENT '是否支持默认的psi处理: 0-否，1:是',
    `output_psi` tinyint NOT NULL DEFAULT '0' COMMENT '是否产生psi: 0-否，1:是',
    `store_pattern` tinyint NOT NULL DEFAULT '1' COMMENT '输出存储形式: 1-明文，2:密文',
    `data_rows_flag` tinyint NOT NULL DEFAULT '0' COMMENT '是否判断数据行数: 0-否，1-是',
    `data_columns_flag` tinyint NOT NULL DEFAULT '0' COMMENT '是否判断数据列数: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB  COMMENT='算法表';

/*Data for the table `mo_algorithm` */

insert  into `mo_algorithm`(`algorithm_id`,`algorithm_desc`,`algorithm_desc_en`,`author`,`max_numbers`,`min_numbers`,`support_language`,`support_os_system`,`cost_mem`,`cost_cpu`,`cost_gpu`,`cost_bandwidth`,`run_time`,`input_model`,`output_model`,`support_default_psi`,`output_psi`,`store_pattern`,`data_rows_flag`,`data_columns_flag`,`create_time`,`update_time`) values (1001,'用于跨组织的数据交集查询','Used for cross-organization data intersection query','Rosetta',3,2,'SQL,Python','window,linux,mac',1073741824,1,2,3145728,180000,0,0,0,1,1,1,0,'2022-03-24 04:09:33','2022-05-11 08:05:10'),(2011,'用于跨组织线性回归训练','Used for cross-organization linear regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:57'),(2012,'用于跨组织线性回归的预测','Used for cross-organization linear regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:57'),(2021,'用于跨组织逻辑回归训练','Used for cross-organization logistic regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:58'),(2022,'用于跨组织逻辑回归预测','Used for cross-organization logistic regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:59'),(2031,'用于跨组织DNN训练','Used for cross-organization DNN training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:00'),(2032,'用于跨组织DNN预测算法','Used for cross-organization DNN prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:00'),(2041,'用于跨组织XGBoost训练','Used for cross-organization XGBoost training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:01'),(2042,'用于跨组织XGBoost预测','Used for cross-organization XGBoost prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:03');

/*Table structure for table `mo_algorithm_classify` */

DROP TABLE IF EXISTS `mo_algorithm_classify`;

CREATE TABLE `mo_algorithm_classify` (
    `id` bigint NOT NULL COMMENT '分类id',
    `parent_id` bigint NOT NULL COMMENT '父分类id，如果为顶级分类，则为0',
    `name` varchar(30)  DEFAULT NULL COMMENT '分类中文名称',
    `name_en` varchar(60)  DEFAULT NULL COMMENT '英文算法名称',
    `image_url` varchar(1024)  DEFAULT NULL COMMENT '算法图片url',
    `is_available` tinyint NOT NULL DEFAULT '1' COMMENT '是否可用: 0-否，1-是',
    `is_algorithm` tinyint NOT NULL DEFAULT '1' COMMENT '是否算法: 0-否，1-是',
    `is_exist_algorithm` tinyint NOT NULL DEFAULT '1' COMMENT '是否存在对应算法: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_ALG_NAME` (`name`)
) ENGINE=InnoDB  COMMENT='算法分类表';

/*Data for the table `mo_algorithm_classify` */

insert  into `mo_algorithm_classify`(`id`,`parent_id`,`name`,`name_en`,`image_url`,`is_available`,`is_algorithm`,`is_exist_algorithm`,`create_time`,`update_time`) values (1,0,'计算','Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:42:10'),(2,1,'隐私计算','Privacy Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(3,1,'非隐私计算','Non-Privacy Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(1000,2,'隐私统计分析','Privacy Statistics',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(1001,1000,'隐私求交集（PSI）','Private Set Intersection','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/SetIntersection.png\r',1,1,1,'2022-03-28 11:41:08','2022-04-25 02:43:23'),(2000,2,'隐私AI计算','Privacy AI Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2010,2000,'隐私线性回归','Private Linear Regression','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/AIComputing.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:44:28'),(2011,2010,'隐私线性回归训练','Private Linear Regression Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2012,2010,'隐私线性回归预测','Private Linear Regression Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-04-01 08:51:39'),(2020,2000,'隐私逻辑回归','Private Logistic Regression','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/logistic.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:44:08'),(2021,2020,'隐私逻辑回归训练','Private Logistic Regression Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2022,2020,'隐私逻辑回归预测','Private Logistic Regression Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2030,2000,'隐私DNN（深度神经网络）','Private DNN','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/DNN.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:43:38'),(2031,2030,'隐私DNN训练','Private DNN Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2032,2030,'隐私DNN预测','Private DNN Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2040,2000,'隐私XGBoost','Private XGBoost','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/XGBoost.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:43:52'),(2041,2040,'隐私XGBoost训练','Private XGBoost Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2042,2040,'隐私XGBoost预测','Private XGBoost Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08');

/*Table structure for table `mo_algorithm_code` */

DROP TABLE IF EXISTS `mo_algorithm_code`;

CREATE TABLE `mo_algorithm_code` (
    `algorithm_id` bigint NOT NULL COMMENT '算法代码表ID',
    `edit_type` tinyint DEFAULT NULL COMMENT '编辑类型:1-sql,2-noteBook',
    `calculate_contract_struct` varchar(1024)  DEFAULT NULL COMMENT '计算合约变量模板json格式结构',
    `calculate_contract_code` text  COMMENT '计算合约',
    `data_split_contract_code` text  COMMENT '数据分片合约',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB  COMMENT='算法代码表';

/*Data for the table `mo_algorithm_code` */

insert  into `mo_algorithm_code`(`algorithm_id`,`edit_type`,`calculate_contract_struct`,`calculate_contract_code`,`data_split_contract_code`,`create_time`,`update_time`) values (1001,2,'{\"use_alignment\":false,\"label_owner\":\"\",\"label_column\":\"\",\"psi_type\":\"T_V1_Basic_GLS254\",\"data_flow_restrict\":{}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.psi as psi\r\nfrom functools import wraps\r\n\r\n\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivateSetIntersection(BaseAlgorithm):\r\n    \'\'\'\r\n    private set intersection.\r\n    \'\'\'\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs) \r\n        self.output_file = os.path.join(self.results_dir, \"psi_result.csv\")\r\n        self.sdk_log_level = 3  # Trace=0, Debug=1, Audit=2, Info=3, Warn=4, Error=5, Fatal=6, Off=7\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_alignment\": true,\r\n                \"label_owner\": \"data1\",\r\n                \"label_column\": \"diagnosis\",\r\n                \"psi_type\": \"T_V1_Basic_GLS254\",\r\n                \"data_flow_restrict\": {\r\n                    \"data1\": [\"compute1\"],\r\n                    \"data2\": [\"compute2\"],\r\n                    \"compute1\": [\"result1\"],\r\n                    \"compute2\": [\"result2\"]\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_alignment = dynamic_parameter[\"use_alignment\"]\r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        self.label_column = dynamic_parameter.get(\"label_column\")\r\n        if self.use_alignment and (self.party_id == self.label_owner):\r\n            self.data_with_label = True\r\n        else:\r\n            self.data_with_label = False\r\n        if not self.use_alignment:\r\n            self.selected_columns = []\r\n        self.psi_type = dynamic_parameter.get(\"psi_type\", \"T_V1_Basic_GLS254\")  # default \'T_V1_Basic_GLS254\'\r\n        self.data_flow_restrict = dynamic_parameter.get(\"data_flow_restrict\")\r\n\r\n    def check_parameters(self):\r\n        assert len(self.data_party) == 2, f\"length of data_party must be 2, not {len(self.data_party)}.\"\r\n        assert len(self.result_party) in [1, 2], f\"length of result_party must be 1 or 2, not {len(self.result_party)}.\"\r\n        self._check_input_data()\r\n        self.check_params_type(data_flow_restrict=(self.data_flow_restrict, dict))\r\n\r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                assert self.key_column, f\"key_column can not empty. key_column={self.key_column}\"\r\n                if self.use_alignment:\r\n                    assert self.selected_columns, f\"selected_columns can not empty. selected_columns={self.selected_columns}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.data_with_label:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n\r\n    def run(self):\r\n        log.info(\"start create and set channel.\")\r\n        log.info(\"start extract data.\")\r\n        usecols_file = self._extract_data_column()\r\n        log.info(\"start send_data_to_compute_party.\")\r\n        self._send_data_to_compute_party(usecols_file)\r\n        psi_output_file = os.path.join(self.temp_dir, \"psi_sdk_output.csv\")\r\n        alignment_output_file = self.output_file\r\n        if self.party_id in self.compute_party:\r\n            log.info(\"start extract key column.\")\r\n            key_col_file, key_col_name, usecols_data = self._extract_key_column(usecols_file)\r\n            log.info(\"start run psi sdk.\")\r\n            self._run_psi_sdk(key_col_file, psi_output_file)\r\n            log.info(\"start alignment result.\")\r\n            self._alignment_result(psi_output_file, usecols_data, alignment_output_file, key_col_name)\r\n        log.info(\"start send data to result party.\")\r\n        self._send_data_to_result_party(alignment_output_file)\r\n        log.info(\"finish send data to result party.\")\r\n        result_path, result_type = \'\', \'\'\r\n        if self.party_id in self.result_party:\r\n            result_path = alignment_output_file\r\n            result_type = \'csv\'\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"psi all success.\")\r\n        return result_path, result_type\r\n    \r\n    def _send_data_to_compute_party(self, data_path):\r\n        if self.party_id in self.data_party:\r\n            compute_party = self.data_flow_restrict[self.party_id][0]\r\n            self.io_channel.send_data_to_other_party(compute_party, data_path)\r\n        elif self.party_id in self.compute_party:\r\n            for party in self.data_party:\r\n                if self.party_id == self.data_flow_restrict[party][0]:\r\n                    self.io_channel.recv_data_from_other_party(party, data_path)\r\n        else:\r\n            pass\r\n    \r\n    def _send_data_to_result_party(self, data_path):\r\n        if self.party_id in self.compute_party:\r\n            result_party = self.data_flow_restrict[self.party_id][0]\r\n            self.io_channel.send_data_to_other_party(result_party, data_path)\r\n        elif self.party_id in self.result_party:\r\n            for party in self.compute_party:\r\n                if self.party_id == self.data_flow_restrict[party][0]:\r\n                    self.io_channel.recv_data_from_other_party(party, data_path)\r\n        else:\r\n            pass\r\n\r\n    def _extract_data_column(self):\r\n        \'\'\'\r\n        Extract data column from input file,\r\n        and then write to a new file.\r\n        \'\'\'\r\n        usecols_file = os.path.join(self.temp_dir, f\"usecols_{self.party_id}.csv\")\r\n\r\n        if self.party_id in self.data_party:\r\n            use_cols = [self.key_column] + self.selected_columns\r\n            if self.data_with_label:\r\n                use_cols += [self.label_column]\r\n            log.info(\"read input file and write to new file.\")\r\n            usecols_data = pd.read_csv(self.input_file, usecols=use_cols, dtype=\"str\")\r\n            usecols_data = usecols_data[use_cols]\r\n            usecols_data.to_csv(usecols_file, header=True, index=False)\r\n        return usecols_file\r\n    \r\n    def _extract_key_column(self, usecols_file):\r\n        usecols_data = pd.read_csv(usecols_file, header=0, dtype=\"str\")\r\n        usecols = list(usecols_data.columns)\r\n        key_col_name = usecols[0]\r\n        if self.use_alignment:\r\n            key_data = usecols_data[key_col_name]\r\n            key_col_file = os.path.join(self.temp_dir, f\"key_col_{self.party_id}.csv\")\r\n            key_data.to_csv(key_col_file, header=True, index=False)\r\n        else:\r\n            key_col_file = usecols_file\r\n        return key_col_file, key_col_name, usecols_data\r\n\r\n    def _run_psi_sdk(self, input_file, output_file):\r\n        \'\'\'\r\n        run psi sdk\r\n        \'\'\'\r\n        log.info(\"start create psihandler.\")\r\n        psihandler = psi.PSIHandler()\r\n        log.info(\"start set log.\")\r\n        psihandler.log_to_stdout(True)\r\n        psihandler.set_loglevel(self.sdk_log_level)\r\n        log.info(\"start set recv party.\")\r\n        psihandler.set_recv_party(2, \"\")\r\n\r\n        log.info(\"start create iohandler.\")\r\n        iohandler = psi.IOHandler()\r\n        log.info(\"start set channel.\")\r\n        iohandler.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"start activate.\")\r\n        psihandler.activate(self.psi_type, \"\")\r\n        log.info(\"finish activate.\")\r\n        log.info(\"start psihandler prepare data.\")\r\n        psihandler.prepare(input_file, taskid=\"\")\r\n        log.info(\"start psihandler run.\")\r\n        psihandler.run(input_file, output_file, taskid=\"\")\r\n        log.info(\"finish psihandler run.\")\r\n        run_stats = psihandler.get_perf_stats(True, \"\")\r\n        run_stats = run_stats.replace(\'\\n\', \'\').replace(\' \', \'\')\r\n        log.info(f\"run stats: {run_stats}\")\r\n        log.info(\"start deactivate.\")\r\n        psihandler.deactivate(\"\")\r\n        log.info(\"finish deactivate.\")\r\n    \r\n    def _alignment_result(self, psi_output_file, usecols_data, alignment_output_file, key_col_name):\r\n        \'\'\'\r\n        for the compute_party, sort the key_col values and alignment the select_columns.\r\n        \'\'\'\r\n        if os.path.exists(psi_output_file):\r\n            psi_result = pd.read_csv(psi_output_file, header=None, dtype=\"str\")\r\n            psi_result = pd.DataFrame(psi_result.values, columns=[key_col_name])\r\n            psi_result.sort_values(by=[key_col_name], ascending=True, inplace=True)\r\n            if self.use_alignment:\r\n                alignment_result = pd.merge(psi_result, usecols_data, on=key_col_name)\r\n            else:\r\n                alignment_result = psi_result\r\n            alignment_result.to_csv(alignment_output_file, index=False, header=True)\r\n            log.info(f\"alignment_result shape: {alignment_result.shape}\")\r\n        else:\r\n            use_cols = list(usecols_data.columns)\r\n            log.info(f\"psi_result is Empty, only have Column name: {use_cols}\")\r\n            with open(alignment_output_file, \'w\') as output_f:\r\n                output_f.write(\',\'.join(use_cols)+\"\\n\")\r\n\r\n\r\n@ErrorTraceback(\"psi\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    psi = PrivateSetIntersection(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type = psi.run()\r\n    return result_path, result_type\r\n',NULL,'2022-04-25 03:02:06','2022-05-12 08:45:16'),(2011,2,'{\"label_owner\":\"p1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivacyLinearRegTrain(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy linear regression train base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_dir = self._get_output_dir()\r\n        self.output_file = os.path.join(self.output_dir, \"model\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"dynamic_parameter\": {\r\n                \"label_owner\": \"p1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1, not {input_type}\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        if self.party_id == self.label_owner:\r\n            self.label_column = dynamic_parameter.get(\"label_column\")\r\n            self.data_with_label = True\r\n        else:\r\n            self.label_column = \"\"\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.001)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(epochs=(self.epochs, int),\r\n                               batch_size=(self.batch_size, int),\r\n                               learning_rate=(self.learning_rate, float),\r\n                               use_validation_set=(self.use_validation_set, bool),\r\n                               validation_set_rate=(self.validation_set_rate, float))\r\n        assert self.epochs > 0, f\"epochs must be greater 0, not {self.epochs}\"\r\n        assert self.batch_size > 0, f\"batch_size must be greater 0, not {self.batch_size}\"\r\n        assert self.learning_rate > 0, f\"learning rate must be greater 0, not {self.learning_rate}\"\r\n        assert 0 < self.validation_set_rate < 1, f\"validattion_set_rate must be between (0,1), not {self.validation_set_rate}\"\r\n        log.info(f\"check parameter finish.\")\r\n\r\n    def _check_input_data(self):    \r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                    key_column=(self.key_column, str),\r\n                                    selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.label_column:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n   \r\n    def train(self):\r\n        \'\'\'\r\n        Linear regression training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num])\r\n            Y = tf.placeholder(tf.float64, [None, 1])\r\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n            pred_Y = tf.matmul(X, W) + b\r\n            loss = tf.square(Y - pred_Y)\r\n            loss = tf.reduce_mean(loss)\r\n            # optimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            \r\n            reveal_Y = rtt.SecureReveal(pred_Y)\r\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\r\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                \r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        \r\n        result_path, result_type, evaluate_result = \"\", \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"result_party deal with the result.\")\r\n            result_path = self.output_dir\r\n            result_type = \"dir\"\r\n            if self.use_validation_set:\r\n                log.info(\"result_party evaluate model.\")\r\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n                evaluate_result = self.evaluate(Y_true, Y_pred)\r\n        \r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"train success all.\")\r\n        return result_path, result_type, evaluate_result\r\n        \r\n    def evaluate(self, Y_true, Y_pred):\r\n        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\r\n        r2 = r2_score(Y_true, Y_pred)\r\n        rmse = mean_squared_error(Y_true, Y_pred, squared=False)\r\n        mse = mean_squared_error(Y_true, Y_pred, squared=True)\r\n        mae = mean_absolute_error(Y_true, Y_pred)\r\n        r2 = round(r2, 6)\r\n        rmse = round(rmse, 6)\r\n        mse = round(mse, 6)\r\n        mae = round(mae, 6)\r\n        evaluate_result = {\r\n            \"R2-score\": r2,\r\n            \"RMSE\": rmse,\r\n            \"MSE\": mse,\r\n            \"MAE\": mae\r\n        }\r\n        log.info(f\"evaluate_result = {evaluate_result}\")\r\n        evaluate_result = json.dumps(evaluate_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluate_result\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def _get_output_dir(self):\r\n        output_dir = os.path.join(self.results_dir, \'model\')\r\n        self.mkdir(output_dir)\r\n        return output_dir\r\n\r\n\r\n@ErrorTraceback(\"privacy_linr_train\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_linr = PrivacyLinearRegTrain(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type, extra = privacy_linr.train()\r\n    return result_path, result_type, extra\r\n',NULL,'2022-03-24 04:09:36','2022-05-12 08:46:03'),(2012,2,'{\"model_restore_party\":\"model1\",\"predict_threshold\":0.5}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\nclass PrivacyLinearRegPredict(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy linear regression predict base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_file = os.path.join(self.results_dir, \"result_predict.csv\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"model_restore_party\": \"model1\",\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2, not {input_type}\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.model_restore_party = dynamic_parameter[\"model_restore_party\"]\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(model_restore_party=(self.model_restore_party, str))\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model_path is not exist. model_path={self.model_path}\"\r\n        log.info(f\"check parameter finish.\")\r\n    \r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:         \r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                   key_column=(self.key_column, str),\r\n                                   selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        Linear regression predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        X = tf.placeholder(tf.float64, [None, column_total_num])\r\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n        init = tf.global_variables_initializer()\r\n        # predict\r\n        pred_Y = tf.matmul(X, W) + b\r\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n        with tf.Session() as sess:\r\n            log.info(\"session init.\")\r\n            sess.run(init)\r\n            log.info(\"start restore model.\")\r\n            if self.party_id == self.model_restore_party:\r\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                    log.info(f\"model restore from: {self.model_file}.\")\r\n                    saver.restore(sess, self.model_file)\r\n                else:\r\n                    raise Exception(\"model not found or model damaged\")\r\n            else:\r\n                log.info(\"restore model...\")\r\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                with open(temp_file, \"w\") as f:\r\n                    pass\r\n                saver.restore(sess, temp_file)\r\n            log.info(\"finish restore model.\")\r\n            \r\n            # predict\r\n            log.info(\"predict start.\")\r\n            predict_start_time = time.time()\r\n            Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n            log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n            predict_use_time = round(time.time() - predict_start_time, 3)\r\n            log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        result_path, result_type = \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            Y_pred = Y_pred.astype(\"float\")\r\n            Y_result = pd.DataFrame(Y_pred, columns=[\"Y_predict\"])\r\n            Y_result.to_csv(self.output_file, header=True, index=False)\r\n            result_path = self.output_file\r\n            result_type = \"csv\"\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n        return result_path, result_type\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n\r\n        return file_x, id_col\r\n\r\n\r\n@ErrorTraceback(\"privacy_linr_predict\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_linr = PrivacyLinearRegPredict(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type = privacy_linr.predict()\r\n    return result_path, result_type\r\n',NULL,'2022-03-24 04:09:36','2022-05-18 07:05:34'),(2021,2,'{\"label_owner\":\"data1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\nclass PrivacyLRTrain(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy logistic regression train base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_dir = self._get_output_dir()\r\n        self.output_file = os.path.join(self.output_dir, \"model\")\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"label_owner\": \"data1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                else:\r\n                    raise Exception(f\"paramter error. input_type only support 1, not {input_type}\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.label_owner = dynamic_parameter[\"label_owner\"]\r\n        self.label_column = dynamic_parameter[\"label_column\"]\r\n        if self.party_id == self.label_owner:\r\n            self.data_with_label = True\r\n        else:\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.001)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(epochs=(self.epochs, int),\r\n                               batch_size=(self.batch_size, int),\r\n                               learning_rate=(self.learning_rate, float),\r\n                               use_validation_set=(self.use_validation_set, bool),\r\n                               validation_set_rate=(self.validation_set_rate, float),\r\n                               predict_threshold=(self.predict_threshold, float))\r\n        assert self.epochs > 0, f\"epochs must be greater 0, not {self.epochs}\"\r\n        assert self.batch_size > 0, f\"batch_size must be greater 0, not {self.batch_size}\"\r\n        assert self.learning_rate > 0, f\"learning rate must be greater 0, not {self.learning_rate}\"\r\n        assert 0 < self.validation_set_rate < 1, f\"validattion_set_rate must be between (0,1), not {self.validation_set_rate}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"\r\n        log.info(f\"check parameter finish.\")\r\n\r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                   key_column=(self.key_column, str),\r\n                                   selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.data_with_label:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")             \r\n \r\n    def train(self):\r\n        \'\'\'\r\n        Logistic regression training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"start build the model structure.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num])\r\n            Y = tf.placeholder(tf.float64, [None, 1])\r\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n            logits = tf.matmul(X, W) + b\r\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)\r\n            loss = tf.reduce_mean(loss)\r\n            # optimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            \r\n            pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\r\n            reveal_Y = rtt.SecureReveal(pred_Y)\r\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\r\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\r\n            log.info(\"finish build the model structure.\")\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                \r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        \r\n        result_path, result_type, evaluate_result = \"\", \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"result_party deal with the result.\")\r\n            result_path = self.output_dir\r\n            result_type = \"dir\"\r\n            if self.use_validation_set:\r\n                log.info(\"result_party evaluate model.\")\r\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n                evaluate_result = self.evaluate(Y_true, Y_pred)\r\n        \r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"train success all.\")\r\n        return result_path, result_type, evaluate_result\r\n    \r\n    def evaluate(self, Y_true, Y_pred):\r\n        \'\'\'\r\n        only support binary class\r\n        \'\'\'\r\n        log.info(\"start model evaluation.\")\r\n        from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\r\n        log.info(\"start evaluate auc score.\")\r\n        auc_score = roc_auc_score(Y_true, Y_pred)\r\n        Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n        log.info(\"start evaluate accuracy score.\")\r\n        accuracy = accuracy_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate f1_score.\")\r\n        f1_score = f1_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate precision score.\")\r\n        precision = precision_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate recall score.\")\r\n        recall = recall_score(Y_true, Y_pred_class)\r\n        auc_score = round(auc_score, 6)\r\n        accuracy = round(accuracy, 6)\r\n        f1_score = round(f1_score, 6)\r\n        precision = round(precision, 6)\r\n        recall = round(recall, 6)\r\n        evaluate_result = {\r\n            \"AUC\": auc_score,\r\n            \"accuracy\": accuracy,\r\n            \"f1_score\": f1_score,\r\n            \"precision\": precision,\r\n            \"recall\": recall\r\n        }\r\n        log.info(f\"evaluate_result = {evaluate_result}\")\r\n        evaluate_result = json.dumps(evaluate_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluate_result\r\n \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\") # usecols not ensure the order of columns\r\n            input_data = input_data[usecols]  # use for ensure the order of columns\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_class_num = train_y_data.unique().shape[0]\r\n                assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_class_num = val_y_data.unique().shape[0]\r\n                    assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def _get_output_dir(self):\r\n        output_dir = os.path.join(self.results_dir, \'model\')\r\n        self.mkdir(output_dir)\r\n        return output_dir\r\n\r\n\r\n@ErrorTraceback(\"privacy_lr_train\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_lr = PrivacyLRTrain(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type, extra = privacy_lr.train()\r\n    return result_path, result_type, extra\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 03:25:53'),(2022,2,'{\"model_restore_party\":\"model1\",\"predict_threshold\":0.5}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                               result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivacyLRPredict(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy logistic regression predict base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_file = os.path.join(self.results_dir, \"result_predict.csv\")\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"model_restore_party\": \"model1\",\r\n                \"predict_threshold\": 0.5\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2, not {input_type}\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.model_restore_party = dynamic_parameter[\"model_restore_party\"]\r\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party      \r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(model_restore_party=(self.model_restore_party, str), \r\n                               predict_threshold=(self.predict_threshold, float))\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model_path is not exist. model_path={self.model_path}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"\r\n        log.info(f\"check parameter finish.\")\r\n    \r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                   key_column=(self.key_column, str),\r\n                                   selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n\r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        Logistic regression predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        X = tf.placeholder(tf.float64, [None, column_total_num])\r\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n        init = tf.global_variables_initializer()\r\n        # predict\r\n        pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\r\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n        with tf.Session() as sess:\r\n            log.info(\"session init.\")\r\n            sess.run(init)\r\n            log.info(\"start restore model.\")\r\n            if self.party_id == self.model_restore_party:\r\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                    log.info(f\"model restore from: {self.model_file}.\")\r\n                    saver.restore(sess, self.model_file)\r\n                else:\r\n                    raise Exception(\"model not found or model damaged\")\r\n            else:\r\n                log.info(\"restore model...\")\r\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                with open(temp_file, \"w\") as f:\r\n                    pass\r\n                saver.restore(sess, temp_file)\r\n            log.info(\"finish restore model.\")\r\n            \r\n            # predict\r\n            log.info(\"predict start.\")\r\n            predict_start_time = time.time()\r\n            Y_pred_prob = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n            log.debug(f\"Y_pred_prob:\\n {Y_pred_prob[:10]}\")\r\n            predict_use_time = round(time.time() - predict_start_time, 3)\r\n            log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        result_path, result_type = \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            Y_pred_prob = Y_pred_prob.astype(\"float\")\r\n            Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\r\n            Y_class = (Y_pred_prob > self.predict_threshold) * 1\r\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\r\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(self.output_file, header=True, index=False)\r\n            result_path = self.output_file\r\n            result_type = \"csv\"\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n        return result_path, result_type\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        for data party, extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n        return file_x, id_col\r\n\r\n\r\n@ErrorTraceback(\"privacy_lr_predict\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_lr = PrivacyLRPredict(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type = privacy_lr.predict()\r\n    return result_path, result_type\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 03:26:00'),(2031,2,'{\"label_owner\":\"data1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"layer_units\":[32,128,32,1],\"layer_activation\":[\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\"],\"init_method\":\"random_normal\",\"use_intercept\":true,\"optimizer\":\"sgd\",\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport copy\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\nclass PrivacyDnnTrain(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy DNN train base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_dir = self._get_output_dir()\r\n        self.output_file = os.path.join(self.output_dir, \"model\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"label_owner\": \"p1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"layer_units\": [32, 1],     # hidden layer and output layer units\r\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\"],   # hidden layer and output layer activation\r\n                    \"init_method\": \"random_normal\",   # weight and bias init method\r\n                    \"use_intercept\": true,     # whether use bias\r\n                    \"optimizer\": \"sgd\",\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1, not {input_type}\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.label_owner = dynamic_parameter[\"label_owner\"]\r\n        self.label_column = dynamic_parameter[\"label_column\"]\r\n        if self.party_id == self.label_owner:\r\n            self.data_with_label = True\r\n        else:\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 50)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.1)\r\n        self.layer_units = hyperparams.get(\"layer_units\", [32, 1])\r\n        self.layer_activation = hyperparams.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\"])\r\n        self.init_method = hyperparams.get(\"init_method\", \"random_normal\")  # \'random_normal\', \'random_uniform\', \'zeros\', \'ones\'\r\n        self.use_intercept = hyperparams.get(\"use_intercept\", True)  # True: use bias, False: not use bias\r\n        self.optimizer = hyperparams.get(\"optimizer\", \"sgd\")\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(epochs=(self.epochs, int),\r\n                               batch_size=(self.batch_size, int),\r\n                               learning_rate=(self.learning_rate, float),\r\n                               layer_units=(self.layer_units, list),\r\n                               layer_activation=(self.layer_activation, list),\r\n                               init_method=(self.init_method, str),\r\n                               use_intercept=(self.use_intercept, bool),\r\n                               optimizer=(self.optimizer, str),\r\n                               use_validation_set=(self.use_validation_set, bool),\r\n                               validation_set_rate=(self.validation_set_rate, float),\r\n                               predict_threshold=(self.predict_threshold, float))\r\n        assert self.epochs > 0, f\"epochs must be greater 0, not {self.epochs}\"\r\n        assert self.batch_size > 0, f\"batch_size must be greater 0, not {self.batch_size}\"\r\n        assert self.learning_rate > 0, f\"learning rate must be greater 0, not {self.learning_rate}\"\r\n        assert 0 < self.validation_set_rate < 1, f\"validattion_set_rate must be between (0,1), not {self.validation_set_rate}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"\r\n        assert self.layer_units, f\"layer_units must not empty, not {self.layer_units}\"\r\n        assert self.layer_activation, f\"layer_activation must not empty, not {self.layer_activation}\"\r\n        assert len(self.layer_units) == len(self.layer_activation), \\\r\n                f\"the length of layer_units:{len(self.layer_units)} and layer_activation:{len(self.layer_activation)} not same\"\r\n        for i in self.layer_units:\r\n            assert isinstance(i, int) and i > 0, f\"layer_units\'element can only be type(int) and greater 0, not {i}\"\r\n        for i in self.layer_activation:\r\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\r\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\r\n        if self.layer_activation[-1] == \'sigmoid\':\r\n            if self.layer_units[-1] != 1:\r\n                raise Exception(f\"when output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\r\n        if self.init_method == \'random_normal\':\r\n            self.init_method = tf.random_normal\r\n        elif self.init_method == \'random_uniform\':\r\n            self.init_method = tf.random_uniform\r\n        elif self.init_method == \'zeros\':  # if len(self.layer_units) != 1, init_method not use zeros, because it can not work well.\r\n            self.init_method = tf.zeros\r\n        elif self.init_method == \'ones\':\r\n            self.init_method = tf.ones\r\n        else:\r\n            raise Exception(f\"init_method only can be random_normal/random_uniform/zeros/ones, not {self.init_method}\")\r\n        if self.optimizer == \'sgd\':\r\n            self.optimizer = tf.train.GradientDescentOptimizer\r\n        else:\r\n            raise Exception(f\"optimizer only can be sgd, not {self.optimizer}\")\r\n        log.info(f\"check parameter finish.\")\r\n\r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                    key_column=(self.key_column, str),\r\n                                    selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.data_with_label:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")                       \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, \r\n                                              label_owner=self.label_owner)\\\r\n                                .load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, \r\n                                                          label_owner=self.label_owner)\\\r\n                                            .load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\r\n            Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'Y\')\r\n            val_Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'val_Y\')\r\n                        \r\n            output = self.dnn(X, column_total_num)\r\n            \r\n            output_layer_activation = self.layer_activation[-1]\r\n            with tf.name_scope(\'output\'):\r\n                if not output_layer_activation:\r\n                    pred_Y = output\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    pred_Y = tf.sigmoid(output)\r\n                elif output_layer_activation == \'relu\':\r\n                    pred_Y = tf.nn.relu(output)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            with tf.name_scope(\'loss\'):\r\n                if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n                    loss = tf.square(Y - pred_Y)\r\n                    loss = tf.reduce_mean(loss)\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\r\n                    loss = tf.reduce_mean(loss)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            \r\n            # optimizer\r\n            with tf.name_scope(\'optimizer\'):\r\n                optimizer = self.optimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'saver\')\r\n                        \r\n            reveal_loss = rtt.SecureReveal(loss) # only reveal to the result party\r\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to the result party\r\n            reveal_val_Y = rtt.SecureReveal(val_Y) # only reveal to the result party\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                summary_writer = tf.summary.FileWriter(self.get_temp_dir(), sess.graph)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                loss_history_train, loss_history_val = [], []\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                    # train_loss = sess.run(reveal_loss, feed_dict={X: shard_x, Y: shard_y})\r\n                    # # collect loss\r\n                    # loss_history_train.append(float(train_loss))\r\n                    # if self.use_validation_set:\r\n                    #     val_loss = sess.run(reveal_loss, feed_dict={X: shard_x_val, Y: shard_y_val})\r\n                    #     loss_history_val.append(float(val_loss))\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_val_Y, feed_dict={val_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        \r\n        result_path, result_type, evaluate_result = \"\", \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"result_party deal with the result.\")\r\n            result_path = self.output_dir\r\n            result_type = \"dir\"\r\n            if self.use_validation_set:\r\n                log.info(\"result_party evaluate model.\")\r\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n                evaluate_result = self.evaluate(Y_true, Y_pred, output_layer_activation)\r\n            # self.show_train_history(loss_history_train, loss_history_val, self.epochs)\r\n            # log.info(f\"result_party show train history finish.\")\r\n        \r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"train success all.\")\r\n        return result_path, result_type, evaluate_result\r\n    \r\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\r\n        with tf.name_scope(layer_name):\r\n            W = tf.Variable(self.init_method([input_dim, output_dim], dtype=tf.float64), name=\'W\')\r\n            if self.use_intercept:\r\n                b = tf.Variable(self.init_method([output_dim], dtype=tf.float64), name=\'b\')\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W) + b\r\n            else:\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W)\r\n            if not activation:\r\n                one_layer = logits\r\n            elif activation == \'sigmoid\':\r\n                one_layer = tf.sigmoid(logits)\r\n            elif activation == \'relu\':\r\n                one_layer = tf.nn.relu(logits)\r\n            else:\r\n                raise Exception(f\'not support {activation} activation.\')\r\n            return one_layer\r\n    \r\n    def dnn(self, input_X, input_dim):\r\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\r\n        layer_activation.append(\"\")\r\n        for i in range(len(self.layer_units)):\r\n            if i == 0:\r\n                input_units = input_dim\r\n                previous_output = input_X\r\n            else:\r\n                input_units = self.layer_units[i-1]\r\n                previous_output = output\r\n            output = self.layer(previous_output, \r\n                                input_units, \r\n                                self.layer_units[i], \r\n                                layer_activation[i], \r\n                                layer_name=f\"Dense_{i}\")\r\n        return output\r\n    \r\n    def evaluate(self, Y_true, Y_pred, output_layer_activation):\r\n        if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\r\n            r2 = r2_score(Y_true, Y_pred)\r\n            rmse = mean_squared_error(Y_true, Y_pred, squared=False)\r\n            mse = mean_squared_error(Y_true, Y_pred, squared=True)\r\n            mae = mean_absolute_error(Y_true, Y_pred)\r\n            r2 = round(r2, 6)\r\n            rmse = round(rmse, 6)\r\n            mse = round(mse, 6)\r\n            mae = round(mae, 6)\r\n            evaluate_result = {\r\n                \"R2-score\": r2,\r\n                \"RMSE\": rmse,\r\n                \"MSE\": mse,\r\n                \"MAE\": mae\r\n            }\r\n        elif output_layer_activation == \'sigmoid\':\r\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\r\n            auc_score = roc_auc_score(Y_true, Y_pred)\r\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n            accuracy = accuracy_score(Y_true, Y_pred_class)\r\n            f1_score = f1_score(Y_true, Y_pred_class)\r\n            precision = precision_score(Y_true, Y_pred_class)\r\n            recall = recall_score(Y_true, Y_pred_class)\r\n            auc_score = round(auc_score, 6)\r\n            accuracy = round(accuracy, 6)\r\n            f1_score = round(f1_score, 6)\r\n            precision = round(precision, 6)\r\n            recall = round(recall, 6)\r\n            evaluate_result = {\r\n                \"AUC\": auc_score,\r\n                \"accuracy\": accuracy,\r\n                \"f1_score\": f1_score,\r\n                \"precision\": precision,\r\n                \"recall\": recall\r\n            }\r\n        else:\r\n            raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n        log.info(f\"evaluate_result = {evaluate_result}\")\r\n        evaluate_result = json.dumps(evaluate_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluate_result\r\n    \r\n    def show_train_history(self, train_history, val_history, epochs, name=\'loss\'):\r\n        log.info(\"start show_train_history\")\r\n        assert all([isinstance(ele, float) for ele in train_history]), \'element of train_history must be float.\'\r\n        import matplotlib.pyplot as plt\r\n        plt.figure()\r\n        y_min = min(train_history)\r\n        y_max = max(train_history)\r\n        y_ticks = np.linspace(y_min, y_max, 10)\r\n        plt.scatter(list(range(1, epochs+1)), train_history, label=\'train\')\r\n        if self.use_validation_set:\r\n            plt.scatter(list(range(1, epochs+1)), val_history, label=\'val\')\r\n        plt.xlabel(\'epochs\') \r\n        plt.ylabel(name)\r\n        plt.yticks(y_ticks)\r\n        plt.title(f\'{name} with epochs\')\r\n        plt.legend()\r\n        figure_path = os.path.join(self.results_dir, f\'{name}.jpg\')\r\n        plt.savefig(figure_path)\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_class_num = train_y_data.unique().shape[0]\r\n                if self.layer_activation[-1] == \'sigmoid\':\r\n                    assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_class_num = val_y_data.unique().shape[0]\r\n                    if self.layer_activation[-1] == \'sigmoid\':\r\n                        assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def _get_output_dir(self):\r\n        output_dir = os.path.join(self.results_dir, \'model\')\r\n        self.mkdir(output_dir)\r\n        return output_dir\r\n\r\n\r\n@ErrorTraceback(\"privacy_dnn_train\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_dnn = PrivacyDnnTrain(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type, extra = privacy_dnn.train()\r\n    return result_path, result_type, extra\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 06:20:10'),(2032,2,'{\"model_restore_party\":\"model1\",\"hyperparams\":{\"layer_units\":[32,128,32,1],\"layer_activation\":[\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\"],\"use_intercept\":true,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport copy\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                               result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivacyDnnPredict(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy Dnn predict base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_file = os.path.join(self.results_dir, \"result_predict.csv\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"model_restore_party\": \"model1\",\r\n                \"hyperparams\": {\r\n                    \"layer_units\": [32, 1],\r\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\"],\r\n                    \"use_intercept\": true,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2, not {input_type}\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.model_restore_party = dynamic_parameter[\"model_restore_party\"]\r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.layer_units = hyperparams.get(\"layer_units\", [32, 1])\r\n        self.layer_activation = hyperparams.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\"])\r\n        self.use_intercept = hyperparams.get(\"use_intercept\", True)  # True: use b, False: not use b        \r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(model_restore_party=(self.model_restore_party, str),\r\n                               layer_units=(self.layer_units, list),\r\n                               layer_activation=(self.layer_activation, list),\r\n                               use_intercept=(self.use_intercept, bool),\r\n                               predict_threshold=(self.predict_threshold, float))\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model_path is not exist. model_path={self.model_path}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"\r\n        assert self.layer_units, f\"layer_units must not empty, not {self.layer_units}\"\r\n        assert self.layer_activation, f\"layer_activation must not empty, not {self.layer_activation}\"\r\n        assert len(self.layer_units) == len(self.layer_activation), \\\r\n                f\"the length of layer_units:{len(self.layer_units)} and layer_activation:{len(self.layer_activation)} not same\"\r\n        for i in self.layer_units:\r\n            assert isinstance(i, int) and i > 0, f\"layer_units\'element can only be type(int) and greater 0, not {i}\"\r\n        for i in self.layer_activation:\r\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\r\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\r\n        if self.layer_activation[-1] == \'sigmoid\':\r\n            if self.layer_units[-1] != 1:\r\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\r\n        log.info(f\"check parameter finish.\")\r\n    \r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                    key_column=(self.key_column, str),\r\n                                    selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\r\n            output = self.dnn(X, column_total_num)\r\n            output_layer_activation = self.layer_activation[-1]\r\n            with tf.name_scope(\'output\'):\r\n                if not output_layer_activation:\r\n                    pred_Y = output\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    pred_Y = tf.sigmoid(output)\r\n                elif output_layer_activation == \'relu\':\r\n                    pred_Y = tf.nn.relu(output)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            init = tf.global_variables_initializer()\r\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                log.info(\"start restore model.\")\r\n                if self.party_id == self.model_restore_party:\r\n                    if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                        log.info(f\"model restore from: {self.model_file}.\")\r\n                        saver.restore(sess, self.model_file)\r\n                    else:\r\n                        raise Exception(\"model not found or model damaged\")\r\n                else:\r\n                    log.info(\"restore model...\")\r\n                    temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                    with open(temp_file, \"w\") as f:\r\n                        pass\r\n                    saver.restore(sess, temp_file)\r\n                log.info(\"finish restore model.\")\r\n                \r\n                # predict\r\n                log.info(\"predict start.\")\r\n                predict_start_time = time.time()\r\n                Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n                log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                predict_use_time = round(time.time() - predict_start_time, 3)\r\n                log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        result_path, result_type = \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            Y_pred = Y_pred.astype(\"float\")\r\n            if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n                Y_result = pd.DataFrame(Y_pred, columns=[\"Y_pred\"])\r\n            elif output_layer_activation == \'sigmoid\':\r\n                Y_prob = pd.DataFrame(Y_pred, columns=[\"Y_prob\"])\r\n                Y_class = (Y_pred > self.predict_threshold) * 1\r\n                Y_class = pd.DataFrame(Y_class, columns=[f\"Y_class(>{self.predict_threshold})\"])\r\n                Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(self.output_file, header=True, index=False)\r\n            result_path = self.output_file\r\n            result_type = \"csv\"\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n        return result_path, result_type\r\n\r\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\r\n        with tf.name_scope(layer_name):\r\n            W = tf.Variable(tf.random_normal([input_dim, output_dim], dtype=tf.float64), name=\'W\')\r\n            if self.use_intercept:\r\n                b = tf.Variable(tf.random_normal([output_dim], dtype=tf.float64), name=\'b\')\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W) + b\r\n            else:\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W)\r\n            if not activation:\r\n                one_layer = logits\r\n            elif activation == \'sigmoid\':\r\n                one_layer = tf.sigmoid(logits)\r\n            elif activation == \'relu\':\r\n                one_layer = tf.nn.relu(logits)\r\n            else:\r\n                raise Exception(f\'not support {activation} activation.\')\r\n            return one_layer\r\n    \r\n    def dnn(self, input_X, input_dim):\r\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\r\n        layer_activation.append(\"\")\r\n        for i in range(len(self.layer_units)):\r\n            if i == 0:\r\n                input_units = input_dim\r\n                previous_output = input_X\r\n            else:\r\n                input_units = self.layer_units[i-1]\r\n                previous_output = output\r\n            output = self.layer(previous_output, \r\n                                input_units, \r\n                                self.layer_units[i], \r\n                                layer_activation[i], \r\n                                layer_name=f\"Dense_{i}\")\r\n        return output\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n\r\n        return file_x, id_col\r\n    \r\n\r\n@ErrorTraceback(\"privacy_dnn_predict\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_dnn = PrivacyDnnPredict(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type = privacy_dnn.predict()\r\n    return result_path, result_type\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 06:20:29'),(2041,2,'{\"label_owner\":\"data1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.01,\"num_trees\":3,\"max_depth\":4,\"num_bins\":5,\"num_class\":2,\"lambd\":1.0,\"gamma\":0.0,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivacyXgbTrain(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy XGBoost train base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_dir = self._get_output_dir()\r\n        self.output_file = os.path.join(self.output_dir, \"model\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"label_owner\": \"data1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.01,\r\n                    \"num_trees\": 1,   # num of trees\r\n                    \"max_depth\": 3,   # max depth of per tree\r\n                    \"num_bins\": 4,    # num of bins of feature\r\n                    \"num_class\": 2,   # num of class of label\r\n                    \"lambd\": 1.0,     # L2 regular coefficient, [0, +∞)\r\n                    \"gamma\": 0.0,     # Gamma, also known as \"complexity control\", is an important parameter we use to prevent over fitting\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1, not {input_type}\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.label_owner = dynamic_parameter[\"label_owner\"]\r\n        self.label_column = dynamic_parameter[\"label_column\"]\r\n        if self.party_id == self.label_owner:\r\n            self.data_with_label = True\r\n        else:\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.1)\r\n        self.num_trees = hyperparams.get(\"num_trees\", 1)\r\n        self.max_depth = hyperparams.get(\"max_depth\", 3)\r\n        self.num_bins = hyperparams.get(\"num_bins\", 4)\r\n        self.num_class = hyperparams.get(\"num_class\", 2)\r\n        self.lambd = hyperparams.get(\"lambd\", 1.0)\r\n        self.gamma = hyperparams.get(\"gamma\", 0.0)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(epochs=(self.epochs, int),\r\n                               batch_size=(self.batch_size, int),\r\n                               learning_rate=(self.learning_rate, float),\r\n                               use_validation_set=(self.use_validation_set, bool),\r\n                               validation_set_rate=(self.validation_set_rate, float),\r\n                               predict_threshold=(self.predict_threshold, float),\r\n                               num_trees=(self.num_trees, int),\r\n                               max_depth=(self.max_depth, int),\r\n                               num_bins=(self.num_bins, int),\r\n                               num_class=(self.num_class, int),\r\n                               lambd=(self.lambd, (float, int)),\r\n                               gamma=(self.gamma, (float, int)))\r\n        assert self.epochs > 0, f\"epochs must be greater 0, not {self.epochs}\"\r\n        assert self.batch_size > 0, f\"batch_size must be greater 0, not {self.batch_size}\"\r\n        assert self.learning_rate > 0, f\"learning rate must be greater 0, not {self.learning_rate}\"\r\n        assert 0 < self.validation_set_rate < 1, f\"validattion_set_rate must be between (0,1), not {self.validation_set_rate}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"    \r\n        assert self.num_trees > 0, f\"num_trees must be greater 0, not {self.num_trees}\"\r\n        assert self.max_depth > 0, f\"max_depth must be greater 0, not {self.max_depth}\"\r\n        assert self.num_bins > 0, f\"num_bins must be greater 0, not {self.num_bins}\"\r\n        assert self.num_class > 1, f\"num_class must be greater 1, not {self.num_class}\"\r\n        assert self.lambd >= 0, f\"lambd must be greater_equal 0, not {self.lambd}\"\r\n        log.info(f\"check parameter finish.\")\r\n    \r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                    key_column=(self.key_column, str),\r\n                                    selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.data_with_label:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")                        \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx\\\r\n            = rtt.PrivateDatasetEx(data_owner=self.data_party, \r\n                                    label_owner=self.label_owner,\r\n                                    dataset_type=rtt.DatasetType.SampleAligned,\r\n                                    num_classes=self.num_class)\\\r\n                    .load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val\\\r\n                = rtt.PrivateDataset(data_owner=self.data_party,\r\n                                     label_owner=self.label_owner,\r\n                                    dataset_type=rtt.DatasetType.SampleAligned,\r\n                                    num_classes=self.num_class)\\\r\n                    .load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:\r\n            log.info(\"start build SecureXGBClassifier.\")\r\n            xgb = rtt.SecureXGBClassifier(epochs=self.epochs,\r\n                                        batch_size=self.batch_size,\r\n                                        learning_rate=self.learning_rate,\r\n                                        max_depth=self.max_depth,\r\n                                        num_trees=self.num_trees,\r\n                                        num_class=self.num_class,\r\n                                        num_bins=self.num_bins,\r\n                                        lambd=self.lambd,\r\n                                        gamma=self.gamma)\r\n            log.info(\"start train XGBoost.\")\r\n            xgb.FitEx(shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx)\r\n            log.info(\"start save model.\")\r\n            xgb.SaveModel(self.output_file)\r\n            log.info(\"save model success.\")    \r\n            if self.use_validation_set:\r\n                # predict Y\r\n                rv_pred = xgb.Reveal(xgb.Predict(shard_x_val), self.result_party)\r\n                y_shape = rv_pred.shape\r\n                log.info(f\"y_shape: {y_shape}, rv_pred: \\n {rv_pred[:10]}\")\r\n                pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\r\n                log.info(f\"pred_y: \\n {pred_y[:10]}\")\r\n                pred_y = np.array(pred_y)\r\n                pred_y.reshape(y_shape)\r\n                Y_pred = np.squeeze(pred_y, 1)\r\n                log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                # actual Y\r\n                Y_actual = xgb.Reveal(shard_y_val, self.result_party)\r\n                log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n\r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate success.\")\r\n             \r\n        result_path, result_type, evaluate_result = \"\", \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"result_party deal with the result.\")\r\n            result_path = self.output_dir\r\n            result_type = \"dir\"\r\n            if self.use_validation_set:\r\n                log.info(\"result_party evaluate model.\")\r\n                Y_pred = np.squeeze(Y_pred.astype(\"float\"))\r\n                Y_true = np.squeeze(Y_actual.astype(\"float\"))\r\n                evaluate_result = self.evaluate(Y_true, Y_pred)\r\n        \r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"train success all.\")\r\n        return result_path, result_type, evaluate_result\r\n    \r\n    def evaluate(self, Y_true, Y_pred):\r\n        from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\r\n        if self.num_class == 2:\r\n            average = \'binary\'\r\n            multi_class = \'raise\'\r\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n        else:\r\n            average = \'weighted\'\r\n            multi_class = \'ovr\'\r\n            Y_pred_class = np.argmax(Y_pred, axis=1)\r\n        auc_score = roc_auc_score(Y_true, Y_pred, multi_class=multi_class)\r\n        accuracy = accuracy_score(Y_true, Y_pred_class)\r\n        f1_score = f1_score(Y_true, Y_pred_class, average=average)\r\n        precision = precision_score(Y_true, Y_pred_class, average=average)\r\n        recall = recall_score(Y_true, Y_pred_class, average=average)\r\n        auc_score = round(auc_score, 6)\r\n        accuracy = round(accuracy, 6)\r\n        f1_score = round(f1_score, 6)\r\n        precision = round(precision, 6)\r\n        recall = round(recall, 6)\r\n        evaluate_result = {\r\n            \"AUC\": auc_score,\r\n            \"accuracy\": accuracy,\r\n            \"f1_score\": f1_score,\r\n            \"precision\": precision,\r\n            \"recall\": recall\r\n        }\r\n        log.info(f\"evaluate_result = {evaluate_result}\")\r\n        evaluate_result = json.dumps(evaluate_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluate_result\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def _get_output_dir(self):\r\n        output_dir = os.path.join(self.results_dir, \'model\')\r\n        self.mkdir(output_dir)\r\n        return output_dir\r\n\r\n\r\n@ErrorTraceback(\"privacy_xgb_train\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_xgb = PrivacyXgbTrain(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type, extra = privacy_xgb.train()\r\n    return result_path, result_type, extra\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 07:22:58'),(2042,2,'{\"model_restore_party\":\"model1\",\"hyperparams\":{\"num_trees\":3,\"max_depth\":4,\"num_bins\":5,\"num_class\":2,\"lambd\":1.0,\"gamma\":0.0,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.rosetta as rtt\r\nfrom functools import wraps\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nclass LogWithStage():\r\n    def __init__(self, name):\r\n        self.run_stage = \'init log.\'\r\n        self.logger = logging.getLogger(name)\r\n\r\n    def info(self, content):\r\n        self.run_stage = content\r\n        self.logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        self.logger.debug(content)\r\nlog = LogWithStage(__name__)\r\n\r\n\r\nclass ErrorTraceback():\r\n    def __init__(self, algo_type):\r\n        self.algo_type = algo_type\r\n    \r\n    def __call__(self, func):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                log.info(f\"start {func.__name__} function. algo: {self.algo_type}.\")\r\n                result = func(*args, **kwargs)\r\n                log.info(f\"finish {func.__name__} function. algo: {self.algo_type}.\")\r\n            except Exception as e:\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                all_error = traceback.extract_tb(exc_traceback)\r\n                error_algo_file = all_error[0].filename\r\n                error_filename = os.path.split(error_algo_file)[1]\r\n                error_lineno, error_function = [], []\r\n                for one_error in all_error:\r\n                    if one_error.filename == error_algo_file:  # only report the algo file error\r\n                        error_lineno.append(one_error.lineno)\r\n                        error_function.append(one_error.name)\r\n                error_msg = repr(e)\r\n                raise Exception(f\"<ALGO>:{self.algo_type}. <RUN_STAGE>:{log.run_stage} \"\r\n                                f\"<ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n            return result\r\n        return wrapper\r\n\r\nclass BaseAlgorithm(object):\r\n    def __init__(self,\r\n                 io_channel,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 compute_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, compute_party:{compute_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        self.check_params_type(cfg_dict=(cfg_dict, dict), data_party=(data_party, list), compute_party=(compute_party, list), \r\n                                result_party=(result_party, list), results_dir=(results_dir, str))        \r\n        log.info(f\"start get input parameter.\")\r\n        self.io_channel = io_channel\r\n        self.data_party = list(data_party)\r\n        self.compute_party = list(compute_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.parse_algo_cfg(cfg_dict)\r\n        self.check_parameters()\r\n        self.temp_dir = self.get_temp_dir()\r\n        log.info(\"finish get input parameter.\")\r\n    \r\n    def check_params_type(self, **kargs):\r\n        for key,value in kargs.items():\r\n            assert isinstance(value[0], value[1]), f\'{key} must be type({value[1]}), not {type(value[0])}\'\r\n    \r\n    def parse_algo_cfg(self, cfg_dict):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n    \r\n    def check_parameters(self):\r\n        raise NotImplementedError(f\'{sys._getframe().f_code.co_name} fuction is not implemented.\')\r\n        \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self.mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        for result party, only delete the temp dir.\r\n        for non-result party, that is data and compute party, delete the all results\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            temp_dir = self.temp_dir\r\n        else:\r\n            temp_dir = self.results_dir\r\n        self.remove_dir(temp_dir)\r\n    \r\n    def mkdir(self, directory):\r\n        if not os.path.exists(directory):\r\n            os.makedirs(directory, exist_ok=True)\r\n\r\n    def remove_dir(self, directory):\r\n        if os.path.exists(directory):\r\n            shutil.rmtree(directory)\r\n\r\n\r\nclass PrivacyXgbPredict(BaseAlgorithm):\r\n    \'\'\'\r\n    Privacy XGBoost predict base on rosetta.\r\n    \'\'\'\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.output_file = os.path.join(self.results_dir, \"result_predict.csv\")\r\n\r\n    def parse_algo_cfg(self, cfg_dict):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"model_restore_party\": \"model1\",\r\n                \"hyperparams\": {\r\n                    \"num_trees\": 1,\r\n                    \"max_depth\": 3,\r\n                    \"num_bins\": 4,\r\n                    \"num_class\": 2,\r\n                    \"lambd\": 1.0,\r\n                    \"gamma\": 0.0,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2, not {input_type}\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.model_restore_party = dynamic_parameter[\"model_restore_party\"]\r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.num_trees = hyperparams.get(\"num_trees\", 1)\r\n        self.max_depth = hyperparams.get(\"max_depth\", 3)\r\n        self.num_bins = hyperparams.get(\"num_bins\", 4)\r\n        self.num_class = hyperparams.get(\"num_class\", 2)\r\n        self.lambd = hyperparams.get(\"lambd\", 1.0)\r\n        self.gamma = hyperparams.get(\"gamma\", 0.0)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n\r\n    def check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        self._check_input_data()\r\n        self.check_params_type(model_restore_party=(self.model_restore_party, str),\r\n                               num_trees=(self.num_trees, int),\r\n                               max_depth=(self.max_depth, int),\r\n                               num_bins=(self.num_bins, int),\r\n                               num_class=(self.num_class, int),\r\n                               lambd=(self.lambd, (float, int)),\r\n                               gamma=(self.gamma, (float, int)),\r\n                               predict_threshold=(self.predict_threshold, float))\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model_path is not exist. model_path={self.model_path}\"\r\n            self.model_path = os.path.abspath(self.model_path)\r\n            self.model_file = os.path.join(self.model_path, \"model\")\r\n            assert os.path.exists(self.model_file), f\"model_file is not exist. model_file={self.model_file}\"\r\n        assert self.num_trees > 0, f\"num_trees must be greater 0, not {self.num_trees}\"\r\n        assert self.max_depth > 0, f\"max_depth must be greater 0, not {self.max_depth}\"\r\n        assert self.num_bins > 0, f\"num_bins must be greater 0, not {self.num_bins}\"\r\n        assert self.num_class > 1, f\"num_class must be greater 1, not {self.num_class}\"\r\n        assert self.lambd >= 0, f\"lambd must be greater_equal 0, not {self.lambd}\"\r\n        assert 0 <= self.predict_threshold <= 1, f\"predict threshold must be between [0,1], not {self.predict_threshold}\"\r\n        log.info(f\"check parameter finish.\")\r\n    \r\n    def _check_input_data(self):\r\n        if self.party_id in self.data_party:\r\n            self.check_params_type(data_path=(self.input_file, str), \r\n                                    key_column=(self.key_column, str),\r\n                                    selected_columns=(self.selected_columns, list))\r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", self.io_channel.channel)\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party,\r\n                                     dataset_type=rtt.DatasetType.SampleAligned,\r\n                                     num_classes=self.num_class)\\\r\n                        .load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n\r\n        xgb = rtt.SecureXGBClassifier(max_depth=self.max_depth, \r\n                                      num_trees=self.num_trees, \r\n                                      num_class=self.num_class, \r\n                                      num_bins=self.num_bins,\r\n                                      lambd=self.lambd,\r\n                                      gamma=self.gamma,\r\n                                      epochs=10,\r\n                                      batch_size=256,\r\n                                      learning_rate=0.01)\r\n        \r\n        log.info(\"start restore model.\")\r\n        if self.party_id == self.model_restore_party:\r\n            log.info(f\"model restore from: {self.model_file}.\")\r\n            xgb.LoadModel(self.model_file)\r\n        else:\r\n            log.info(\"restore model...\")\r\n            xgb.LoadModel(\"\")\r\n        log.info(\"finish restore model.\")\r\n                \r\n        # predict\r\n        predict_start_time = time.time()\r\n        rv_pred = xgb.Reveal(xgb.Predict(shard_x), self.result_party)\r\n        predict_use_time = round(time.time() - predict_start_time, 3)\r\n        log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        y_shape = rv_pred.shape\r\n        pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\r\n        pred_y = np.array(pred_y)\r\n        pred_y.reshape(y_shape)\r\n        pred_y = np.squeeze(pred_y, 1)\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        result_path, result_type = \"\", \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            Y_pred_prob = pred_y.astype(\"float\")\r\n            if self.num_class == 2:\r\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\r\n                Y_class = (Y_pred_prob > self.predict_threshold) * 1\r\n            else:\r\n                columns = [f\"Y_prob_{i}\" for i in range(Y_pred_prob.shape[1])]\r\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=columns)\r\n                Y_class = np.argmax(Y_prob, axis=1)\r\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\r\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(self.output_file, header=True, index=False)\r\n            result_path = self.output_file\r\n            result_type = \"csv\"\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n        return result_path, result_type\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n        \r\n        return file_x, id_col\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        self._mkdir(temp_dir)\r\n        return temp_dir\r\n    \r\n    def remove_temp_dir(self):\r\n        if self.party_id in self.result_party:\r\n            # only delete the temp dir\r\n            temp_dir = self.get_temp_dir()\r\n        else:\r\n            # delete the all results in the non-result party.\r\n            temp_dir = self.results_dir\r\n        self._remove_dir(temp_dir)\r\n    \r\n    def _mkdir(self, _directory):\r\n        if not os.path.exists(_directory):\r\n            os.makedirs(_directory, exist_ok=True)\r\n\r\n    def _remove_dir(self, _directory):\r\n        if os.path.exists(_directory):\r\n            shutil.rmtree(_directory)\r\n\r\n\r\n@ErrorTraceback(\"privacy_xgb_predict\")\r\ndef main(io_channel, cfg_dict: dict, data_party: list, compute_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    privacy_xgb = PrivacyXgbPredict(io_channel, cfg_dict, data_party, compute_party, result_party, results_dir)\r\n    result_path, result_type = privacy_xgb.predict()\r\n    return result_path, result_type\r\n',NULL,'2022-03-24 04:09:36','2022-05-16 07:23:14');

/*Table structure for table `mo_algorithm_variable` */

DROP TABLE IF EXISTS `mo_algorithm_variable`;

CREATE TABLE `mo_algorithm_variable` (
    `algorithm_id` bigint NOT NULL COMMENT '算法id',
    `var_key` varchar(128)  NOT NULL COMMENT '变量key',
    `var_type` tinyint NOT NULL COMMENT '变量类型. 1-boolean, 2-number, 3-string, 4-numberArray, 5-stringArray',
    `var_value` varchar(128)  NOT NULL COMMENT '变量默认值',
    `var_desc` varchar(512)  DEFAULT NULL COMMENT '变量中文描述',
    `var_desc_en` varchar(512)  DEFAULT NULL COMMENT '变量英文描述',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`,`var_key`)
) ENGINE=InnoDB  COMMENT='算法变量表';

/*Data for the table `mo_algorithm_variable` */

insert  into `mo_algorithm_variable`(`algorithm_id`,`var_key`,`var_type`,`var_value`,`var_desc`,`var_desc_en`,`create_time`,`update_time`) values (2011,'batch_size',2,'256','批量大小，大于0的整数','批量大小，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:36:02'),(2011,'epochs',2,'10','训练轮次，大于0的整数','训练轮次，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:35:33'),(2011,'learning_rate',2,'0.1','学习率，大于0的数','学习率，大于0的数','2022-03-24 06:17:20','2022-04-25 06:36:55'),(2011,'use_validation_set',1,'true','是否使用验证集，true-用，false-不用','是否使用验证集，true-用，false-不用','2022-03-24 06:17:20','2022-04-25 06:37:23'),(2011,'validation_set_rate',2,'0.2','验证集占输入数据集的比例，值域(0,1)','验证集占输入数据集的比例，值域(0,1)','2022-03-24 06:17:20','2022-04-25 06:38:15'),(2021,'batch_size',2,'256','批量大小，大于0的整数','批量大小，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:36:02'),(2021,'epochs',2,'10','训练轮次，大于0的整数','训练轮次，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:35:33'),(2021,'learning_rate',2,'0.1','学习率，大于0的数','学习率，大于0的数','2022-03-24 06:17:20','2022-04-25 06:36:55'),(2021,'predict_threshold',2,'0.5','验证集预测结果的分类阈值，值域[0,1]','验证集预测结果的分类阈值，值域[0,1]','2022-03-24 06:17:20','2022-04-25 06:39:12'),(2021,'use_validation_set',1,'true','是否使用验证集，true-用，false-不用','是否使用验证集，true-用，false-不用','2022-03-24 06:17:20','2022-04-25 06:37:23'),(2021,'validation_set_rate',2,'0.2','验证集占输入数据集的比例，值域(0,1)','验证集占输入数据集的比例，值域(0,1)','2022-03-24 06:17:20','2022-04-25 06:38:15'),(2031,'batch_size',2,'256','批量大小，大于0的整数','批量大小，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:36:02'),(2031,'epochs',2,'10','训练轮次，大于0的整数','训练轮次，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:35:33'),(2031,'init_method',3,'random_normal','指定模型参数初始化方法, 仅支持random_normal/random_uniform/zeros/ones','指定模型参数初始化方法, 仅支持random_normal/random_uniform/zeros/ones','2022-04-25 06:17:37','2022-04-25 06:40:13'),(2031,'layer_activation',5,'sigmoid,sigmoid,sigmoid,sigmoid','隐藏层与输出层的每层的激活函数, 仅支持\"sigmoid\"/\"relu\"/\"\"/null, 如果是预测，此参数配置必须与训练时的一样','隐藏层与输出层的每层的激活函数, 仅支持\"sigmoid\"/\"relu\"/\"\"/null, 如果是预测，此参数配置必须与训练时的一样','2022-04-25 06:17:01','2022-04-25 06:45:25'),(2031,'layer_units',4,'32,128,32,1','隐藏层与输出层的每层单元数，例子中有3个隐藏层,每层的单元数分别是32，128，32。输出层单元数是1。 大于0的整数，如果是预测，此参数配置必须与训练时的一样','隐藏层与输出层的每层单元数，例子中有3个隐藏层,每层的单元数分别是32，128，32。输出层单元数是1。 大于0的整数，如果是预测，此参数配置必须与训练时的一样','2022-04-25 06:16:10','2022-04-25 06:45:56'),(2031,'learning_rate',2,'0.1','学习率，大于0的数','学习率，大于0的数','2022-03-24 06:17:20','2022-04-25 06:36:55'),(2031,'optimizer',3,'sgd','优化器，暂时仅支持sgd','优化器，暂时仅支持sgd','2022-04-25 06:19:39','2022-04-25 06:46:32'),(2031,'predict_threshold',2,'0.5','验证集预测结果的分类阈值，值域[0,1]','验证集预测结果的分类阈值，值域[0,1]','2022-03-24 06:17:20','2022-04-25 06:39:12'),(2031,'use_intercept',1,'true','指定模型结构中是否使用bias, true-用，false-不用。如果是预测，此参数配置必须与训练时的一样','指定模型结构中是否使用bias, true-用，false-不用。如果是预测，此参数配置必须与训练时的一样','2022-04-25 06:19:08','2022-04-25 06:47:34'),(2031,'use_validation_set',1,'true','是否使用验证集，true-用，false-不用','是否使用验证集，true-用，false-不用','2022-03-24 06:17:20','2022-04-25 06:37:23'),(2031,'validation_set_rate',2,'0.2','验证集占输入数据集的比例，值域(0,1)','验证集占输入数据集的比例，值域(0,1)','2022-03-24 06:17:20','2022-04-25 06:38:15'),(2032,'layer_activation',5,'sigmoid,sigmoid,sigmoid,sigmoid','隐藏层与输出层的每层的激活函数, 仅支持\"sigmoid\"/\"relu\"/\"\"/null, 如果是预测，此参数配置必须与训练时的一样','隐藏层与输出层的每层的激活函数, 仅支持\"sigmoid\"/\"relu\"/\"\"/null, 如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:45:25'),(2032,'layer_units',4,'32,128,32,1','隐藏层与输出层的每层单元数，例子中有3个隐藏层,每层的单元数分别是32，128，32。输出层单元数是1。 大于0的整数，如果是预测，此参数配置必须与训练时的一样','隐藏层与输出层的每层单元数，例子中有3个隐藏层,每层的单元数分别是32，128，32。输出层单元数是1。 大于0的整数，如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:45:56'),(2032,'predict_threshold',2,'0.5','验证集预测结果的分类阈值，值域[0,1]','验证集预测结果的分类阈值，值域[0,1]','2022-03-24 06:17:20','2022-04-25 06:39:12'),(2032,'use_intercept',1,'true','指定模型结构中是否使用bias, true-用，false-不用。如果是预测，此参数配置必须与训练时的一样','指定模型结构中是否使用bias, true-用，false-不用。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:47:34'),(2041,'batch_size',2,'256','批量大小，大于0的整数','批量大小，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:36:02'),(2041,'epochs',2,'10','训练轮次，大于0的整数','训练轮次，大于0的整数','2022-03-24 06:17:20','2022-04-25 06:35:33'),(2041,'gamma',2,'0.0','复杂度控制因子，用于防止过拟合。如果是预测，此参数配置必须与训练时的一样','复杂度控制因子，用于防止过拟合。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:48:50'),(2041,'lambd',2,'1.0','L2正则项系数, [0, +∞)。如果是预测，此参数配置必须与训练时的一样','L2正则项系数, [0, +∞)。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:49:37'),(2041,'learning_rate',2,'0.1','学习率，大于0的数','学习率，大于0的数','2022-03-24 06:17:20','2022-04-25 06:36:55'),(2041,'max_depth',2,'4','树的深度，大于0的整数。如果是预测，此参数配置必须与训练时的一样','树的深度，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:54:03'),(2041,'num_bins',2,'5','特征的分箱数，大于0的整数。如果是预测，此参数配置必须与训练时的一样','特征的分箱数，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:54:48'),(2041,'num_class',2,'2','标签的类别数，大于1的整数。如果是预测，此参数配置必须与训练时的一样','标签的类别数，大于1的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:55:23'),(2041,'num_trees',2,'3','多少棵树，大于0的整数。如果是预测，此参数配置必须与训练时的一样','多少棵树，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:56:02'),(2041,'predict_threshold',2,'0.5','验证集预测结果的分类阈值，值域[0,1]','验证集预测结果的分类阈值，值域[0,1]','2022-03-24 06:17:20','2022-04-25 06:39:12'),(2041,'use_validation_set',1,'true','是否使用验证集，true-用，false-不用','是否使用验证集，true-用，false-不用','2022-03-24 06:17:20','2022-04-25 06:37:23'),(2041,'validation_set_rate',2,'0.2','验证集占输入数据集的比例，值域(0,1)','验证集占输入数据集的比例，值域(0,1)','2022-03-24 06:17:20','2022-04-25 06:38:15'),(2042,'gamma',2,'0.0','复杂度控制因子，用于防止过拟合。如果是预测，此参数配置必须与训练时的一样','复杂度控制因子，用于防止过拟合。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:48:50'),(2042,'lambd',2,'1.0','L2正则项系数, [0, +∞)。如果是预测，此参数配置必须与训练时的一样','L2正则项系数, [0, +∞)。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:49:37'),(2042,'max_depth',2,'4','树的深度，大于0的整数。如果是预测，此参数配置必须与训练时的一样','树的深度，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:54:03'),(2042,'num_bins',2,'5','特征的分箱数，大于0的整数。如果是预测，此参数配置必须与训练时的一样','特征的分箱数，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:54:48'),(2042,'num_class',2,'2','标签的类别数，大于1的整数。如果是预测，此参数配置必须与训练时的一样','标签的类别数，大于1的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:55:23'),(2042,'num_trees',2,'3','多少棵树，大于0的整数。如果是预测，此参数配置必须与训练时的一样','多少棵树，大于0的整数。如果是预测，此参数配置必须与训练时的一样','2022-03-24 06:17:20','2022-04-25 06:56:02'),(2042,'predict_threshold',2,'0.5','验证集预测结果的分类阈值，值域[0,1]','验证集预测结果的分类阈值，值域[0,1]','2022-03-24 06:17:20','2022-04-25 06:39:12');

/*Table structure for table `mo_calculation_process` */

DROP TABLE IF EXISTS `mo_calculation_process`;

CREATE TABLE `mo_calculation_process` (
    `calculation_process_id` bigint NOT NULL AUTO_INCREMENT COMMENT '计算流程ID',
    `name` varchar(200)  NOT NULL COMMENT '计算流程中文名字',
    `name_en` varchar(200)  DEFAULT NULL COMMENT '计算流程英文名字',
    `status` tinyint NOT NULL DEFAULT '1' COMMENT '状态: 0-无效，1- 有效',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`calculation_process_id`)
) ENGINE=InnoDB AUTO_INCREMENT=5  COMMENT='计算流程配置表';

/*Data for the table `mo_calculation_process` */

insert  into `mo_calculation_process`(`calculation_process_id`,`name`,`name_en`,`status`,`create_time`,`update_time`) values (1,'训练','Train',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(2,'预测','Predict',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(3,'训练，并预测','Train and then predict',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(4,'PSI','PSI',1,'2022-03-24 08:17:01','2022-03-24 08:17:01');

/*Table structure for table `mo_calculation_process_algorithm` */

DROP TABLE IF EXISTS `mo_calculation_process_algorithm`;

CREATE TABLE `mo_calculation_process_algorithm` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `algorithm_id` bigint NOT NULL COMMENT '算法ID',
    PRIMARY KEY (`algorithm_id`,`calculation_process_id`)
) ENGINE=InnoDB  COMMENT='计算流程配置和算法关系表';

/*Data for the table `mo_calculation_process_algorithm` */

insert  into `mo_calculation_process_algorithm`(`calculation_process_id`,`algorithm_id`) values (4,1001),(1,2010),(2,2010),(3,2010),(1,2020),(2,2020),(3,2020),(1,2030),(2,2030),(3,2030),(1,2040),(2,2040),(3,2040);

/*Table structure for table `mo_calculation_process_step` */

DROP TABLE IF EXISTS `mo_calculation_process_step`;

CREATE TABLE `mo_calculation_process_step` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `step` int NOT NULL COMMENT '步骤. 从0开始',
    `type` int NOT NULL COMMENT '部署说明. 0-选择训练输入数据, 1-选择预测输入数据, 2-选择PSI输入数据, 3-选择计算环境(通用), 4-选择计算环境(训练&预测), 5-选择结果接收方(通用), 6-选择结果接收方(训练&预测)',
    `task_1_step` int DEFAULT NULL COMMENT '任务1对应的步骤',
    `task_2_step` int DEFAULT NULL COMMENT '任务2对应的步骤',
    `task_3_step` int DEFAULT NULL COMMENT '任务3对应的步骤',
    `task_4_step` int DEFAULT NULL COMMENT '任务4对应的步骤',
    PRIMARY KEY (`calculation_process_id`,`step`)
) ENGINE=InnoDB  COMMENT='计算流程配置步骤表';

/*Data for the table `mo_calculation_process_step` */

insert  into `mo_calculation_process_step`(`calculation_process_id`,`step`,`type`,`task_1_step`,`task_2_step`,`task_3_step`,`task_4_step`) values (1,1,0,1,2,NULL,NULL),(1,2,3,1,2,NULL,NULL),(1,3,5,1,2,NULL,NULL),(2,1,1,1,2,NULL,NULL),(2,2,3,1,2,NULL,NULL),(2,3,5,1,2,NULL,NULL),(3,1,0,1,2,NULL,NULL),(3,2,1,3,4,NULL,NULL),(3,3,4,1,2,3,4),(3,4,6,1,2,3,4),(4,1,2,NULL,1,NULL,NULL),(4,2,3,NULL,1,NULL,NULL),(4,3,5,NULL,1,NULL,NULL);

/*Table structure for table `mo_calculation_process_task` */

DROP TABLE IF EXISTS `mo_calculation_process_task`;

CREATE TABLE `mo_calculation_process_task` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `step` int NOT NULL COMMENT '任务步骤',
    `algorithm_select` int NOT NULL COMMENT '算法选择方式. 0-用户输入母算法; 1-用户输入子训练算法, 2-用户输入子预测算法, 3-内置PSI算法',
    `input_model_step` int DEFAULT NULL COMMENT '工作流节点需要的模型产生的步骤',
    `input_psi_step` int DEFAULT NULL COMMENT '工作流节点需要的psi产生步骤',
    PRIMARY KEY (`calculation_process_id`,`step`)
) ENGINE=InnoDB  COMMENT='计算流程任务步骤表';

/*Data for the table `mo_calculation_process_task` */

insert  into `mo_calculation_process_task`(`calculation_process_id`,`step`,`algorithm_select`,`input_model_step`,`input_psi_step`) values (1,1,3,NULL,NULL),(1,2,1,NULL,1),(2,1,3,NULL,NULL),(2,2,2,NULL,1),(3,1,3,NULL,NULL),(3,2,1,NULL,1),(3,3,3,NULL,NULL),(3,4,2,2,3),(4,1,0,NULL,NULL);

/*Table structure for table `mo_data_sync` */

DROP TABLE IF EXISTS `mo_data_sync`;

CREATE TABLE `mo_data_sync` (
    `data_type` varchar(256)  NOT NULL COMMENT '数据类型',
    `latest_synced` bigint NOT NULL DEFAULT '0' COMMENT '数据最新同步时间戳，精确到毫秒',
    `info` varchar(50)  DEFAULT '' COMMENT '描述',
    PRIMARY KEY (`data_type`)
) ENGINE=InnoDB  COMMENT='数据同步时间记录';

/*Data for the table `mo_data_sync` */

/*Table structure for table `mo_model` */

DROP TABLE IF EXISTS `mo_model`;

CREATE TABLE `mo_model` (
    `meta_data_id` varchar(128)  NOT NULL COMMENT '模型元数据id',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `name` varchar(128)  DEFAULT NULL COMMENT '名称',
    `file_id` varchar(256)  DEFAULT NULL COMMENT '源文件ID',
    `file_path` varchar(128)  NOT NULL COMMENT '源文件存放路径',
    `train_task_id` varchar(256)  NOT NULL COMMENT '训练模型的任务id',
    `supported_algorithm_id` bigint DEFAULT NULL COMMENT '输入模型的算法id',
    `train_algorithm_id` bigint DEFAULT NULL COMMENT '训练模型的算法id',
    `train_user_address` varchar(64)  DEFAULT NULL COMMENT '训练模型的账户',
    `evaluate` varchar(2048)  DEFAULT NULL COMMENT '模型评估结果',
    `data_type` int NOT NULL DEFAULT '0' COMMENT '原始数据的类型',
    `metadata_option` text  COMMENT '元数据的选项，和 data_type 配套使用',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`)
) ENGINE=InnoDB  COMMENT='模型表';

/*Data for the table `mo_model` */

/*Table structure for table `mo_org_expand` */

DROP TABLE IF EXISTS `mo_org_expand`;

CREATE TABLE `mo_org_expand` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `identity_ip` varchar(20)  NOT NULL COMMENT '组织的ip',
    `identity_port` int NOT NULL COMMENT '组织的端口',
    `is_public` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否公共组织: 0-否，1-是',
    `observer_proxy_wallet_address` varchar(100)  DEFAULT NULL COMMENT '当前组织内置系统钱包地址 (见证人代理钱包)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`)
) ENGINE=InnoDB  COMMENT='组织扩展表';

/*Data for the table `mo_org_expand` */

/*Table structure for table `mo_org_user` */

DROP TABLE IF EXISTS `mo_org_user`;

CREATE TABLE `mo_org_user` (
    `identity_id` varchar(128)  NOT NULL COMMENT '组织的身份标识Id',
    `address` varchar(64)  NOT NULL COMMENT '用户的钱包地址',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`,`address`)
) ENGINE=InnoDB  COMMENT='组织管理用户';

/*Data for the table `mo_org_user` */

/*Table structure for table `mo_psi` */

DROP TABLE IF EXISTS `mo_psi`;

CREATE TABLE `mo_psi` (
    `meta_data_id` varchar(128)  NOT NULL COMMENT '模型元数据id',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `name` varchar(128)  DEFAULT NULL COMMENT '名称',
    `file_id` varchar(256)  DEFAULT NULL COMMENT '源文件ID',
    `file_path` varchar(128)  NOT NULL COMMENT '源文件存放路径',
    `train_task_id` varchar(256)  NOT NULL COMMENT '训练模型的任务id',
    `train_algorithm_id` bigint DEFAULT NULL COMMENT '训练模型的算法id, 母算法',
    `train_user_address` varchar(64)  DEFAULT NULL COMMENT '训练模型的账户',
    `data_type` int DEFAULT '0' COMMENT '原始数据的类型',
    `metadata_option` text  COMMENT '元数据的选项，和 data_type 配套使用',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`)
) ENGINE=InnoDB  COMMENT='PSI结果表';

/*Data for the table `mo_psi` */

/*Table structure for table `mo_stats_global` */

DROP TABLE IF EXISTS `mo_stats_global`;

CREATE TABLE `mo_stats_global` (
    `id` bigint NOT NULL COMMENT 'ID(自增长)',
    `task_count` int DEFAULT '0' COMMENT '隐私计算总次数(总的任务数,包括成功和失败的)',
    `address_count_of_task` int DEFAULT '0' COMMENT '参与计算地址总数',
    `address_count_of_active` int DEFAULT '0' COMMENT '24h活跃地址总数',
    `data_size` bigint DEFAULT '0' COMMENT '全网数据总量，单位：字节',
    `data_token_count` int DEFAULT '0' COMMENT '数据凭证数量',
    `data_token_used` bigint DEFAULT '0' COMMENT '数据凭证使用量',
    `total_core` int DEFAULT '0' COMMENT '全网目前的算力核心数，单位：个',
    `total_memory` bigint DEFAULT '0' COMMENT '全网目前的算力内存数，单位：字节',
    `total_bandwidth` bigint DEFAULT '0' COMMENT '全网目前的带宽，单位：字节',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='全网维度统计';

/*Data for the table `mo_stats_global` */

/*Table structure for table `mo_stats_org` */

DROP TABLE IF EXISTS `mo_stats_org`;

CREATE TABLE `mo_stats_org` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `org_total_core` int DEFAULT '0' COMMENT '全网目前的算力核心数，单位：个',
    `org_total_memory` bigint DEFAULT '0' COMMENT '全网目前的算力内存数，单位：字节',
    `org_total_bandwidth` bigint DEFAULT '0' COMMENT '全网目前的带宽，单位：字节',
    `computing_power_ratio` int DEFAULT '0' COMMENT '算力占比. 万分比。100 = 1%',
    `total_task` bigint DEFAULT '0' COMMENT '参与任务数量',
    `total_data` bigint DEFAULT '0' COMMENT '数据数',
    `total_data_token` bigint DEFAULT '0' COMMENT '凭证数',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`)
) ENGINE=InnoDB  COMMENT='组织维度统计';

/*Data for the table `mo_stats_org` */

/*Table structure for table `mo_stats_token` */

DROP TABLE IF EXISTS `mo_stats_token`;

CREATE TABLE `mo_stats_token` (
    `address` varchar(200)  NOT NULL COMMENT '凭证地址',
    `token_used` bigint DEFAULT '0' COMMENT '数据凭证使用量',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`address`)
) ENGINE=InnoDB  COMMENT='数据维度统计';

/*Data for the table `mo_stats_token` */

/*Table structure for table `mo_task_expand` */

DROP TABLE IF EXISTS `mo_task_expand`;

CREATE TABLE `mo_task_expand` (
    `id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `event_synced` tinyint(1) NOT NULL DEFAULT '0' COMMENT '事件是否同步完成 0-否 1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='任务扩展表';

/*Data for the table `mo_task_expand` */

/*Table structure for table `mo_token` */

DROP TABLE IF EXISTS `mo_token`;

CREATE TABLE `mo_token` (
    `address` varchar(64)  NOT NULL COMMENT '合约地址',
    `name` varchar(64)  DEFAULT NULL COMMENT '合约名称',
    `symbol` varchar(64)  DEFAULT NULL COMMENT '合约符号',
    `decimal` bigint DEFAULT NULL COMMENT '合约精度',
    `price` varchar(128)  DEFAULT NULL COMMENT 'LAT的价格',
    `is_add_liquidity` tinyint NOT NULL DEFAULT '0' COMMENT '是否添加流动性: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`address`)
) ENGINE=InnoDB  COMMENT='token信息';

/*Data for the table `mo_token` */

/*Table structure for table `mo_token_holder` */

DROP TABLE IF EXISTS `mo_token_holder`;

CREATE TABLE `mo_token_holder` (
    `token_address` varchar(64)  NOT NULL COMMENT '合约地址',
    `address` varchar(64)  NOT NULL COMMENT '用户地址',
    `balance` varchar(128)  DEFAULT NULL COMMENT '地址代币余额, ERC20为金额',
    `authorize_balance` varchar(128)  DEFAULT NULL COMMENT '地址已授权支付助手的代币余额, ERC20为金额',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`token_address`,`address`)
) ENGINE=InnoDB  COMMENT='token持有表';

/*Data for the table `mo_token_holder` */

/*Table structure for table `mo_user` */

DROP TABLE IF EXISTS `mo_user`;

CREATE TABLE `mo_user` (
    `address` varchar(64)  NOT NULL COMMENT '用户钱包地址',
    `user_name` varchar(64)  NOT NULL COMMENT '用户名',
    `is_valid` tinyint NOT NULL DEFAULT '1' COMMENT '是否有效: 0-否，1-是',
    `org_identity_id` varchar(128)  DEFAULT NULL COMMENT '默认连接的组织id',
    `heart_beat_time` timestamp NULL DEFAULT NULL COMMENT '最后的心跳时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`address`),
    UNIQUE KEY `UK_USERNAME` (`user_name`) USING BTREE COMMENT '用户名称唯一'
) ENGINE=InnoDB  COMMENT='用户表';

/*Data for the table `mo_user` */

/*Table structure for table `mo_user_login` */

DROP TABLE IF EXISTS `mo_user_login`;

CREATE TABLE `mo_user_login` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '日志表id(自增长)',
    `address` varchar(64)  NOT NULL COMMENT '登录地址',
    `is_success` tinyint NOT NULL DEFAULT '1' COMMENT '是否成功: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='用户登录日志表';

/*Data for the table `mo_user_login` */

/*Table structure for table `mo_workflow` */

DROP TABLE IF EXISTS `mo_workflow`;

CREATE TABLE `mo_workflow` (
    `workflow_id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流ID(自增长)',
    `create_mode` tinyint NOT NULL COMMENT '创建模式:1-专家模式,2-向导模式',
    `address` varchar(64)  NOT NULL COMMENT '用户地址',
    `workflow_name` varchar(128)  NOT NULL COMMENT '工作流名称',
    `workflow_desc` varchar(200)  DEFAULT NULL COMMENT '工作流描述',
    `algorithm_id` bigint DEFAULT NULL COMMENT '算法id',
    `algorithm_name` varchar(200)  DEFAULT NULL COMMENT '算法名称',
    `calculation_process_id` bigint DEFAULT NULL COMMENT '计算流程id',
    `calculation_process_name` varchar(200)  DEFAULT NULL COMMENT '计算流程名称',
    `last_run_time` timestamp(3) NULL DEFAULT NULL COMMENT '最后运行时间',
    `is_setting_completed` tinyint NOT NULL DEFAULT '0' COMMENT '是否设置完成:  0-否  1-是',
    `calculation_process_step` int DEFAULT NULL COMMENT '向导模式下当前步骤',
    `is_delete` tinyint NOT NULL DEFAULT '0' COMMENT '是否删除: 0-否  1-是',
    `workflow_version` bigint DEFAULT '1' COMMENT '当前最大版本号,从1开始',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_id`),
    KEY `user_address` (`address`)
) ENGINE=InnoDB  COMMENT='工作流表';

/*Data for the table `mo_workflow` */

/*Table structure for table `mo_workflow_run_status` */

DROP TABLE IF EXISTS `mo_workflow_run_status`;

CREATE TABLE `mo_workflow_run_status` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流节点表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` int NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `sign` varchar(512)  NOT NULL COMMENT '发起任务的账户的签名',
    `address` varchar(64)  NOT NULL COMMENT '发起任务的账户的地址',
    `step` int DEFAULT NULL COMMENT '总步骤',
    `cur_step` int DEFAULT NULL COMMENT '当前步骤',
    `begin_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '开始时间',
    `end_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '结束时间',
    `run_status` tinyint DEFAULT NULL COMMENT '运行状态: 1-运行中,2-运行成功,3-运行失败',
    `cancel_status` tinyint DEFAULT NULL COMMENT '取消状态: 1-取消中,2-取消成功,3-取消失败',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流运行状态';

/*Data for the table `mo_workflow_run_status` */

/*Table structure for table `mo_workflow_run_task_result` */

DROP TABLE IF EXISTS `mo_workflow_run_task_result`;

CREATE TABLE `mo_workflow_run_task_result` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'ID(自增长)',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `task_id` varchar(256)  DEFAULT NULL COMMENT '任务ID,底层处理完成后返回',
    `file_name` varchar(128)  NOT NULL COMMENT '任务结果文件的名称',
    `metadata_id` varchar(128)  NOT NULL COMMENT '任务结果文件的元数据Id <系统默认生成的元数据>',
    `origin_id` varchar(128)  NOT NULL COMMENT '任务结果文件的原始文件Id',
    `ip` varchar(32)  NOT NULL COMMENT '任务结果文件所在的 数据服务内网ip',
    `port` varchar(8)  NOT NULL COMMENT '任务结果文件所在的 数据服务内网port',
    `data_type` int NOT NULL DEFAULT '0' COMMENT '原始数据类型 (文件类型)',
    `metadata_option` text  COMMENT '元数据的选项，和 data_type 配套使用',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流任务运行状态';

/*Data for the table `mo_workflow_run_task_result` */

/*Table structure for table `mo_workflow_run_task_status` */

DROP TABLE IF EXISTS `mo_workflow_run_task_status`;

CREATE TABLE `mo_workflow_run_task_status` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'ID(自增长)',
    `workflow_run_id` bigint NOT NULL COMMENT '工作流运行状态ID',
    `workflow_task_id` bigint NOT NULL COMMENT '工作流节点配置ID',
    `step` int DEFAULT NULL COMMENT '节点在工作流中序号,从1开始',
    `begin_time` datetime DEFAULT NULL COMMENT '开始时间',
    `end_time` datetime DEFAULT NULL COMMENT '结束时间',
    `run_status` tinyint DEFAULT NULL COMMENT '运行状态: :0-未开始 1-运行中,2-运行成功,3-运行失败',
    `task_id` varchar(256)  DEFAULT NULL COMMENT '任务ID,底层处理完成后返回',
    `run_msg` varchar(256)  DEFAULT NULL COMMENT '任务处理结果描述',
    `model_id` varchar(128)  DEFAULT '0' COMMENT '工作流节点需要的模型id',
    `psi_id` varchar(128)  DEFAULT '0' COMMENT '工作流节点需要的psi的id',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流任务运行状态';

/*Data for the table `mo_workflow_run_task_status` */

/*Table structure for table `mo_workflow_setting_expert` */

DROP TABLE IF EXISTS `mo_workflow_setting_expert`;

CREATE TABLE `mo_workflow_setting_expert` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流节点表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `node_step` int DEFAULT '1' COMMENT '工作流中节点的顺序,从1开始',
    `node_name` varchar(30)  DEFAULT NULL COMMENT '节点名称',
    `psi_task_step` int DEFAULT NULL COMMENT '工作流任务步骤',
    `task_step` int NOT NULL COMMENT '工作流任务步骤',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_NODE_STEP` (`workflow_id`,`workflow_version`,`node_step`)
) ENGINE=InnoDB  COMMENT='工作流专家模式节点表';

/*Data for the table `mo_workflow_setting_expert` */

/*Table structure for table `mo_workflow_setting_wizard` */

DROP TABLE IF EXISTS `mo_workflow_setting_wizard`;

CREATE TABLE `mo_workflow_setting_wizard` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流步骤表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `step` int DEFAULT '1' COMMENT '当前步骤,从1开始',
    `calculation_process_step_type` int DEFAULT '1' COMMENT '部署说明. 0-选择训练输入数据, 1-选择预测输入数据, 2-选择PSI输入数据, 3-选择计算环境(通用), 4-选择计算环境(训练&预测), 5-选择结果接收方(通用), 6-选择结果接收方(训练&预测)',
    `task_1_step` int DEFAULT NULL COMMENT '任务1对应的步骤',
    `task_2_step` int DEFAULT NULL COMMENT '任务2对应的步骤',
    `task_3_step` int DEFAULT NULL COMMENT '任务3对应的步骤',
    `task_4_step` int DEFAULT NULL COMMENT '任务4对应的步骤',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_NODE_STEP` (`workflow_id`,`workflow_version`,`step`)
) ENGINE=InnoDB  COMMENT='工作流向导模式步骤表';

/*Data for the table `mo_workflow_setting_wizard` */

/*Table structure for table `mo_workflow_task` */

DROP TABLE IF EXISTS `mo_workflow_task`;

CREATE TABLE `mo_workflow_task` (
    `workflow_task_id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流任务配置id',
    `workflow_id` bigint NOT NULL COMMENT '工作流ID',
    `workflow_version` bigint DEFAULT '1' COMMENT '编辑版本标识,从1开始',
    `step` int DEFAULT '1' COMMENT '工作流中任务的顺序,从1开始',
    `algorithm_id` bigint DEFAULT NULL COMMENT '算法id',
    `identity_id` varchar(128)  DEFAULT NULL COMMENT '任务发启放组织id',
    `input_model` int NOT NULL DEFAULT '0' COMMENT '是否需要输入模型: 0-否，1:是',
    `input_model_id` varchar(128)  DEFAULT NULL COMMENT '工作流节点需要的模型id',
    `input_model_step` int DEFAULT NULL COMMENT '工作流节点需要的模型产生的步骤',
    `input_psi` int NOT NULL DEFAULT '0' COMMENT '是否需要输入PSI: 0-否，1:是',
    `input_psi_step` int DEFAULT NULL COMMENT '工作流节点需要的psi产生步骤',
    `enable` tinyint NOT NULL DEFAULT '1' COMMENT '是否可用: 0-否，1:是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`),
    UNIQUE KEY `UK_WF_TASK` (`workflow_id`,`workflow_version`,`step`)
) ENGINE=InnoDB  COMMENT='工作流任务配置表';

/*Data for the table `mo_workflow_task` */

/*Table structure for table `mo_workflow_task_input` */

DROP TABLE IF EXISTS `mo_workflow_task_input`;

CREATE TABLE `mo_workflow_task_input` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `meta_data_id` varchar(128)  NOT NULL COMMENT '数据表ID',
    `identity_id` varchar(128)  NOT NULL COMMENT '组织的身份标识Id',
    `key_column` bigint DEFAULT NULL COMMENT 'ID列(列索引)(存id值)',
    `dependent_variable` bigint DEFAULT NULL COMMENT '因变量(标签)(存id值)',
    `data_column_ids` varchar(1024)  DEFAULT NULL COMMENT '数据字段ID索引(存id值)',
    `party_id` varchar(64)  DEFAULT NULL COMMENT '任务里面定义的 (p0 -> pN 方 ...)',
    `sort_key` int NOT NULL DEFAULT '0' COMMENT '用于排序的字段',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`meta_data_id`)
) ENGINE=InnoDB  COMMENT='工作流任务输入表';

/*Data for the table `mo_workflow_task_input` */

/*Table structure for table `mo_workflow_task_output` */

DROP TABLE IF EXISTS `mo_workflow_task_output`;

CREATE TABLE `mo_workflow_task_output` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `identity_id` varchar(128)  NOT NULL COMMENT '协同方组织的身份标识Id',
    `store_pattern` tinyint DEFAULT '1' COMMENT '存储形式: 1-明文，2:密文',
    `output_content` text  COMMENT '输出内容',
    `party_id` varchar(64)  DEFAULT NULL COMMENT '任务里面定义的 (q0 -> qN 方 ...)',
    `sort_key` int NOT NULL DEFAULT '0' COMMENT '用于排序的字段',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`identity_id`)
) ENGINE=InnoDB  COMMENT='项目工作流节点输出表';

/*Data for the table `mo_workflow_task_output` */

/*Table structure for table `mo_workflow_task_resource` */

DROP TABLE IF EXISTS `mo_workflow_task_resource`;

CREATE TABLE `mo_workflow_task_resource` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `cost_mem` bigint DEFAULT NULL COMMENT '所需的内存 (单位: byte)',
    `cost_cpu` int DEFAULT NULL COMMENT '所需的核数 (单位: 个)',
    `cost_gpu` int DEFAULT NULL COMMENT 'GPU核数(单位：核)',
    `cost_bandwidth` bigint DEFAULT '0' COMMENT '所需的带宽 (单位: bps)',
    `run_time` bigint DEFAULT NULL COMMENT '所需的运行时长 (单位: ms)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`)
) ENGINE=InnoDB  COMMENT='工作流任务资源表';

/*Data for the table `mo_workflow_task_resource` */

/*Table structure for table `mo_workflow_task_variable` */

DROP TABLE IF EXISTS `mo_workflow_task_variable`;

CREATE TABLE `mo_workflow_task_variable` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `var_key` varchar(128)  NOT NULL COMMENT '变量key',
    `var_value` varchar(128)  NOT NULL COMMENT '变量值',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`var_key`)
) ENGINE=InnoDB  COMMENT='工作流任务变量表';

/*Data for the table `mo_workflow_task_variable` */

/*Table structure for table `mo_workflow_version` */

DROP TABLE IF EXISTS `mo_workflow_version`;

CREATE TABLE `mo_workflow_version` (
    `workflow_id` bigint NOT NULL COMMENT '工作流ID',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '编辑版本标识,从1开始',
    `workflow_version_name` varchar(128)  DEFAULT NULL COMMENT '工作流版本名称',
    `status` tinyint NOT NULL DEFAULT '0' COMMENT '状态: 0-待支付、1-支付中、2-已支付、3-运行中、4-运行成功、5-运行失败',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_id`,`workflow_version`)
) ENGINE=InnoDB  COMMENT='工作流不同版本设置表';

/*Data for the table `mo_workflow_version` */
