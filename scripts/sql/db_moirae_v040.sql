CREATE DATABASE IF NOT EXISTS `db_moirae_cd` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

USE `db_moirae_cd`;

/*Table structure for table `dc_meta_data` */

DROP TABLE IF EXISTS `dc_meta_data`;

CREATE TABLE `dc_meta_data` (
    `meta_data_id` varchar(200)  NOT NULL COMMENT '元数据ID,hash',
    `meta_data_type` tinyint NOT NULL DEFAULT '1' COMMENT '表示该元数据是 `普通数据` 还是 `模型数据` 的元数据 (0: 未定义; 1: 普通数据元数据; 2: 模型数据元数据)',
    `file_name` varchar(100)  NOT NULL COMMENT '文件名称',
    `identity_id` varchar(200)  NOT NULL COMMENT '组织身份ID',
    `file_type` int NOT NULL COMMENT '文件后缀/类型, 0:未知; 1:csv',
    `industry` varchar(100)  DEFAULT NULL COMMENT '行业名称',
    `published_at` datetime(3) NOT NULL COMMENT '发布时间，精确到毫秒',
    `remarks` varchar(100)  DEFAULT NULL COMMENT '数据描述',
    `status` int DEFAULT NULL COMMENT '元数据的状态 (0: 未知; 1: 还未发布的新表; 2: 已发布的表; 3: 已撤销的表)',
    `token_address` varchar(100)  DEFAULT NULL COMMENT '对应合约的地址',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `location_type` tinyint NOT NULL DEFAULT '1' COMMENT '源数据的存储位置类型 (组织本地服务器、远端服务器、云等)：0-未知，1-存储在组织本地服务器上，2-存储在远端服务器上',
    `nonce` int DEFAULT '0' COMMENT '元数据的 nonce (用来标识该元数据在所属组织中的元数据的序号, 从 0 开始递增)',
    `allow_expose` tinyint(1) DEFAULT '0' COMMENT '是否可以被曝光 (1: 可以; 0: 不可以; 如 数据原始内容可以被下载或者支持外域查看时则为1, 默认为0)',
    `origin_id` varchar(200)  NOT NULL COMMENT '数据文件ID,hash',
    `file_path` varchar(100)  NOT NULL COMMENT '文件存储路径',
    `rows` int NOT NULL DEFAULT '0' COMMENT '数据行数(不算title)',
    `size` bigint NOT NULL DEFAULT '0' COMMENT '文件大小(字节)',
    `columns` int NOT NULL DEFAULT '0' COMMENT '数据列数',
    `has_title` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否带标题',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='数据文件信息';

/*Table structure for table `dc_meta_data_column` */

DROP TABLE IF EXISTS `dc_meta_data_column`;

CREATE TABLE `dc_meta_data_column` (
    `meta_data_id` varchar(200)  NOT NULL COMMENT '元数据ID,hash',
    `column_idx` int NOT NULL COMMENT '字段索引序号',
    `column_name` varchar(100)  NOT NULL COMMENT '字段名称',
    `column_type` varchar(100)  NOT NULL COMMENT '字段类型',
    `column_size` int NOT NULL DEFAULT '0' COMMENT '字段大小',
    `remarks` varchar(100)  DEFAULT NULL COMMENT '字段描述',
    PRIMARY KEY (`meta_data_id`,`column_idx`)
) ENGINE=InnoDB  COMMENT='数据文件元数据信息';

/*Table structure for table `dc_org` */

DROP TABLE IF EXISTS `dc_org`;

CREATE TABLE `dc_org` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `node_name` varchar(100)  DEFAULT NULL COMMENT '组织身份名称',
    `node_id` varchar(200)  NOT NULL COMMENT '组织节点ID',
    `image_url` varchar(256)  DEFAULT NULL COMMENT '组织机构图像url',
    `details` varchar(256)  DEFAULT NULL COMMENT '组织机构简介',
    `status` int NOT NULL DEFAULT '1' COMMENT '状态,1-Normal; 2-NonNormal',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='数据中心组织信息';

/*Table structure for table `dc_power_server` */

DROP TABLE IF EXISTS `dc_power_server`;

CREATE TABLE `dc_power_server` (
    `id` varchar(200)  NOT NULL COMMENT '计算服务主机ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '组织身份ID',
    `memory` bigint NOT NULL DEFAULT '0' COMMENT '计算服务内存, 字节',
    `core` int NOT NULL DEFAULT '0' COMMENT '计算服务core',
    `bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '计算服务带宽, bps',
    `used_memory` bigint DEFAULT '0' COMMENT '使用的内存, 字节',
    `used_core` int DEFAULT '0' COMMENT '使用的core',
    `used_bandwidth` bigint DEFAULT '0' COMMENT '使用的带宽, bps',
    `published` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否发布，true/false',
    `published_at` datetime(3) NOT NULL COMMENT '发布时间，精确到毫秒',
    `status` int DEFAULT NULL COMMENT '算力的状态 (0: 未知; 1: 还未发布的算力; 2: 已发布的算力(算力未被占用); 3: 已发布的算力(算力正在被占用); 4: 已撤销的算力)',
    `update_at` timestamp(3) NOT NULL COMMENT '(状态)修改时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    KEY `update_at` (`update_at`)
) ENGINE=InnoDB  COMMENT='计算服务信息';

/*Table structure for table `dc_task` */

DROP TABLE IF EXISTS `dc_task`;

CREATE TABLE `dc_task` (
    `id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `task_name` varchar(100)  NOT NULL COMMENT '任务名称',
    `user_id` varchar(200)  NOT NULL COMMENT '发起任务的用户的信息 (task是属于用户的)',
    `user_type` int NOT NULL COMMENT '用户类型 (0: 未定义; 1: 以太坊地址; 2: Alaya地址; 3: PlatON地址',
    `required_memory` bigint NOT NULL DEFAULT '0' COMMENT '需要的内存, 字节',
    `required_core` int NOT NULL DEFAULT '0' COMMENT '需要的core',
    `required_bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '需要的带宽, bps',
    `required_duration` bigint NOT NULL DEFAULT '0' COMMENT '需要的时间, milli seconds',
    `owner_identity_id` varchar(200)  NOT NULL COMMENT '任务创建者组织身份ID',
    `owner_party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `create_at` datetime(3) NOT NULL COMMENT '任务创建时间，精确到毫秒',
    `start_at` datetime(3) DEFAULT NULL COMMENT '任务开始执行时间，精确到毫秒',
    `end_at` datetime(3) DEFAULT NULL COMMENT '任务结束时间，精确到毫秒',
    `used_memory` bigint NOT NULL DEFAULT '0' COMMENT '使用的内存, 字节',
    `used_core` int NOT NULL DEFAULT '0' COMMENT '使用的core',
    `used_bandwidth` bigint NOT NULL DEFAULT '0' COMMENT '使用的带宽, bps',
    `used_file_size` bigint DEFAULT '0' COMMENT '使用的所有数据大小，字节',
    `status` int DEFAULT NULL COMMENT '任务状态, 0:未知;1:等待中;2:计算中,3:失败;4:成功',
    `status_desc` varchar(255)  DEFAULT NULL COMMENT '任务状态说明',
    `remarks` varchar(255)  DEFAULT NULL COMMENT '任务描述',
    `task_sign` varchar(1024)  DEFAULT NULL COMMENT '任务签名',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    KEY `end_at` (`end_at`)
) ENGINE=InnoDB  COMMENT='任务';

/*Table structure for table `dc_task_algo_provider` */

DROP TABLE IF EXISTS `dc_task_algo_provider`;

CREATE TABLE `dc_task_algo_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '算法提供者组织身份ID',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    PRIMARY KEY (`task_id`)
) ENGINE=InnoDB  COMMENT='任务算法提供者';

/*Table structure for table `dc_task_data_provider` */

DROP TABLE IF EXISTS `dc_task_data_provider`;

CREATE TABLE `dc_task_data_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `meta_data_id` varchar(200)  NOT NULL COMMENT '参与任务的元数据ID',
    `identity_id` varchar(200)  NOT NULL COMMENT '(冗余)参与任务的元数据的所属组织的identity_id',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `key_column_idx` int DEFAULT NULL COMMENT '元数据在此次任务中的主键列下标索引序号',
    PRIMARY KEY (`task_id`,`meta_data_id`)
) ENGINE=InnoDB  COMMENT='任务数据提供者（数据和模型）';

/*Table structure for table `dc_task_event` */

DROP TABLE IF EXISTS `dc_task_event`;

CREATE TABLE `dc_task_event` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'id',
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `event_type` varchar(20)  NOT NULL COMMENT '事件类型',
    `identity_id` varchar(200)  NOT NULL COMMENT '产生事件的组织身份ID',
    `party_id` varchar(200)  NOT NULL COMMENT '产生事件的partyId (单个组织可以担任任务的多个party, 区分是哪一方产生的event)',
    `event_at` datetime(3) NOT NULL COMMENT '产生事件的时间，精确到毫秒',
    `event_content` varchar(1024)  NOT NULL COMMENT '事件内容',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='任务事件';

/*Table structure for table `dc_task_meta_data_column` */

DROP TABLE IF EXISTS `dc_task_meta_data_column`;

CREATE TABLE `dc_task_meta_data_column` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `meta_data_id` varchar(200)  NOT NULL COMMENT '参与任务的元数据ID',
    `selected_column_idx` int NOT NULL COMMENT '元数据在此次任务中的参与计算的字段索引序号',
    PRIMARY KEY (`task_id`,`meta_data_id`,`selected_column_idx`)
) ENGINE=InnoDB  COMMENT='任务数据metadata明细';

/*Table structure for table `dc_task_power_provider` */

DROP TABLE IF EXISTS `dc_task_power_provider`;

CREATE TABLE `dc_task_power_provider` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `identity_id` varchar(200)  NOT NULL COMMENT '算力提供者组织身份ID',
    `party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `used_memory` bigint DEFAULT '0' COMMENT '任务使用的内存, 字节',
    `used_core` int DEFAULT '0' COMMENT '任务使用的core',
    `used_bandwidth` bigint DEFAULT '0' COMMENT '任务使用的带宽, bps',
    PRIMARY KEY (`task_id`,`identity_id`)
) ENGINE=InnoDB  COMMENT='任务算力提供者';

/*Table structure for table `dc_task_result_consumer` */

DROP TABLE IF EXISTS `dc_task_result_consumer`;

CREATE TABLE `dc_task_result_consumer` (
    `task_id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `consumer_identity_id` varchar(200)  NOT NULL COMMENT '结果消费者组织身份ID',
    `consumer_party_id` varchar(200)  NOT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    `producer_identity_id` varchar(200)  DEFAULT NULL COMMENT '结果产生者的组织身份ID',
    `producer_party_id` varchar(200)  DEFAULT NULL COMMENT '任务参与方在本次任务中的唯一识别ID',
    PRIMARY KEY (`task_id`,`consumer_identity_id`)
) ENGINE=InnoDB  COMMENT='任务结果接收者';

/*Table structure for table `mo_algorithm` */

DROP TABLE IF EXISTS `mo_algorithm`;

CREATE TABLE `mo_algorithm` (
    `algorithm_id` bigint NOT NULL COMMENT '算法表ID',
    `algorithm_desc` varchar(200)  DEFAULT NULL COMMENT '中文算法描述',
    `algorithm_desc_en` varchar(200)  DEFAULT NULL COMMENT '英文算法描述',
    `author` varchar(30)  DEFAULT NULL COMMENT '算法作者',
    `max_numbers` bigint DEFAULT NULL COMMENT '支持协同方最大数量',
    `min_numbers` bigint DEFAULT NULL COMMENT '支持协同方最小数量',
    `support_language` varchar(64)  DEFAULT NULL COMMENT '支持语言,多个以","进行分隔',
    `support_os_system` varchar(64)  DEFAULT NULL COMMENT '支持操作系统,多个以","进行分隔',
    `cost_mem` bigint DEFAULT NULL COMMENT '所需的内存 (单位: byte)',
    `cost_cpu` int DEFAULT NULL COMMENT '所需的核数 (单位: 个)',
    `cost_gpu` int DEFAULT NULL COMMENT 'GPU核数(单位：核)',
    `cost_bandwidth` bigint DEFAULT '0' COMMENT '所需的带宽 (单位: bps)',
    `run_time` bigint NOT NULL DEFAULT '3600000' COMMENT '所需的运行时长,默认1小时 (单位: ms)',
    `input_model` tinyint NOT NULL DEFAULT '0' COMMENT '是否需要输入模型: 0-否，1:是',
    `output_model` tinyint NOT NULL DEFAULT '0' COMMENT '是否产生模型: 0-否，1:是',
    `support_default_psi` tinyint NOT NULL DEFAULT '0' COMMENT '是否支持默认的psi处理: 0-否，1:是',
    `output_psi` tinyint NOT NULL DEFAULT '0' COMMENT '是否产生psi: 0-否，1:是',
    `store_pattern` tinyint NOT NULL DEFAULT '1' COMMENT '输出存储形式: 1-明文，2:密文',
    `data_rows_flag` tinyint NOT NULL DEFAULT '0' COMMENT '是否判断数据行数: 0-否，1-是',
    `data_columns_flag` tinyint NOT NULL DEFAULT '0' COMMENT '是否判断数据列数: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB  COMMENT='算法表';

/*Table structure for table `mo_algorithm_classify` */

DROP TABLE IF EXISTS `mo_algorithm_classify`;

CREATE TABLE `mo_algorithm_classify` (
    `id` bigint NOT NULL COMMENT '分类id',
    `parent_id` bigint NOT NULL COMMENT '父分类id，如果为顶级分类，则为0',
    `name` varchar(30)  DEFAULT NULL COMMENT '分类中文名称',
    `name_en` varchar(60)  DEFAULT NULL COMMENT '英文算法名称',
    `image_url` varchar(1024)  DEFAULT NULL COMMENT '算法图片url',
    `is_available` tinyint NOT NULL DEFAULT '1' COMMENT '是否可用: 0-否，1-是',
    `is_algorithm` tinyint NOT NULL DEFAULT '1' COMMENT '是否算法: 0-否，1-是',
    `is_exist_algorithm` tinyint NOT NULL DEFAULT '1' COMMENT '是否存在对应算法: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_ALG_NAME` (`name`)
) ENGINE=InnoDB  COMMENT='算法分类表';

/*Table structure for table `mo_algorithm_code` */

DROP TABLE IF EXISTS `mo_algorithm_code`;

CREATE TABLE `mo_algorithm_code` (
    `algorithm_id` bigint NOT NULL COMMENT '算法代码表ID',
    `edit_type` tinyint DEFAULT NULL COMMENT '编辑类型:1-sql,2-noteBook',
    `calculate_contract_struct` varchar(1024)  DEFAULT NULL COMMENT '计算合约变量模板json格式结构',
    `calculate_contract_code` text  COMMENT '计算合约',
    `data_split_contract_code` text  COMMENT '数据分片合约',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB  COMMENT='算法代码表';

/*Table structure for table `mo_algorithm_variable` */

DROP TABLE IF EXISTS `mo_algorithm_variable`;

CREATE TABLE `mo_algorithm_variable` (
    `algorithm_id` bigint NOT NULL COMMENT '算法id',
    `var_key` varchar(128)  NOT NULL COMMENT '变量key',
    `var_type` tinyint NOT NULL COMMENT '变量类型. 1-boolean, 2-number, 3-string, 4-numberArray, 5-stringArray',
    `var_value` varchar(128)  NOT NULL COMMENT '变量默认值',
    `var_desc` varchar(512)  DEFAULT NULL COMMENT '变量中文描述',
    `var_desc_en` varchar(512)  DEFAULT NULL COMMENT '变量英文描述',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`,`var_key`)
) ENGINE=InnoDB  COMMENT='算法变量表';

/*Table structure for table `mo_calculation_process` */

DROP TABLE IF EXISTS `mo_calculation_process`;

CREATE TABLE `mo_calculation_process` (
    `calculation_process_id` bigint NOT NULL AUTO_INCREMENT COMMENT '计算流程ID',
    `name` varchar(200)  NOT NULL COMMENT '计算流程中文名字',
    `name_en` varchar(200)  DEFAULT NULL COMMENT '计算流程英文名字',
    `status` tinyint NOT NULL DEFAULT '1' COMMENT '状态: 0-无效，1- 有效',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`calculation_process_id`)
) ENGINE=InnoDB  COMMENT='计算流程配置表';

/*Table structure for table `mo_calculation_process_algorithm` */

DROP TABLE IF EXISTS `mo_calculation_process_algorithm`;

CREATE TABLE `mo_calculation_process_algorithm` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `algorithm_id` bigint NOT NULL COMMENT '算法ID',
    PRIMARY KEY (`algorithm_id`,`calculation_process_id`)
) ENGINE=InnoDB  COMMENT='计算流程配置和算法关系表';

/*Table structure for table `mo_calculation_process_step` */

DROP TABLE IF EXISTS `mo_calculation_process_step`;

CREATE TABLE `mo_calculation_process_step` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `step` int NOT NULL COMMENT '步骤. 从0开始',
    `type` int NOT NULL COMMENT '部署说明. 0-选择训练输入数据, 1-选择预测输入数据, 2-选择PSI输入数据, 3-选择计算环境(通用), 4-选择计算环境(训练&预测), 5-选择结果接收方(通用), 6-选择结果接收方(训练&预测)',
    `task_1_step` int DEFAULT NULL COMMENT '任务1对应的步骤',
    `task_2_step` int DEFAULT NULL COMMENT '任务2对应的步骤',
    `task_3_step` int DEFAULT NULL COMMENT '任务3对应的步骤',
    `task_4_step` int DEFAULT NULL COMMENT '任务4对应的步骤',
    PRIMARY KEY (`calculation_process_id`,`step`)
) ENGINE=InnoDB  COMMENT='计算流程配置步骤表';

/*Table structure for table `mo_calculation_process_task` */

DROP TABLE IF EXISTS `mo_calculation_process_task`;

CREATE TABLE `mo_calculation_process_task` (
    `calculation_process_id` bigint NOT NULL COMMENT '计算流程ID',
    `step` int NOT NULL COMMENT '任务步骤',
    `algorithm_select` int NOT NULL COMMENT '算法选择方式. 0-用户输入母算法; 1-用户输入子训练算法, 2-用户输入子预测算法, 3-内置PSI算法',
    `input_model_step` int DEFAULT NULL COMMENT '工作流节点需要的模型产生的步骤',
    `input_psi_step` int DEFAULT NULL COMMENT '工作流节点需要的psi产生步骤',
    PRIMARY KEY (`calculation_process_id`,`step`)
) ENGINE=InnoDB  COMMENT='计算流程任务步骤表';

/*Table structure for table `mo_data_sync` */

DROP TABLE IF EXISTS `mo_data_sync`;

CREATE TABLE `mo_data_sync` (
    `data_type` varchar(256)  NOT NULL COMMENT '数据类型',
    `latest_synced` bigint NOT NULL DEFAULT '0' COMMENT '数据最新同步时间戳，精确到毫秒',
    `info` varchar(50)  DEFAULT '' COMMENT '描述',
    PRIMARY KEY (`data_type`)
) ENGINE=InnoDB  COMMENT='数据同步时间记录';

/*Table structure for table `mo_model` */

DROP TABLE IF EXISTS `mo_model`;

CREATE TABLE `mo_model` (
    `meta_data_id` varchar(128)  NOT NULL COMMENT '模型元数据id',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `name` varchar(128)  DEFAULT NULL COMMENT '名称',
    `file_id` varchar(256)  DEFAULT NULL COMMENT '源文件ID',
    `file_path` varchar(128)  NOT NULL COMMENT '源文件存放路径',
    `train_task_id` varchar(256)  NOT NULL COMMENT '训练模型的任务id',
    `supported_algorithm_id` bigint DEFAULT NULL COMMENT '输入模型的算法id',
    `train_algorithm_id` bigint DEFAULT NULL COMMENT '训练模型的算法id',
    `train_user_address` varchar(64)  DEFAULT NULL COMMENT '训练模型的账户',
    `evaluate` varchar(2048)  DEFAULT NULL COMMENT '模型评估结果',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`)
) ENGINE=InnoDB  COMMENT='模型表';

/*Table structure for table `mo_org_expand` */

DROP TABLE IF EXISTS `mo_org_expand`;

CREATE TABLE `mo_org_expand` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `identity_ip` varchar(20)  NOT NULL COMMENT '组织的ip',
    `identity_port` int NOT NULL COMMENT '组织的端口',
    `is_public` tinyint(1) NOT NULL DEFAULT '0' COMMENT '是否公共组织: 0-否，1-是',
    `observer_proxy_wallet_address` varchar(100)  DEFAULT NULL COMMENT '当前组织内置系统钱包地址 (见证人代理钱包)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`)
) ENGINE=InnoDB  COMMENT='组织扩展表';

/*Table structure for table `mo_org_user` */

DROP TABLE IF EXISTS `mo_org_user`;

CREATE TABLE `mo_org_user` (
    `identity_id` varchar(128)  NOT NULL COMMENT '组织的身份标识Id',
    `address` varchar(64)  NOT NULL COMMENT '用户的钱包地址',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`,`address`)
) ENGINE=InnoDB  COMMENT='组织管理用户';

/*Table structure for table `mo_psi` */

DROP TABLE IF EXISTS `mo_psi`;

CREATE TABLE `mo_psi` (
    `meta_data_id` varchar(128)  NOT NULL COMMENT '模型元数据id',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `name` varchar(128)  DEFAULT NULL COMMENT '名称',
    `file_id` varchar(256)  DEFAULT NULL COMMENT '源文件ID',
    `file_path` varchar(128)  NOT NULL COMMENT '源文件存放路径',
    `train_task_id` varchar(256)  NOT NULL COMMENT '训练模型的任务id',
    `train_algorithm_id` bigint DEFAULT NULL COMMENT '训练模型的算法id, 母算法',
    `train_user_address` varchar(64)  DEFAULT NULL COMMENT '训练模型的账户',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`)
) ENGINE=InnoDB  COMMENT='PSI结果表';

/*Table structure for table `mo_stats_data` */

DROP TABLE IF EXISTS `mo_stats_data`;

CREATE TABLE `mo_stats_data` (
    `meta_data_id` varchar(200)  NOT NULL COMMENT '元数据ID,hash',
    `token_used` bigint DEFAULT '0' COMMENT '数据凭证使用量',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`meta_data_id`)
) ENGINE=InnoDB  COMMENT='数据维度统计';

/*Table structure for table `mo_stats_day` */

DROP TABLE IF EXISTS `mo_stats_day`;

CREATE TABLE `mo_stats_day` (
    `stats_time` date NOT NULL COMMENT '统计时间',
    `stats_key` varchar(64)  NOT NULL COMMENT '统计可用: taskCount-计算走势',
    `stats_value` bigint DEFAULT '0' COMMENT '统计值',
    PRIMARY KEY (`stats_time`,`stats_key`)
) ENGINE=InnoDB  COMMENT='日期维度统计';

/*Table structure for table `mo_stats_global` */

DROP TABLE IF EXISTS `mo_stats_global`;

CREATE TABLE `mo_stats_global` (
    `id` bigint NOT NULL COMMENT 'ID(自增长)',
    `task_count` int DEFAULT '0' COMMENT '隐私计算总次数(总的任务数,包括成功和失败的)',
    `address_count_of_task` int DEFAULT '0' COMMENT '参与计算地址总数',
    `address_count_of_active` int DEFAULT '0' COMMENT '24h活跃地址总数',
    `data_size` bigint DEFAULT '0' COMMENT '全网数据总量，单位：字节',
    `data_token_count` int DEFAULT '0' COMMENT '数据凭证数量',
    `data_token_used` bigint DEFAULT '0' COMMENT '数据凭证使用量',
    `total_core` int DEFAULT '0' COMMENT '全网目前的算力核心数，单位：个',
    `total_memory` bigint DEFAULT '0' COMMENT '全网目前的算力内存数，单位：字节',
    `total_bandwidth` bigint DEFAULT '0' COMMENT '全网目前的带宽，单位：字节',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='全网维度统计';

/*Table structure for table `mo_stats_org` */

DROP TABLE IF EXISTS `mo_stats_org`;

CREATE TABLE `mo_stats_org` (
    `identity_id` varchar(200)  NOT NULL COMMENT '身份认证标识的id',
    `org_total_core` int DEFAULT '0' COMMENT '全网目前的算力核心数，单位：个',
    `org_total_memory` bigint DEFAULT '0' COMMENT '全网目前的算力内存数，单位：字节',
    `org_total_bandwidth` bigint DEFAULT '0' COMMENT '全网目前的带宽，单位：字节',
    `computing_power_ratio` int DEFAULT '0' COMMENT '算力占比. 万分比。100 = 1%',
    `total_task` bigint DEFAULT '0' COMMENT '参与任务数量',
    `total_data` bigint DEFAULT '0' COMMENT '数据数',
    `total_data_token` bigint DEFAULT '0' COMMENT '凭证数',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`identity_id`)
) ENGINE=InnoDB  COMMENT='组织维度统计';

/*Table structure for table `mo_task_expand` */

DROP TABLE IF EXISTS `mo_task_expand`;

CREATE TABLE `mo_task_expand` (
    `id` varchar(200)  NOT NULL COMMENT '任务ID,hash',
    `event_synced` tinyint(1) NOT NULL DEFAULT '0' COMMENT '事件是否同步完成 0-否 1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='任务扩展表';

/*Table structure for table `mo_token` */

DROP TABLE IF EXISTS `mo_token`;

CREATE TABLE `mo_token` (
    `address` varchar(64)  NOT NULL COMMENT '合约地址',
    `name` varchar(64)  DEFAULT NULL COMMENT '合约名称',
    `symbol` varchar(64)  DEFAULT NULL COMMENT '合约符号',
    `decimal` bigint DEFAULT NULL COMMENT '合约精度',
    `price` varchar(128)  DEFAULT NULL COMMENT 'LAT的价格',
    `is_add_liquidity` tinyint NOT NULL DEFAULT '0' COMMENT '是否添加流动性: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`address`)
) ENGINE=InnoDB  COMMENT='token信息';

/*Table structure for table `mo_token_holder` */

DROP TABLE IF EXISTS `mo_token_holder`;

CREATE TABLE `mo_token_holder` (
    `token_address` varchar(64)  NOT NULL COMMENT '合约地址',
    `address` varchar(64)  NOT NULL COMMENT '用户地址',
    `balance` varchar(128)  DEFAULT NULL COMMENT '地址代币余额, ERC20为金额',
    `authorize_balance` varchar(128)  DEFAULT NULL COMMENT '地址已授权支付助手的代币余额, ERC20为金额',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`token_address`,`address`)
) ENGINE=InnoDB  COMMENT='token持有表';

/*Table structure for table `mo_user` */

DROP TABLE IF EXISTS `mo_user`;

CREATE TABLE `mo_user` (
    `address` varchar(64)  NOT NULL COMMENT '用户钱包地址',
    `user_name` varchar(64)  NOT NULL COMMENT '用户名',
    `is_valid` tinyint NOT NULL DEFAULT '1' COMMENT '是否有效: 0-否，1-是',
    `org_identity_id` varchar(128)  DEFAULT NULL COMMENT '默认连接的组织id',
    `heart_beat_time` timestamp NULL DEFAULT NULL COMMENT '最后的心跳时间',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`address`),
    UNIQUE KEY `UK_USERNAME` (`user_name`) USING BTREE COMMENT '用户名称唯一'
) ENGINE=InnoDB  COMMENT='用户表';

/*Table structure for table `mo_user_login` */

DROP TABLE IF EXISTS `mo_user_login`;

CREATE TABLE `mo_user_login` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '日志表id(自增长)',
    `address` varchar(64)  NOT NULL COMMENT '登录地址',
    `is_success` tinyint NOT NULL DEFAULT '1' COMMENT '是否成功: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='用户登录日志表';

/*Table structure for table `mo_workflow` */

DROP TABLE IF EXISTS `mo_workflow`;

CREATE TABLE `mo_workflow` (
    `workflow_id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流ID(自增长)',
    `create_mode` tinyint NOT NULL COMMENT '创建模式:1-专家模式,2-向导模式',
    `address` varchar(64)  NOT NULL COMMENT '用户地址',
    `workflow_name` varchar(60)  NOT NULL COMMENT '工作流名称',
    `workflow_desc` varchar(200)  DEFAULT NULL COMMENT '工作流描述',
    `algorithm_id` bigint DEFAULT NULL COMMENT '算法id',
    `algorithm_name` varchar(200)  DEFAULT NULL COMMENT '算法名称',
    `calculation_process_id` bigint DEFAULT NULL COMMENT '计算流程id',
    `calculation_process_name` varchar(200)  DEFAULT NULL COMMENT '计算流程名称',
    `last_run_time` timestamp(3) NULL DEFAULT NULL COMMENT '最后运行时间',
    `is_setting_completed` tinyint NOT NULL DEFAULT '0' COMMENT '是否设置完成:  0-否  1-是',
    `calculation_process_step` int DEFAULT NULL COMMENT '向导模式下当前步骤',
    `is_delete` tinyint NOT NULL DEFAULT '0' COMMENT '是否删除: 0-否  1-是',
    `workflow_version` bigint DEFAULT '1' COMMENT '当前最大版本号,从1开始',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_id`),
    KEY `user_address` (`address`)
) ENGINE=InnoDB  COMMENT='工作流表';

/*Table structure for table `mo_workflow_run_status` */

DROP TABLE IF EXISTS `mo_workflow_run_status`;

CREATE TABLE `mo_workflow_run_status` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流节点表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` int NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `sign` varchar(512)  NOT NULL COMMENT '发起任务的账户的签名',
    `address` varchar(64)  NOT NULL COMMENT '发起任务的账户的地址',
    `step` int DEFAULT NULL COMMENT '总步骤',
    `cur_step` int DEFAULT NULL COMMENT '当前步骤',
    `begin_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '开始时间',
    `end_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '结束时间',
    `run_status` tinyint DEFAULT NULL COMMENT '运行状态: 1-运行中,2-运行成功,3-运行失败',
    `cancel_status` tinyint DEFAULT NULL COMMENT '取消状态: 1-取消中,2-取消成功,3-取消失败',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流运行状态';

/*Table structure for table `mo_workflow_run_task_result` */

DROP TABLE IF EXISTS `mo_workflow_run_task_result`;

CREATE TABLE `mo_workflow_run_task_result` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'ID(自增长)',
    `identity_id` varchar(128)  NOT NULL COMMENT '所属组织',
    `task_id` varchar(256)  DEFAULT NULL COMMENT '任务ID,底层处理完成后返回',
    `file_name` varchar(128)  NOT NULL COMMENT '任务结果文件的名称',
    `metadata_id` varchar(128)  NOT NULL COMMENT '任务结果文件的元数据Id <系统默认生成的元数据>',
    `origin_id` varchar(128)  NOT NULL COMMENT '任务结果文件的原始文件Id',
    `file_path` varchar(256)  NOT NULL COMMENT '任务结果文件的完整相对路径名',
    `ip` varchar(32)  NOT NULL COMMENT '任务结果文件所在的 数据服务内网ip',
    `port` varchar(8)  NOT NULL COMMENT '任务结果文件所在的 数据服务内网port',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流任务运行状态';

/*Table structure for table `mo_workflow_run_task_status` */

DROP TABLE IF EXISTS `mo_workflow_run_task_status`;

CREATE TABLE `mo_workflow_run_task_status` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT 'ID(自增长)',
    `workflow_run_id` bigint NOT NULL COMMENT '工作流运行状态ID',
    `workflow_task_id` bigint NOT NULL COMMENT '工作流节点配置ID',
    `step` int DEFAULT NULL COMMENT '节点在工作流中序号,从1开始',
    `begin_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '开始时间',
    `end_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '结束时间',
    `run_status` tinyint DEFAULT NULL COMMENT '运行状态: :0-未开始 1-运行中,2-运行成功,3-运行失败',
    `task_id` varchar(256)  DEFAULT NULL COMMENT '任务ID,底层处理完成后返回',
    `run_msg` varchar(256)  DEFAULT NULL COMMENT '任务处理结果描述',
    `model_id` varchar(128)  DEFAULT '0' COMMENT '工作流节点需要的模型id',
    `psi_id` varchar(128)  DEFAULT '0' COMMENT '工作流节点需要的psi的id',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB  COMMENT='工作流任务运行状态';

/*Table structure for table `mo_workflow_setting_expert` */

DROP TABLE IF EXISTS `mo_workflow_setting_expert`;

CREATE TABLE `mo_workflow_setting_expert` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流节点表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `node_step` int DEFAULT '1' COMMENT '工作流中节点的顺序,从1开始',
    `node_name` varchar(30)  DEFAULT NULL COMMENT '节点名称',
    `psi_task_step` int DEFAULT NULL COMMENT '工作流任务步骤',
    `task_step` int NOT NULL COMMENT '工作流任务步骤',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_NODE_STEP` (`workflow_id`,`workflow_version`,`node_step`)
) ENGINE=InnoDB  COMMENT='工作流专家模式节点表';

/*Table structure for table `mo_workflow_setting_wizard` */

DROP TABLE IF EXISTS `mo_workflow_setting_wizard`;

CREATE TABLE `mo_workflow_setting_wizard` (
    `id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流步骤表ID(自增长)',
    `workflow_id` bigint DEFAULT NULL COMMENT '工作流id',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '工作流版本号',
    `step` int DEFAULT '1' COMMENT '当前步骤,从1开始',
    `calculation_process_step_type` int DEFAULT '1' COMMENT '部署说明. 0-选择训练输入数据, 1-选择预测输入数据, 2-选择PSI输入数据, 3-选择计算环境(通用), 4-选择计算环境(训练&预测), 5-选择结果接收方(通用), 6-选择结果接收方(训练&预测)',
    `task_1_step` int DEFAULT NULL COMMENT '任务1对应的步骤',
    `task_2_step` int DEFAULT NULL COMMENT '任务2对应的步骤',
    `task_3_step` int DEFAULT NULL COMMENT '任务3对应的步骤',
    `task_4_step` int DEFAULT NULL COMMENT '任务4对应的步骤',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_NODE_STEP` (`workflow_id`,`workflow_version`,`step`)
) ENGINE=InnoDB COMMENT='工作流向导模式步骤表';

/*Table structure for table `mo_workflow_task` */

DROP TABLE IF EXISTS `mo_workflow_task`;

CREATE TABLE `mo_workflow_task` (
    `workflow_task_id` bigint NOT NULL AUTO_INCREMENT COMMENT '工作流任务配置id',
    `workflow_id` bigint NOT NULL COMMENT '工作流ID',
    `workflow_version` bigint DEFAULT '1' COMMENT '编辑版本标识,从1开始',
    `step` int DEFAULT '1' COMMENT '工作流中任务的顺序,从1开始',
    `algorithm_id` bigint DEFAULT NULL COMMENT '算法id',
    `identity_id` varchar(128)  DEFAULT NULL COMMENT '任务发启放组织id',
    `input_model` int NOT NULL DEFAULT '0' COMMENT '是否需要输入模型: 0-否，1:是',
    `input_model_id` varchar(128)  DEFAULT NULL COMMENT '工作流节点需要的模型id',
    `input_model_step` int DEFAULT NULL COMMENT '工作流节点需要的模型产生的步骤',
    `input_psi` int NOT NULL DEFAULT '0' COMMENT '是否需要输入PSI: 0-否，1:是',
    `input_psi_step` int DEFAULT NULL COMMENT '工作流节点需要的psi产生步骤',
    `enable` tinyint NOT NULL DEFAULT '1' COMMENT '是否可用: 0-否，1:是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`),
    UNIQUE KEY `UK_WF_TASK` (`workflow_id`,`workflow_version`,`step`)
) ENGINE=InnoDB COMMENT='工作流任务配置表';

/*Table structure for table `mo_workflow_task_code` */

DROP TABLE IF EXISTS `mo_workflow_task_code`;

CREATE TABLE `mo_workflow_task_code` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `edit_type` int NOT NULL COMMENT '编辑类型:1-sql, 2-noteBook',
    `calculate_contract_struct` varchar(1024)  DEFAULT NULL COMMENT '计算合约变量模板json格式结构',
    `calculate_contract_code` text  COMMENT '计算合约',
    `data_split_contract_code` text  COMMENT '数据分片合约',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`)
) ENGINE=InnoDB  COMMENT='工作流任务算法代码表';

/*Table structure for table `mo_workflow_task_input` */

DROP TABLE IF EXISTS `mo_workflow_task_input`;

CREATE TABLE `mo_workflow_task_input` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `meta_data_id` varchar(128)  NOT NULL COMMENT '数据表ID',
    `identity_id` varchar(128)  NOT NULL COMMENT '组织的身份标识Id',
    `key_column` bigint DEFAULT NULL COMMENT 'ID列(列索引)(存id值)',
    `dependent_variable` bigint DEFAULT NULL COMMENT '因变量(标签)(存id值)',
    `data_column_ids` varchar(1024)  DEFAULT NULL COMMENT '数据字段ID索引(存id值)',
    `party_id` varchar(64)  DEFAULT NULL COMMENT '任务里面定义的 (p0 -> pN 方 ...)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`meta_data_id`)
) ENGINE=InnoDB  COMMENT='工作流任务输入表';

/*Table structure for table `mo_workflow_task_output` */

DROP TABLE IF EXISTS `mo_workflow_task_output`;

CREATE TABLE `mo_workflow_task_output` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `identity_id` varchar(128)  NOT NULL COMMENT '协同方组织的身份标识Id',
    `store_pattern` tinyint DEFAULT '1' COMMENT '存储形式: 1-明文，2:密文',
    `output_content` text  COMMENT '输出内容',
    `party_id` varchar(64)  DEFAULT NULL COMMENT '任务里面定义的 (q0 -> qN 方 ...)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`identity_id`)
) ENGINE=InnoDB  COMMENT='项目工作流节点输出表';

/*Table structure for table `mo_workflow_task_resource` */

DROP TABLE IF EXISTS `mo_workflow_task_resource`;

CREATE TABLE `mo_workflow_task_resource` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `cost_mem` bigint DEFAULT NULL COMMENT '所需的内存 (单位: byte)',
    `cost_cpu` int DEFAULT NULL COMMENT '所需的核数 (单位: 个)',
    `cost_gpu` int DEFAULT NULL COMMENT 'GPU核数(单位：核)',
    `cost_bandwidth` bigint DEFAULT '0' COMMENT '所需的带宽 (单位: bps)',
    `run_time` bigint DEFAULT NULL COMMENT '所需的运行时长 (单位: ms)',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`)
) ENGINE=InnoDB  COMMENT='工作流任务资源表';

/*Table structure for table `mo_workflow_task_variable` */

DROP TABLE IF EXISTS `mo_workflow_task_variable`;

CREATE TABLE `mo_workflow_task_variable` (
    `workflow_task_id` bigint NOT NULL COMMENT '工作流任务配置id',
    `var_key` varchar(128)  NOT NULL COMMENT '变量key',
    `var_value` varchar(128)  NOT NULL COMMENT '变量值',
    `var_desc` varchar(512)  DEFAULT NULL COMMENT '变量中文描述',
    `var_desc_en` varchar(512)  DEFAULT NULL COMMENT '变量英文描述',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_task_id`,`var_key`)
) ENGINE=InnoDB  COMMENT='工作流任务变量表';

/*Table structure for table `mo_workflow_version` */

DROP TABLE IF EXISTS `mo_workflow_version`;

CREATE TABLE `mo_workflow_version` (
    `workflow_id` bigint NOT NULL COMMENT '工作流ID',
    `workflow_version` bigint NOT NULL DEFAULT '1' COMMENT '编辑版本标识,从1开始',
    `workflow_version_name` varchar(200)  DEFAULT NULL COMMENT '工作流版本名称',
    `status` tinyint NOT NULL DEFAULT '0' COMMENT '状态: 0-待支付、1-支付中、2-已支付、3-运行中、4-运行成功、5-运行失败',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`workflow_id`,`workflow_version`)
) ENGINE=InnoDB  COMMENT='工作流不同版本设置表';

/*Data for the table `mo_algorithm` */

insert  into `mo_algorithm`(`algorithm_id`,`algorithm_desc`,`algorithm_desc_en`,`author`,`max_numbers`,`min_numbers`,`support_language`,`support_os_system`,`cost_mem`,`cost_cpu`,`cost_gpu`,`cost_bandwidth`,`run_time`,`input_model`,`output_model`,`support_default_psi`,`output_psi`,`store_pattern`,`data_rows_flag`,`data_columns_flag`,`create_time`,`update_time`) values (1001,'用于跨组织的数据交集查询','Used for cross-organization data intersection query','Rosetta',3,2,'SQL,Python','window,linux,mac',1073741824,1,2,3145728,180000,0,0,0,0,1,1,0,'2022-03-24 04:09:33','2022-03-24 04:09:33'),(2011,'用于跨组织线性回归训练','Used for cross-organization linear regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:57'),(2012,'用于跨组织线性回归的预测','Used for cross-organization linear regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:57'),(2021,'用于跨组织逻辑回归训练','Used for cross-organization logistic regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:58'),(2022,'用于跨组织逻辑回归预测','Used for cross-organization logistic regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:01:59'),(2031,'用于跨组织DNN训练','Used for cross-organization DNN training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:00'),(2032,'用于跨组织DNN预测算法','Used for cross-organization DNN prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:00'),(2041,'用于跨组织XGBoost训练','Used for cross-organization XGBoost training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:01'),(2042,'用于跨组织XGBoost预测','Used for cross-organization XGBoost prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,0,1,1,0,'2022-03-24 04:09:33','2022-04-11 04:02:03');

/*Data for the table `mo_algorithm_classify` */

insert  into `mo_algorithm_classify`(`id`,`parent_id`,`name`,`name_en`,`image_url`,`is_available`,`is_algorithm`,`is_exist_algorithm`,`create_time`,`update_time`) values (1,0,'计算','Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:42:10'),(2,1,'隐私计算','Privacy Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(3,1,'非隐私计算','Non-Privacy Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(1000,2,'隐私统计分析','Privacy Statistics',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(1001,1000,'隐私求交集（PSI）','Private Set Intersection','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/SetIntersection.png\r',1,1,1,'2022-03-28 11:41:08','2022-04-25 02:43:23'),(2000,2,'隐私AI计算','Privacy AI Computing',NULL,1,0,0,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2010,2000,'隐私线性回归','Private Linear Regression','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/AIComputing.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:44:28'),(2011,2010,'隐私线性回归训练','Private Linear Regression Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2012,2010,'隐私线性回归预测','Private Linear Regression Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-04-01 08:51:39'),(2020,2000,'隐私逻辑回归','Private Logistic Regression','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/logistic.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:44:08'),(2021,2020,'隐私逻辑回归训练','Private Logistic Regression Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2022,2020,'隐私逻辑回归预测','Private Logistic Regression Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2030,2000,'隐私DNN（深度神经网络）','Private DNN','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/DNN.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:43:38'),(2031,2030,'隐私DNN训练','Private DNN Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2032,2030,'隐私DNN预测','Private DNN Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2040,2000,'隐私XGBoost','Private XGBoost','https://nft-sgp-oss-001.oss-ap-southeast-1.aliyuncs.com/moirae/alg/XGBoost.png',1,1,0,'2022-03-28 11:41:08','2022-04-25 02:43:52'),(2041,2040,'隐私XGBoost训练','Private XGBoost Training',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08'),(2042,2040,'隐私XGBoost预测','Private XGBoost Prediction',NULL,1,1,1,'2022-03-28 11:41:08','2022-03-28 11:41:08');

/*Data for the table `mo_algorithm_code` */

insert  into `mo_algorithm_code`(`algorithm_id`,`edit_type`,`calculate_contract_struct`,`calculate_contract_code`,`data_split_contract_code`,`create_time`,`update_time`) values (1001,2,'{\"psi_type\":\"T_V1_Basic_GLS254\"}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.psi as psi\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivateSetIntersection(object):\r\n    \'\'\'\r\n    private set intersection.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"psi_type\": \"T_V1_Basic_GLS254\"\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}, cfg_dict:{cfg_dict}, data_party:{data_party}, \"\r\n                 f\"result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be string\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self._check_parameters()\r\n        self.result_type = self._get_result_type()        \r\n        self.output_file = os.path.join(results_dir, \"psi_result.csv\")\r\n        self.sdk_log_level = 3  # Trace=0, Debug=1, Audit=2, Info=3, Warn=4, Error=5, Fatal=6, Off=7\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.psi_type = dynamic_parameter.get(\"psi_type\", \"T_V1_Basic_GLS254\")  # default \'T_V1_Basic_GLS254\'\r\n\r\n    def _check_parameters(self):\r\n        assert len(self.data_party) == 2, f\"length of data_party must be 2, not {len(self.data_party)}.\"\r\n        assert len(self.result_party) in [1, 2], f\"length of result_party must be 1 or 2, not {len(self.result_party)}.\"\r\n        if len(self.result_party) == 2:\r\n            assert self.result_party[0] == self.data_party[0] and self.result_party[1] == self.data_party[1], \\\r\n                    f\"result_party:{self.result_party} not equal to data_party:{self.data_party}\"\r\n        else:\r\n            assert self.result_party[0] in self.data_party, \\\r\n                    f\"result_party:{self.result_party} not in data_party:{self.data_party}\"\r\n        self._check_input_file()\r\n\r\n    def _check_input_file(self):\r\n        assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n        self.input_file = self.input_file.strip()\r\n        if os.path.exists(self.input_file):\r\n            file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n            assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n            assert self.key_column, f\"key_column can not empty. key_column={self.key_column}\"\r\n            input_columns = pd.read_csv(self.input_file, nrows=0)\r\n            input_columns = list(input_columns.columns)\r\n            assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n        else:\r\n            raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n    \r\n    def _get_result_type(self):\r\n        \'\'\'\r\n        assume data_party = [P0, P1]:\r\n            if result_party = [P0, P1], then result_type = 2\r\n            if result_party = [P0], then result_type = 0\r\n            if result_party = [P1], then result_type = 1\r\n        \'\'\'\r\n        if len(self.result_party) == 2:\r\n            result_type = 2\r\n        else:\r\n            if self.result_party[0] == self.data_party[0]:\r\n                result_type = 0\r\n            else:\r\n                result_type = 1\r\n        return result_type\r\n\r\n    def run(self):\r\n        log.info(\"start extract key_column.\")\r\n        temp_input_file = self._extract_key_column()\r\n        log.info(\"start get temp_output file name.\")\r\n        temp_dir = self._get_temp_dir()\r\n        temp_output_file = os.path.join(temp_dir, \"psi_sdk_output.csv\")\r\n\r\n        log.info(\"start run psi sdk.\")\r\n        self._run_psi_sdk(temp_input_file, temp_output_file)\r\n\r\n        log.info(\"start process result.\")\r\n        self._process_result(temp_output_file)\r\n        log.info(\"start remove temp dir.\")\r\n        self._remove_temp_dir()\r\n        log.info(\"psi all success.\")\r\n    \r\n    def _extract_key_column(self):\r\n        \'\'\'\r\n        Extract key column from input file,\r\n        and then write to a new file.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self._get_temp_dir()\r\n        key_col_file = os.path.join(temp_dir, f\"key_column_{self.party_id}.csv\")\r\n        log.info(\"read input file and write key_column to new file.\")\r\n        key_col = pd.read_csv(self.input_file, usecols=[self.key_column], dtype=\"str\")\r\n        key_col.to_csv(key_col_file, header=True, index=False)\r\n        return key_col_file\r\n    \r\n    def _run_psi_sdk(self, input_file, output_file):\r\n        \'\'\'\r\n        run psi sdk\r\n        \'\'\'\r\n        log.info(\"start create psihandler.\")\r\n        psihandler = psi.PSIHandler()\r\n        log.info(\"start set log.\")\r\n        psihandler.log_to_stdout(True)\r\n        psihandler.set_loglevel(self.sdk_log_level)\r\n        log.info(\"start set recv party.\")\r\n        psihandler.set_recv_party(self.result_party, \"\")\r\n\r\n        log.info(\"start create and set channel.\")\r\n        self._create_set_channel()\r\n        log.info(\"start activate.\")\r\n        psihandler.activate(self.psi_type, \"\")\r\n        log.info(\"finish activate.\")\r\n\r\n        log.info(\"start prepare data.\")\r\n        psihandler.prepare(input_file, taskid=\"\")\r\n        log.info(\"start run.\")\r\n        psihandler.run(input_file, output_file, taskid=\"\")\r\n        log.info(\"finish run.\")\r\n        run_stats = psihandler.get_perf_stats(True, \"\")\r\n        run_stats = run_stats.replace(\'\\n\', \'\').replace(\' \', \'\')\r\n        log.info(f\"run stats: {run_stats}\")\r\n        log.info(\"start deactivate.\")\r\n        psihandler.deactivate(\"\")\r\n        log.info(\"finish deactivate.\")\r\n    \r\n    def _process_result(self, file_name):\r\n        \'\'\'\r\n        for the result_party, add the col name to the beginning of the file, \r\n        and sort the values.\r\n        \'\'\'\r\n        if self.party_id in self.result_party:\r\n            log.info(f\"result party performs post-processing on the result.\")\r\n            key_col_name = self.key_column\r\n            if os.path.exists(file_name):\r\n                psi_result = pd.read_csv(file_name, header=None)\r\n                psi_result = pd.DataFrame(psi_result.values, columns=[key_col_name])\r\n                psi_result.sort_values(by=[key_col_name], ascending=True, inplace=True)\r\n                psi_result.to_csv(self.output_file, index=False, header=True)\r\n                log.info(f\"psi_result shape: {psi_result.shape}\")\r\n            else:\r\n                with open(self.output_file, \'w\') as output_f:\r\n                    output_f.write(key_col_name+\"\\n\")\r\n                log.info(f\"psi_result file is Empty, only have Column name: {key_col_name}\")\r\n        else:\r\n            log.info(f\"{self.party_id} is not result party, no result.\")\r\n\r\n    def _create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        log.info(\"start create iohandler.\")\r\n        iohandler = psi.IOHandler()\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        iohandler.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n    \r\n    def _get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def _remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self._get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n    \r\n    def _remove_output_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        This is used to delete all output files of the non-resulting party\r\n        \'\'\'\r\n        temp_dir = os.path.dirname(self.output_file)\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. private set intersection.\")\r\n    try:\r\n        psi = PrivateSetIntersection(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        psi.run()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:psi. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. private set intersection.\")\r\n',NULL,'2022-04-25 03:02:06','2022-04-25 03:05:31'),(2011,2,'{\"use_psi\":true,\"label_owner\":\"p1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyLinearRegTrain(object):\r\n    \'\'\'\r\n    Privacy linear regression train base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"dynamic_parameter\": {\r\n                \"use_psi\": true,\r\n                \"label_owner\": \"p1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(results_dir, \"model\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        if self.party_id == self.label_owner:\r\n            self.label_column = dynamic_parameter.get(\"label_column\")\r\n            self.data_with_label = True\r\n        else:\r\n            self.label_column = \"\"\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.001)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")        \r\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\r\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\r\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"\r\n        assert isinstance(self.use_validation_set, bool), \"use_validation_set must be type(bool), true or false\"\r\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.label_column:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        log.info(f\"check parameter finish.\")\r\n                        \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        Linear regression training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num])\r\n            Y = tf.placeholder(tf.float64, [None, 1])\r\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n            pred_Y = tf.matmul(X, W) + b\r\n            loss = tf.square(Y - pred_Y)\r\n            loss = tf.reduce_mean(loss)\r\n            # optimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            \r\n            reveal_Y = rtt.SecureReveal(pred_Y)\r\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\r\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                \r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n     \r\n        log.info(\"remove temp dir.\")\r\n        if self.party_id in (self.data_party + self.result_party):\r\n            # self.remove_temp_dir()\r\n            pass\r\n        else:\r\n            # delete the model in the compute party.\r\n            self.remove_output_dir()\r\n        \r\n        if (self.party_id in self.result_party) and self.use_validation_set:\r\n            log.info(\"result_party evaluate model.\")\r\n            Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n            evaluation_result = self.model_evaluation(Y_true, Y_pred)\r\n        else:\r\n            evaluation_result = \"\"\r\n        log.info(\"train success all.\")\r\n        return evaluation_result\r\n    \r\n    def model_evaluation(self, Y_true, Y_pred):\r\n        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\r\n        r2 = r2_score(Y_true, Y_pred)\r\n        rmse = mean_squared_error(Y_true, Y_pred, squared=False)\r\n        mse = mean_squared_error(Y_true, Y_pred, squared=True)\r\n        mae = mean_absolute_error(Y_true, Y_pred)\r\n        r2 = round(r2, 6)\r\n        rmse = round(rmse, 6)\r\n        mse = round(mse, 6)\r\n        mae = round(mae, 6)\r\n        evaluation_result = {\r\n            \"R2-score\": r2,\r\n            \"RMSE\": rmse,\r\n            \"MSE\": mse,\r\n            \"MAE\": mae\r\n        }\r\n        log.info(f\"evaluation_result = {evaluation_result}\")\r\n        evaluation_result = json.dumps(evaluation_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluation_result\r\n    \r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n    \r\n    def remove_output_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        This is used to delete all output files of the non-resulting party\r\n        \'\'\'\r\n        temp_dir = os.path.dirname(self.output_file)\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. linear regression train.\")\r\n    try:\r\n        privacy_linear_reg = PrivacyLinearRegTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_linear_reg.train()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:linr_train. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. linear regression train.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:06:53'),(2012,2,'{\"use_psi\":true,\"model_restore_party\":\"model1\",}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyLinearRegPredict(object):\r\n    \'\'\'\r\n    Privacy linear regression predict base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"model_restore_party\": \"model1\",\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(results_dir, \"result\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                elif input_type == 3:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2/3\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")        \r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\r\n        log.info(f\"check parameter finish.\")\r\n       \r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        Linear regression predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        X = tf.placeholder(tf.float64, [None, column_total_num])\r\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n        init = tf.global_variables_initializer()\r\n        # predict\r\n        pred_Y = tf.matmul(X, W) + b\r\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n        with tf.Session() as sess:\r\n            log.info(\"session init.\")\r\n            sess.run(init)\r\n            log.info(\"start restore model.\")\r\n            if self.party_id == self.model_restore_party:\r\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                    log.info(f\"model restore from: {self.model_file}.\")\r\n                    saver.restore(sess, self.model_file)\r\n                else:\r\n                    raise Exception(\"model not found or model damaged\")\r\n            else:\r\n                log.info(\"restore model...\")\r\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                with open(temp_file, \"w\") as f:\r\n                    pass\r\n                saver.restore(sess, temp_file)\r\n            log.info(\"finish restore model.\")\r\n            \r\n            # predict\r\n            log.info(\"predict start.\")\r\n            predict_start_time = time.time()\r\n            Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n            log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n            predict_use_time = round(time.time() - predict_start_time, 3)\r\n            log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\r\n            Y_pred = Y_pred.astype(\"float\")\r\n            Y_result = pd.DataFrame(Y_pred, columns=[\"result\"])\r\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n\r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n\r\n        return file_x, id_col\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. linear regression predict.\")\r\n    try:\r\n        privacy_linear_reg = PrivacyLinearRegPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_linear_reg.predict()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:linr_predict. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. linear regression predict.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:07:59'),(2021,2,'{\"use_psi\":true,\"label_owner\":\"data1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyLRTrain(object):\r\n    \'\'\'\r\n    Privacy logistic regression train base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"label_owner\": \"data1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(results_dir, \"model\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self._check_parameters()\r\n    \r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)            \r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        if self.party_id == self.label_owner:\r\n            self.label_column = dynamic_parameter.get(\"label_column\")\r\n            self.data_with_label = True\r\n        else:\r\n            self.label_column = \"\"\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.001)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")       \r\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\r\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\r\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"\r\n        assert isinstance(self.use_validation_set, bool), \"use_validation_set must be type(bool), true or false\"\r\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin data_path must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.label_column:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        log.info(f\"check parameter finish.\")\r\n                        \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        Logistic regression training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"start build the model structure.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num])\r\n            Y = tf.placeholder(tf.float64, [None, 1])\r\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n            logits = tf.matmul(X, W) + b\r\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)\r\n            loss = tf.reduce_mean(loss)\r\n            # optimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            \r\n            pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\r\n            reveal_Y = rtt.SecureReveal(pred_Y)\r\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\r\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\r\n            log.info(\"finish build the model structure.\")\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                \r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n     \r\n        log.info(\"remove temp dir.\")\r\n        if self.party_id in (self.data_party + self.result_party):\r\n            # self.remove_temp_dir()\r\n            pass\r\n        else:\r\n            # delete the model in the compute party.\r\n            self.remove_output_dir()\r\n        \r\n        if (self.party_id in self.result_party) and self.use_validation_set:\r\n            log.info(\"result_party evaluate model.\")\r\n            Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n            evaluation_result = self.model_evaluation(Y_true, Y_pred)\r\n        else:\r\n            evaluation_result = \"\"\r\n        log.info(\"train success all.\")\r\n        return evaluation_result\r\n    \r\n    def model_evaluation(self, Y_true, Y_pred):\r\n        \'\'\'\r\n        only support binary class\r\n        \'\'\'\r\n        log.info(\"start model evaluation.\")\r\n        from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\r\n        log.info(\"start evaluate auc score.\")\r\n        auc_score = roc_auc_score(Y_true, Y_pred)\r\n        Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n        log.info(\"start evaluate accuracy score.\")\r\n        accuracy = accuracy_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate f1_score.\")\r\n        f1_score = f1_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate precision score.\")\r\n        precision = precision_score(Y_true, Y_pred_class)\r\n        log.info(\"start evaluate recall score.\")\r\n        recall = recall_score(Y_true, Y_pred_class)\r\n        auc_score = round(auc_score, 6)\r\n        accuracy = round(accuracy, 6)\r\n        f1_score = round(f1_score, 6)\r\n        precision = round(precision, 6)\r\n        recall = round(recall, 6)\r\n        evaluation_result = {\r\n            \"AUC\": auc_score,\r\n            \"accuracy\": accuracy,\r\n            \"f1_score\": f1_score,\r\n            \"precision\": precision,\r\n            \"recall\": recall\r\n        }\r\n        log.info(f\"evaluation_result = {evaluation_result}\")\r\n        evaluation_result = json.dumps(evaluation_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluation_result\r\n    \r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\") # usecols not ensure the order of columns\r\n            input_data = input_data[usecols]  # use for ensure the order of columns\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_class_num = train_y_data.unique().shape[0]\r\n                assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_class_num = val_y_data.unique().shape[0]\r\n                    assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n    \r\n    def remove_output_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        This is used to delete all output files of the non-resulting party\r\n        \'\'\'\r\n        temp_dir = os.path.dirname(self.output_file)\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. logistic regression train.\")\r\n    try:\r\n        privacy_lr = PrivacyLRTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_lr.train()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:lr_train. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. logistic regression train.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:09:37'),(2022,2,'{\"use_psi\":true,\"model_restore_party\":\"model1\",\"predict_threshold\":0.5}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyLRPredict(object):\r\n    \'\'\'\r\n    Privacy logistic regression predict base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"model_restore_party\": \"model1\",\r\n                \"predict_threshold\": 0.5\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(results_dir, \"result\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n        self._check_parameters()\r\n    \r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                elif input_type == 3:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2/3\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\r\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)        \r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")        \r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\r\n        log.info(f\"check parameter finish.\")\r\n       \r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        Logistic regression predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        X = tf.placeholder(tf.float64, [None, column_total_num])\r\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\r\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\r\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n        init = tf.global_variables_initializer()\r\n        # predict\r\n        pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\r\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n        with tf.Session() as sess:\r\n            log.info(\"session init.\")\r\n            sess.run(init)\r\n            log.info(\"start restore model.\")\r\n            if self.party_id == self.model_restore_party:\r\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                    log.info(f\"model restore from: {self.model_file}.\")\r\n                    saver.restore(sess, self.model_file)\r\n                else:\r\n                    raise Exception(\"model not found or model damaged\")\r\n            else:\r\n                log.info(\"restore model...\")\r\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                with open(temp_file, \"w\") as f:\r\n                    pass\r\n                saver.restore(sess, temp_file)\r\n            log.info(\"finish restore model.\")\r\n            \r\n            # predict\r\n            log.info(\"predict start.\")\r\n            predict_start_time = time.time()\r\n            Y_pred_prob = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n            log.debug(f\"Y_pred_prob:\\n {Y_pred_prob[:10]}\")\r\n            predict_use_time = round(time.time() - predict_start_time, 3)\r\n            log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\r\n            Y_pred_prob = Y_pred_prob.astype(\"float\")\r\n            Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\r\n            Y_class = (Y_pred_prob > self.predict_threshold) * 1\r\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\r\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n\r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n        \r\n        return file_x, id_col\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. logistic regression predict.\")\r\n    try:\r\n        privacy_lr = PrivacyLRPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_lr.predict()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:lr_predict. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. logistic regression predict.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:10:20'),(2031,2,'{\"use_psi\":true,\"label_owner\":\"p1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.1,\"layer_units\":[32,1],#hiddenlayerandoutputlayerunits\"layer_activation\":[\"sigmoid\",\"sigmoid\"],#hiddenlayerandoutputlayeractivation\"init_method\":\"random_normal\",#weightandbiasinitmethod\"use_intercept\":true,#whetherusebias\"optimizer\":\"sgd\",\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport copy\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyDnnTrain(object):\r\n    \'\'\'\r\n    Privacy DNN train base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"label_owner\": \"p1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.1,\r\n                    \"layer_units\": [32, 1],     # hidden layer and output layer units\r\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\"],   # hidden layer and output layer activation\r\n                    \"init_method\": \"random_normal\",   # weight and bias init method\r\n                    \"use_intercept\": true,     # whether use bias\r\n                    \"optimizer\": \"sgd\",\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(results_dir, \"model\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        if self.party_id == self.label_owner:\r\n            self.label_column = dynamic_parameter.get(\"label_column\")\r\n            self.data_with_label = True\r\n        else:\r\n            self.label_column = \"\"\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 50)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.1)\r\n        self.layer_units = hyperparams.get(\"layer_units\", [32, 1])\r\n        self.layer_activation = hyperparams.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\"])\r\n        self.init_method = hyperparams.get(\"init_method\", \"random_normal\")  # \'random_normal\', \'random_uniform\', \'zeros\', \'ones\'\r\n        self.use_intercept = hyperparams.get(\"use_intercept\", True)  # True: use bias, False: not use bias\r\n        self.optimizer = hyperparams.get(\"optimizer\", \"sgd\")\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")        \r\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\r\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\r\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"\r\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\r\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\r\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\r\n        for i in self.layer_units:\r\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\r\n        for i in self.layer_activation:\r\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\r\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\r\n        if self.layer_activation[-1] == \'sigmoid\':\r\n            if self.layer_units[-1] != 1:\r\n                raise Exception(f\"when output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\r\n        \r\n        assert isinstance(self.init_method, str), \"init_method must be type(str)\"\r\n        if self.init_method == \'random_normal\':\r\n            self.init_method = tf.random_normal\r\n        elif self.init_method == \'random_uniform\':\r\n            self.init_method = tf.random_uniform\r\n        elif self.init_method == \'zeros\':  # if len(self.layer_units) != 1, init_method not use zeros, because it can not work well.\r\n            self.init_method = tf.zeros\r\n        elif self.init_method == \'ones\':\r\n            self.init_method = tf.ones\r\n        else:\r\n            raise Exception(f\"init_method only can be random_normal/random_uniform/zeros/ones, not {self.init_method}\")\r\n        assert isinstance(self.optimizer, str), \"optimizer must be type(str)\"\r\n        if self.optimizer == \'sgd\':\r\n            self.optimizer = tf.train.GradientDescentOptimizer\r\n        else:\r\n            raise Exception(f\"optimizer only can be sgd, not {self.optimizer}\")\r\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"\r\n        assert isinstance(self.use_validation_set, bool), \"use_validation_set must be type(bool), true or false\"\r\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.label_column:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        log.info(f\"check parameter finish.\")\r\n                        \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, \r\n                                              label_owner=self.label_owner)\\\r\n                                .load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, \r\n                                                          label_owner=self.label_owner)\\\r\n                                            .load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\r\n            Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'Y\')\r\n            val_Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'val_Y\')\r\n                        \r\n            output = self.dnn(X, column_total_num)\r\n            \r\n            output_layer_activation = self.layer_activation[-1]\r\n            with tf.name_scope(\'output\'):\r\n                if not output_layer_activation:\r\n                    pred_Y = output\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    pred_Y = tf.sigmoid(output)\r\n                elif output_layer_activation == \'relu\':\r\n                    pred_Y = tf.nn.relu(output)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            with tf.name_scope(\'loss\'):\r\n                if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n                    loss = tf.square(Y - pred_Y)\r\n                    loss = tf.reduce_mean(loss)\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\r\n                    loss = tf.reduce_mean(loss)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            \r\n            # optimizer\r\n            with tf.name_scope(\'optimizer\'):\r\n                optimizer = self.optimizer(self.learning_rate).minimize(loss)\r\n            init = tf.global_variables_initializer()\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'saver\')\r\n                        \r\n            reveal_loss = rtt.SecureReveal(loss) # only reveal to the result party\r\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to the result party\r\n            reveal_val_Y = rtt.SecureReveal(val_Y) # only reveal to the result party\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                summary_writer = tf.summary.FileWriter(self.get_temp_dir(), sess.graph)\r\n                # train\r\n                log.info(\"train start.\")\r\n                train_start_time = time.time()\r\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\r\n                loss_history_train, loss_history_val = [], []\r\n                for e in range(self.epochs):\r\n                    for i in range(batch_num):\r\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\r\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\r\n                        if (i % 50 == 0) or (i == batch_num - 1):\r\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\r\n                    # train_loss = sess.run(reveal_loss, feed_dict={X: shard_x, Y: shard_y})\r\n                    # # collect loss\r\n                    # loss_history_train.append(float(train_loss))\r\n                    # if self.use_validation_set:\r\n                    #     val_loss = sess.run(reveal_loss, feed_dict={X: shard_x_val, Y: shard_y_val})\r\n                    #     loss_history_val.append(float(val_loss))\r\n                log.info(f\"model save to: {self.output_file}\")\r\n                saver.save(sess, self.output_file)\r\n                train_use_time = round(time.time()-train_start_time, 3)\r\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\r\n                if self.use_validation_set:\r\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\r\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                    Y_actual = sess.run(reveal_val_Y, feed_dict={val_Y: shard_y_val})\r\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n        \r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n     \r\n        log.info(\"remove temp dir.\")\r\n        if self.party_id in (self.data_party + self.result_party):\r\n            # self.remove_temp_dir()\r\n            pass\r\n        else:\r\n            # delete the model in the compute party.\r\n            self.remove_output_dir()\r\n        \r\n        evaluation_result = \"\"\r\n        if self.party_id in self.result_party:\r\n            log.info(f\"result_party evaluation the model.\")\r\n            if self.use_validation_set:\r\n                log.info(\"result_party evaluate model.\")\r\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\r\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\r\n                evaluation_result = self.model_evaluation(Y_true, Y_pred, output_layer_activation)\r\n            # self.show_train_history(loss_history_train, loss_history_val, self.epochs)\r\n            # log.info(f\"result_party show train history finish.\")\r\n        log.info(\"train success all.\")\r\n        return evaluation_result\r\n    \r\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\r\n        with tf.name_scope(layer_name):\r\n            W = tf.Variable(self.init_method([input_dim, output_dim], dtype=tf.float64), name=\'W\')\r\n            if self.use_intercept:\r\n                b = tf.Variable(self.init_method([output_dim], dtype=tf.float64), name=\'b\')\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W) + b\r\n            else:\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W)\r\n            if not activation:\r\n                one_layer = logits\r\n            elif activation == \'sigmoid\':\r\n                one_layer = tf.sigmoid(logits)\r\n            elif activation == \'relu\':\r\n                one_layer = tf.nn.relu(logits)\r\n            else:\r\n                raise Exception(f\'not support {activation} activation.\')\r\n            return one_layer\r\n    \r\n    def dnn(self, input_X, input_dim):\r\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\r\n        layer_activation.append(\"\")\r\n        for i in range(len(self.layer_units)):\r\n            if i == 0:\r\n                input_units = input_dim\r\n                previous_output = input_X\r\n            else:\r\n                input_units = self.layer_units[i-1]\r\n                previous_output = output\r\n            output = self.layer(previous_output, \r\n                                input_units, \r\n                                self.layer_units[i], \r\n                                layer_activation[i], \r\n                                layer_name=f\"Dense_{i}\")\r\n        return output\r\n    \r\n    def model_evaluation(self, Y_true, Y_pred, output_layer_activation):\r\n        if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\r\n            r2 = r2_score(Y_true, Y_pred)\r\n            rmse = mean_squared_error(Y_true, Y_pred, squared=False)\r\n            mse = mean_squared_error(Y_true, Y_pred, squared=True)\r\n            mae = mean_absolute_error(Y_true, Y_pred)\r\n            r2 = round(r2, 6)\r\n            rmse = round(rmse, 6)\r\n            mse = round(mse, 6)\r\n            mae = round(mae, 6)\r\n            evaluation_result = {\r\n                \"R2-score\": r2,\r\n                \"RMSE\": rmse,\r\n                \"MSE\": mse,\r\n                \"MAE\": mae\r\n            }\r\n        elif output_layer_activation == \'sigmoid\':\r\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\r\n            auc_score = roc_auc_score(Y_true, Y_pred)\r\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n            accuracy = accuracy_score(Y_true, Y_pred_class)\r\n            f1_score = f1_score(Y_true, Y_pred_class)\r\n            precision = precision_score(Y_true, Y_pred_class)\r\n            recall = recall_score(Y_true, Y_pred_class)\r\n            auc_score = round(auc_score, 6)\r\n            accuracy = round(accuracy, 6)\r\n            f1_score = round(f1_score, 6)\r\n            precision = round(precision, 6)\r\n            recall = round(recall, 6)\r\n            evaluation_result = {\r\n                \"AUC\": auc_score,\r\n                \"accuracy\": accuracy,\r\n                \"f1_score\": f1_score,\r\n                \"precision\": precision,\r\n                \"recall\": recall\r\n            }\r\n        else:\r\n            raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n        log.info(f\"evaluation_result = {evaluation_result}\")\r\n        evaluation_result = json.dumps(evaluation_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluation_result\r\n    \r\n    def show_train_history(self, train_history, val_history, epochs, name=\'loss\'):\r\n        log.info(\"start show_train_history\")\r\n        assert all([isinstance(ele, float) for ele in train_history]), \'element of train_history must be float.\'\r\n        import matplotlib.pyplot as plt\r\n        plt.figure()\r\n        y_min = min(train_history)\r\n        y_max = max(train_history)\r\n        y_ticks = np.linspace(y_min, y_max, 10)\r\n        plt.scatter(list(range(1, epochs+1)), train_history, label=\'train\')\r\n        if self.use_validation_set:\r\n            plt.scatter(list(range(1, epochs+1)), val_history, label=\'val\')\r\n        plt.xlabel(\'epochs\') \r\n        plt.ylabel(name)\r\n        plt.yticks(y_ticks)\r\n        plt.title(f\'{name} with epochs\')\r\n        plt.legend()\r\n        figure_path = os.path.join(self.results_dir, f\'{name}.jpg\')\r\n        plt.savefig(figure_path)\r\n        \r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_class_num = train_y_data.unique().shape[0]\r\n                if self.layer_activation[-1] == \'sigmoid\':\r\n                    assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_class_num = val_y_data.unique().shape[0]\r\n                    if self.layer_activation[-1] == \'sigmoid\':\r\n                        assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n    \r\n    def remove_output_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        This is used to delete all output files of the non-resulting party\r\n        \'\'\'\r\n        path = self.results_dir\r\n        if os.path.exists(path):\r\n            shutil.rmtree(path)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. dnn train.\")\r\n    try:\r\n        privacy_dnn = PrivacyDnnTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_dnn.train()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:dnn_train. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. dnn train.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:11:26'),(2032,2,'{\"use_psi\":true,\"model_restore_party\":\"model1\",\"hyperparams\":{\"layer_units\":[32,1],\"layer_activation\":[\"sigmoid\",\"sigmoid\"],\"use_intercept\":true,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport copy\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyDnnPredict(object):\r\n    \'\'\'\r\n    Privacy Dnn predict base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"model_restore_party\": \"model1\",\r\n                \"hyperparams\": {\r\n                    \"layer_units\": [32, 1],\r\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\"],\r\n                    \"use_intercept\": true,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(self.results_dir, \"result\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                elif input_type == 3:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2/3\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\r\n        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.layer_units = hyperparams.get(\"layer_units\", [32, 1])\r\n        self.layer_activation = hyperparams.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\"])\r\n        self.use_intercept = hyperparams.get(\"use_intercept\", True)  # True: use b, False: not use b        \r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\r\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\r\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\r\n        for i in self.layer_units:\r\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\r\n        for i in self.layer_activation:\r\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\r\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\r\n        if self.layer_activation[-1] == \'sigmoid\':\r\n            if self.layer_units[-1] != 1:\r\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\r\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"     \r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\r\n        log.info(f\"check parameter finish.\")\r\n       \r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n\r\n        if self.party_id not in self.data_party:  \r\n            # mean the compute party and result party\r\n            log.info(\"compute start.\")\r\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\r\n            output = self.dnn(X, column_total_num)\r\n            output_layer_activation = self.layer_activation[-1]\r\n            with tf.name_scope(\'output\'):\r\n                if not output_layer_activation:\r\n                    pred_Y = output\r\n                elif output_layer_activation == \'sigmoid\':\r\n                    pred_Y = tf.sigmoid(output)\r\n                elif output_layer_activation == \'relu\':\r\n                    pred_Y = tf.nn.relu(output)\r\n                else:\r\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\r\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\r\n            init = tf.global_variables_initializer()\r\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\r\n\r\n            with tf.Session() as sess:\r\n                log.info(\"session init.\")\r\n                sess.run(init)\r\n                log.info(\"start restore model.\")\r\n                if self.party_id == self.model_restore_party:\r\n                    if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\r\n                        log.info(f\"model restore from: {self.model_file}.\")\r\n                        saver.restore(sess, self.model_file)\r\n                    else:\r\n                        raise Exception(\"model not found or model damaged\")\r\n                else:\r\n                    log.info(\"restore model...\")\r\n                    temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\r\n                    with open(temp_file, \"w\") as f:\r\n                        pass\r\n                    saver.restore(sess, temp_file)\r\n                log.info(\"finish restore model.\")\r\n                \r\n                # predict\r\n                log.info(\"predict start.\")\r\n                predict_start_time = time.time()\r\n                Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\r\n                log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                predict_use_time = round(time.time() - predict_start_time, 3)\r\n                log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\r\n            Y_pred = Y_pred.astype(\"float\")\r\n            if (not output_layer_activation) or (output_layer_activation == \'relu\'):\r\n                Y_result = pd.DataFrame(Y_pred, columns=[\"Y_pred\"])\r\n            elif output_layer_activation == \'sigmoid\':\r\n                Y_prob = pd.DataFrame(Y_pred, columns=[\"Y_prob\"])\r\n                Y_class = (Y_pred > self.predict_threshold) * 1\r\n                Y_class = pd.DataFrame(Y_class, columns=[f\"Y_class(>{self.predict_threshold})\"])\r\n                Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n\r\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\r\n        with tf.name_scope(layer_name):\r\n            W = tf.Variable(tf.random_normal([input_dim, output_dim], dtype=tf.float64), name=\'W\')\r\n            if self.use_intercept:\r\n                b = tf.Variable(tf.random_normal([output_dim], dtype=tf.float64), name=\'b\')\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W) + b\r\n            else:\r\n                with tf.name_scope(\'logits\'):\r\n                    logits = tf.matmul(input_tensor, W)\r\n            if not activation:\r\n                one_layer = logits\r\n            elif activation == \'sigmoid\':\r\n                one_layer = tf.sigmoid(logits)\r\n            elif activation == \'relu\':\r\n                one_layer = tf.nn.relu(logits)\r\n            else:\r\n                raise Exception(f\'not support {activation} activation.\')\r\n            return one_layer\r\n    \r\n    def dnn(self, input_X, input_dim):\r\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\r\n        layer_activation.append(\"\")\r\n        for i in range(len(self.layer_units)):\r\n            if i == 0:\r\n                input_units = input_dim\r\n                previous_output = input_X\r\n            else:\r\n                input_units = self.layer_units[i-1]\r\n                previous_output = output\r\n            output = self.layer(previous_output, \r\n                                input_units, \r\n                                self.layer_units[i], \r\n                                layer_activation[i], \r\n                                layer_name=f\"Dense_{i}\")\r\n        return output\r\n    \r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            \r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n\r\n        return file_x, id_col\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. dnn predict.\")\r\n    try:\r\n        privacy_dnn = PrivacyDnnPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_dnn.predict()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:dnn_predict. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. dnn predict.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:12:06'),(2041,2,'{\"use_psi\":true,\"label_owner\":\"data1\",\"label_column\":\"Y\",\"hyperparams\":{\"epochs\":10,\"batch_size\":256,\"learning_rate\":0.01,\"num_trees\":1,#numoftrees\"max_depth\":3,#maxdepthofpertree\"num_bins\":4,#numofbinsoffeature\"num_class\":2,#numofclassoflabel\"lambd\":1.0,#L2regularcoefficient,[0,+∞)\"gamma\":0.0,#Gamma,alsoknownas\"complexity control\",isanimportantparameterweusetopreventoverfitting\"use_validation_set\":true,\"validation_set_rate\":0.2,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyXgbTrain(object):\r\n    \'\'\'\r\n    Privacy XGBoost train base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"label_owner\": \"data1\",\r\n                \"label_column\": \"Y\",\r\n                \"hyperparams\": {\r\n                    \"epochs\": 10,\r\n                    \"batch_size\": 256,\r\n                    \"learning_rate\": 0.01,\r\n                    \"num_trees\": 1,   # num of trees\r\n                    \"max_depth\": 3,   # max depth of per tree\r\n                    \"num_bins\": 4,    # num of bins of feature\r\n                    \"num_class\": 2,   # num of class of label\r\n                    \"lambd\": 1.0,     # L2 regular coefficient, [0, +∞)\r\n                    \"gamma\": 0.0,     # Gamma, also known as \"complexity control\", is an important parameter we use to prevent over fitting\r\n                    \"use_validation_set\": true,\r\n                    \"validation_set_rate\": 0.2,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.results_dir = results_dir\r\n        self.output_file = os.path.join(self.results_dir, \"model\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2\")\r\n        \r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\r\n        if self.party_id == self.label_owner:\r\n            self.label_column = dynamic_parameter.get(\"label_column\")\r\n            self.data_with_label = True\r\n        else:\r\n            self.label_column = \"\"\r\n            self.data_with_label = False\r\n                        \r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.epochs = hyperparams.get(\"epochs\", 10)\r\n        self.batch_size = hyperparams.get(\"batch_size\", 256)\r\n        self.learning_rate = hyperparams.get(\"learning_rate\", 0.1)\r\n        self.num_trees = hyperparams.get(\"num_trees\", 1)\r\n        self.max_depth = hyperparams.get(\"max_depth\", 3)\r\n        self.num_bins = hyperparams.get(\"num_bins\", 4)\r\n        self.num_class = hyperparams.get(\"num_class\", 2)\r\n        self.lambd = hyperparams.get(\"lambd\", 1.0)\r\n        self.gamma = hyperparams.get(\"gamma\", 0.0)\r\n        self.use_validation_set = hyperparams.get(\"use_validation_set\", True)\r\n        self.validation_set_rate = hyperparams.get(\"validation_set_rate\", 0.2)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")\r\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\r\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\r\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"       \r\n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\r\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\r\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\r\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\r\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\r\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\"\r\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n                if self.label_column:\r\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\r\n                    assert self.label_column not in self.selected_columns, f\"label_column:{self.label_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        log.info(f\"check parameter finish.\")\r\n                        \r\n        \r\n    def train(self):\r\n        \'\'\'\r\n        training algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or label.\")\r\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set save model. save to party: {self.result_party}\")\r\n        rtt.set_saver_model(False, plain_model=self.result_party)\r\n        # sharing data\r\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\r\n        shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx\\\r\n            = rtt.PrivateDatasetEx(data_owner=self.data_party, \r\n                                    label_owner=self.label_owner,\r\n                                    dataset_type=rtt.DatasetType.SampleAligned,\r\n                                    num_classes=self.num_class)\\\r\n                    .load_data(train_x, train_y, header=0)\r\n        log.info(\"finish sharing train data.\")\r\n        column_total_num = shard_x.shape[1]\r\n        log.info(f\"column_total_num = {column_total_num}.\")\r\n        \r\n        if self.use_validation_set:\r\n            log.info(\"start sharing validation data.\")\r\n            shard_x_val, shard_y_val\\\r\n                = rtt.PrivateDataset(data_owner=self.data_party,\r\n                                     label_owner=self.label_owner,\r\n                                    dataset_type=rtt.DatasetType.SampleAligned,\r\n                                    num_classes=self.num_class)\\\r\n                    .load_data(val_x, val_y, header=0)\r\n            log.info(\"finish sharing validation data.\")\r\n\r\n        if self.party_id not in self.data_party:\r\n            log.info(\"start build SecureXGBClassifier.\")\r\n            xgb = rtt.SecureXGBClassifier(epochs=self.epochs,\r\n                                        batch_size=self.batch_size,\r\n                                        learning_rate=self.learning_rate,\r\n                                        max_depth=self.max_depth,\r\n                                        num_trees=self.num_trees,\r\n                                        num_class=self.num_class,\r\n                                        num_bins=self.num_bins,\r\n                                        lambd=self.lambd,\r\n                                        gamma=self.gamma)\r\n            log.info(\"start train XGBoost.\")\r\n            xgb.FitEx(shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx)\r\n            log.info(\"start save model.\")\r\n            xgb.SaveModel(self.output_file)\r\n            log.info(\"save model success.\")    \r\n            if self.use_validation_set:\r\n                # predict Y\r\n                rv_pred = xgb.Reveal(xgb.Predict(shard_x_val), self.result_party)\r\n                y_shape = rv_pred.shape\r\n                log.info(f\"y_shape: {y_shape}, rv_pred: \\n {rv_pred[:10]}\")\r\n                pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\r\n                log.info(f\"pred_y: \\n {pred_y[:10]}\")\r\n                pred_y = np.array(pred_y)\r\n                pred_y.reshape(y_shape)\r\n                Y_pred = np.squeeze(pred_y, 1)\r\n                log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\r\n                # actual Y\r\n                Y_actual = xgb.Reveal(shard_y_val, self.result_party)\r\n                log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\r\n\r\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\r\n            log.info(f\"running stats: {running_stats}\")\r\n        else:\r\n            log.info(\"computing, please waiting for compute finish...\")\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate success.\")\r\n             \r\n        if (self.party_id in self.result_party) and self.use_validation_set:\r\n            log.info(\"result_party evaluate model.\")\r\n            Y_pred = np.squeeze(Y_pred.astype(\"float\"))\r\n            Y_true = np.squeeze(Y_actual.astype(\"float\"))\r\n            evaluation_result = self.model_evaluation(Y_true, Y_pred)\r\n        else:\r\n            evaluation_result = \"\"\r\n        \r\n        log.info(\"remove temp dir.\")\r\n        if self.party_id in (self.data_party + self.result_party):\r\n            # self.remove_temp_dir()\r\n            pass\r\n        else:\r\n            # delete the model in the compute party.\r\n            self.remove_output_dir()\r\n        log.info(\"train success all.\")\r\n        return evaluation_result\r\n    \r\n    def model_evaluation(self, Y_true, Y_pred):\r\n        from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\r\n        if self.num_class == 2:\r\n            average = \'binary\'\r\n            multi_class = \'raise\'\r\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\r\n        else:\r\n            average = \'weighted\'\r\n            multi_class = \'ovr\'\r\n            Y_pred_class = np.argmax(Y_pred, axis=1)\r\n        auc_score = roc_auc_score(Y_true, Y_pred, multi_class=multi_class)\r\n        accuracy = accuracy_score(Y_true, Y_pred_class)\r\n        f1_score = f1_score(Y_true, Y_pred_class, average=average)\r\n        precision = precision_score(Y_true, Y_pred_class, average=average)\r\n        recall = recall_score(Y_true, Y_pred_class, average=average)\r\n        auc_score = round(auc_score, 6)\r\n        accuracy = round(accuracy, 6)\r\n        f1_score = round(f1_score, 6)\r\n        precision = round(precision, 6)\r\n        recall = round(recall, 6)\r\n        evaluation_result = {\r\n            \"AUC\": auc_score,\r\n            \"accuracy\": accuracy,\r\n            \"f1_score\": f1_score,\r\n            \"precision\": precision,\r\n            \"recall\": recall\r\n        }\r\n        log.info(f\"evaluation_result = {evaluation_result}\")\r\n        evaluation_result = json.dumps(evaluation_result)\r\n        log.info(\"evaluation success.\")\r\n        return evaluation_result\r\n    \r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n    \r\n    def extract_feature_or_label(self, with_label: bool=False):\r\n        \'\'\'\r\n        Extract feature columns or label column from input file,\r\n        and then divide them into train set and validation set.\r\n        \'\'\'\r\n        train_x = \"\"\r\n        train_y = \"\"\r\n        val_x = \"\"\r\n        val_y = \"\"\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            if with_label:\r\n                usecols += [self.label_column]\r\n            \r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            # only if self.validation_set_rate==0, split_point==input_data.shape[0]\r\n            split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\r\n            assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\r\n            \r\n            if with_label:\r\n                y_data = input_data[self.label_column]\r\n                train_y_data = y_data.iloc[:split_point]\r\n                train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\r\n                train_y_data.to_csv(train_y, header=True, index=False)\r\n                if self.use_validation_set:\r\n                    assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\r\n                    val_y_data = y_data.iloc[split_point:]\r\n                    val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\r\n                    val_y_data.to_csv(val_y, header=True, index=False)\r\n            \r\n            x_data = input_data[self.selected_columns]\r\n            train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\r\n            x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\r\n            if self.use_validation_set:\r\n                assert split_point < input_data.shape[0], \\\r\n                        f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\r\n                val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\r\n                x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\r\n\r\n        return train_x, train_y, val_x, val_y\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(self.results_dir, \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n    \r\n    def remove_output_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        This is used to delete all output files of the non-resulting party\r\n        \'\'\'\r\n        temp_dir = self.results_dir\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. xgboost train.\")\r\n    try:\r\n        privacy_xgb = PrivacyXgbTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_xgb.train()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:xgboost_train. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. xgboost train.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:13:03'),(2042,2,'{\"use_psi\":true,\"model_restore_party\":\"model1\",\"hyperparams\":{\"num_trees\":1,\"max_depth\":3,\"num_bins\":4,\"num_class\":2,\"lambd\":1.0,\"gamma\":0.0,\"predict_threshold\":0.5}}','# coding:utf-8\r\n\r\nimport os\r\nimport sys\r\nimport math\r\nimport json\r\nimport time\r\nimport logging\r\nimport shutil\r\nimport traceback\r\nimport numpy as np\r\nimport pandas as pd\r\nimport latticex.rosetta as rtt\r\nimport channel_sdk.pyio as io\r\n\r\n\r\nnp.set_printoptions(suppress=True)\r\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\r\nlogger = logging.getLogger(__name__)\r\nclass LogWithStage():\r\n    def __init__(self):\r\n        self.run_stage = \'init log.\'\r\n    \r\n    def info(self, content):\r\n        self.run_stage = content\r\n        logger.info(content)\r\n    \r\n    def debug(self, content):\r\n        logger.debug(content)\r\n\r\nlog = LogWithStage()\r\n\r\nclass PrivacyXgbPredict(object):\r\n    \'\'\'\r\n    Privacy XGBoost predict base on rosetta.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 channel_config: str,\r\n                 cfg_dict: dict,\r\n                 data_party: list,\r\n                 result_party: list,\r\n                 results_dir: str):\r\n        \'\'\'\r\n        cfg_dict:\r\n        {\r\n            \"self_cfg_params\": {\r\n                \"party_id\": \"data1\",\r\n                \"input_data\": [\r\n                    {\r\n                        \"input_type\": 1,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data\",\r\n                        \"key_column\": \"col1\",\r\n                        \"selected_columns\": [\"col2\", \"col3\"]\r\n                    },\r\n                    {\r\n                        \"input_type\": 2,\r\n                        \"data_type\": 1,\r\n                        \"data_path\": \"path/to/data1/psi_result.csv\",\r\n                        \"key_column\": \"\",\r\n                        \"selected_columns\": []\r\n                    }\r\n                ]\r\n            },\r\n            \"algorithm_dynamic_params\": {\r\n                \"use_psi\": true,\r\n                \"model_restore_party\": \"model1\",\r\n                \"hyperparams\": {\r\n                    \"num_trees\": 1,\r\n                    \"max_depth\": 3,\r\n                    \"num_bins\": 4,\r\n                    \"num_class\": 2,\r\n                    \"lambd\": 1.0,\r\n                    \"gamma\": 0.0,\r\n                    \"predict_threshold\": 0.5\r\n                }\r\n            }\r\n\r\n        }\r\n        \'\'\'\r\n        log.info(f\"channel_config:{channel_config}\")\r\n        log.info(f\"cfg_dict:{cfg_dict}\")\r\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\r\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\r\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\r\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\r\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\r\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\r\n        \r\n        log.info(f\"start get input parameter.\")\r\n        self.channel_config = channel_config\r\n        self.data_party = list(data_party)\r\n        self.result_party = list(result_party)\r\n        self.output_file = os.path.join(results_dir, \"result\")\r\n        self._parse_algo_cfg(cfg_dict)\r\n        self.data_party.remove(self.model_restore_party)  # except restore party\r\n        self._check_parameters()\r\n\r\n    def _parse_algo_cfg(self, cfg_dict):\r\n        self.party_id = cfg_dict[\"self_cfg_params\"][\"party_id\"]\r\n        input_data = cfg_dict[\"self_cfg_params\"][\"input_data\"]\r\n        self.psi_result_data = None\r\n        if self.party_id in self.data_party:\r\n            for data in input_data:\r\n                input_type = data[\"input_type\"]\r\n                data_type = data[\"data_type\"]\r\n                if input_type == 1:\r\n                    self.input_file = data[\"data_path\"]\r\n                    self.key_column = data.get(\"key_column\")\r\n                    self.selected_columns = data.get(\"selected_columns\")\r\n                elif input_type == 2:\r\n                    self.psi_result_data = data[\"data_path\"]\r\n                elif input_type == 3:\r\n                    self.model_path = data[\"data_path\"]\r\n                    self.model_file = os.path.join(self.model_path, \"model\")\r\n                else:\r\n                    raise Exception(\"paramter error. input_type only support 1/2/3\")\r\n\r\n        dynamic_parameter = cfg_dict[\"algorithm_dynamic_params\"]\r\n        self.use_psi = dynamic_parameter.get(\"use_psi\", True)\r\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\r\n\r\n        hyperparams = dynamic_parameter[\"hyperparams\"]\r\n        self.num_trees = hyperparams.get(\"num_trees\", 1)\r\n        self.max_depth = hyperparams.get(\"max_depth\", 3)\r\n        self.num_bins = hyperparams.get(\"num_bins\", 4)\r\n        self.num_class = hyperparams.get(\"num_class\", 2)\r\n        self.lambd = hyperparams.get(\"lambd\", 1.0)\r\n        self.gamma = hyperparams.get(\"gamma\", 0.0)\r\n        self.predict_threshold = hyperparams.get(\"predict_threshold\", 0.5)\r\n\r\n    def _check_parameters(self):\r\n        log.info(f\"check parameter start.\")      \r\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\r\n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\r\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\r\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\r\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\r\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\r\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\" \r\n        \r\n        if self.party_id in self.data_party:\r\n            assert isinstance(self.use_psi, bool), \"use_psi must be type(bool), true or false\"\r\n            if self.use_psi:\r\n                assert isinstance(self.psi_result_data, str), f\"psi_result_data must be type(string), not {self.psi_result_data}\"\r\n                self.psi_result_data = self.psi_result_data.strip()\r\n                if os.path.exists(self.psi_result_data):\r\n                    file_suffix = os.path.splitext(self.psi_result_data)[-1][1:]\r\n                    assert file_suffix == \"csv\", f\"psi_result_data must csv file, not {file_suffix}\"\r\n                else:\r\n                    raise Exception(f\"psi_result_data is not exist. psi_result_data={self.psi_result_data}\")\r\n            \r\n            assert isinstance(self.input_file, str), \"origin input_data must be type(string)\"\r\n            assert isinstance(self.key_column, str), \"key_column must be type(string)\"\r\n            assert isinstance(self.selected_columns, list), \"selected_columns must be type(list)\" \r\n            self.input_file = self.input_file.strip()\r\n            if os.path.exists(self.input_file):\r\n                file_suffix = os.path.splitext(self.input_file)[-1][1:]\r\n                assert file_suffix == \"csv\", f\"input_file must csv file, not {file_suffix}\"\r\n                input_columns = pd.read_csv(self.input_file, nrows=0)\r\n                input_columns = list(input_columns.columns)\r\n                assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\r\n                error_col = []\r\n                for col in self.selected_columns:\r\n                    if col not in input_columns:\r\n                        error_col.append(col)   \r\n                assert not error_col, f\"selected_columns:{error_col} not in input_file\"\r\n                assert self.key_column not in self.selected_columns, f\"key_column:{self.key_column} can not in selected_columns\"\r\n            else:\r\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\r\n        if self.party_id == self.model_restore_party:\r\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\r\n            self.model_path = os.path.abspath(self.model_path)\r\n            self.model_file = os.path.join(self.model_path, \"model\")\r\n            assert os.path.exists(self.model_file), f\"model file not found. model_file={self.model_file}\"\r\n        log.info(f\"check parameter finish.\")\r\n       \r\n\r\n    def predict(self):\r\n        \'\'\'\r\n        predict algorithm implementation function\r\n        \'\'\'\r\n\r\n        log.info(\"extract feature or id.\")\r\n        file_x, id_col = self.extract_feature_or_index()\r\n        \r\n        log.info(\"start create and set channel.\")\r\n        self.create_set_channel()\r\n        log.info(\"waiting other party connect...\")\r\n        rtt.activate(\"SecureNN\")\r\n        log.info(\"protocol has been activated.\")\r\n        \r\n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\r\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\r\n        # sharing data\r\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\r\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party,\r\n                                     dataset_type=rtt.DatasetType.SampleAligned,\r\n                                     num_classes=self.num_class)\\\r\n                        .load_X(file_x, header=0)\r\n        log.info(\"finish sharing data .\")\r\n\r\n        xgb = rtt.SecureXGBClassifier(max_depth=self.max_depth, \r\n                                      num_trees=self.num_trees, \r\n                                      num_class=self.num_class, \r\n                                      num_bins=self.num_bins,\r\n                                      lambd=self.lambd,\r\n                                      gamma=self.gamma,\r\n                                      epochs=10,\r\n                                      batch_size=256,\r\n                                      learning_rate=0.01)\r\n        \r\n        log.info(\"start restore model.\")\r\n        if self.party_id == self.model_restore_party:\r\n            log.info(f\"model restore from: {self.model_file}.\")\r\n            xgb.LoadModel(self.model_file)\r\n        else:\r\n            log.info(\"restore model...\")\r\n            xgb.LoadModel(\"\")\r\n        log.info(\"finish restore model.\")\r\n                \r\n        # predict\r\n        predict_start_time = time.time()\r\n        rv_pred = xgb.Reveal(xgb.Predict(shard_x), self.result_party)\r\n        predict_use_time = round(time.time() - predict_start_time, 3)\r\n        log.info(f\"predict finish. predict_use_time={predict_use_time}s\")\r\n        y_shape = rv_pred.shape\r\n        pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\r\n        pred_y = np.array(pred_y)\r\n        pred_y.reshape(y_shape)\r\n        pred_y = np.squeeze(pred_y, 1)\r\n        rtt.deactivate()\r\n        log.info(\"rtt deactivate finish.\")\r\n        \r\n        if self.party_id in self.result_party:\r\n            log.info(\"predict result write to file.\")\r\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\r\n            Y_pred_prob = pred_y.astype(\"float\")\r\n            if self.num_class == 2:\r\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\r\n                Y_class = (Y_pred_prob > self.predict_threshold) * 1\r\n            else:\r\n                columns = [f\"Y_prob_{i}\" for i in range(Y_pred_prob.shape[1])]\r\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=columns)\r\n                Y_class = np.argmax(Y_prob, axis=1)\r\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\r\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\r\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\r\n        log.info(\"start remove temp dir.\")\r\n        self.remove_temp_dir()\r\n        log.info(\"predict success all.\")\r\n\r\n    def create_set_channel(self):\r\n        \'\'\'\r\n        create and set channel.\r\n        \'\'\'\r\n        io_channel = io.APIManager()\r\n        log.info(\"start create channel.\")\r\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\r\n        log.info(\"start set channel.\")\r\n        rtt.set_channel(\"\", channel)\r\n        log.info(\"set channel success.\")\r\n        \r\n    def extract_feature_or_index(self):\r\n        \'\'\'\r\n        Extract feature columns or index column from input file.\r\n        \'\'\'\r\n        file_x = \"\"\r\n        id_col = None\r\n        temp_dir = self.get_temp_dir()\r\n        if self.party_id in self.data_party:\r\n            usecols = [self.key_column] + self.selected_columns\r\n            input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\r\n            input_data = input_data[usecols]\r\n            assert input_data.shape[0] > 0, \'input file is no data.\'\r\n            if self.use_psi:\r\n                psi_result = pd.read_csv(self.psi_result_data, dtype=\"str\")\r\n                psi_result.name = self.key_column\r\n                input_data = pd.merge(psi_result, input_data, on=self.key_column, how=\'inner\')\r\n                assert input_data.shape[0] > 0, \'input data is empty. because no intersection with psi result.\'\r\n            id_col = input_data[self.key_column]\r\n            file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\r\n            x_data = input_data.drop(labels=self.key_column, axis=1)\r\n            x_data.to_csv(file_x, header=True, index=False)\r\n        \r\n        return file_x, id_col\r\n    \r\n    def get_temp_dir(self):\r\n        \'\'\'\r\n        Get the directory for temporarily saving files\r\n        \'\'\'\r\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\r\n        if not os.path.exists(temp_dir):\r\n            os.makedirs(temp_dir, exist_ok=True)\r\n        return temp_dir\r\n\r\n    def remove_temp_dir(self):\r\n        \'\'\'\r\n        Delete all files in the temporary directory, these files are some temporary data.\r\n        Only delete temp file.\r\n        \'\'\'\r\n        temp_dir = self.get_temp_dir()\r\n        if os.path.exists(temp_dir):\r\n            shutil.rmtree(temp_dir)\r\n\r\n\r\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str, **kwargs):\r\n    \'\'\'\r\n    This is the entrance to this module\r\n    \'\'\'\r\n    log.info(\"start main function. xgboost predict.\")\r\n    try:\r\n        privacy_xgb = PrivacyXgbPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\r\n        privacy_xgb.predict()\r\n    except Exception as e:\r\n        et, ev, tb = sys.exc_info()\r\n        error_filename = traceback.extract_tb(tb)[1].filename\r\n        error_filename = os.path.split(error_filename)[1]\r\n        error_lineno = traceback.extract_tb(tb)[1].lineno\r\n        error_function = traceback.extract_tb(tb)[1].name\r\n        error_msg = str(e)\r\n        raise Exception(f\"<ALGO>:xgboost_predict. <RUN_STAGE>:{log.run_stage} <ERROR>: {error_filename},{error_lineno},{error_function},{error_msg}\")\r\n    log.info(\"finish main function. xgboost predict.\")\r\n',NULL,'2022-03-24 04:09:36','2022-04-25 03:14:03');

/*Data for the table `mo_calculation_process` */

insert  into `mo_calculation_process`(`calculation_process_id`,`name`,`name_en`,`status`,`create_time`,`update_time`) values (1,'训练','Train',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(2,'预测','Predict',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(3,'训练，并预测','Train and then predict',1,'2022-03-24 08:17:01','2022-03-24 08:17:01'),(4,'PSI','PSI',1,'2022-03-24 08:17:01','2022-03-24 08:17:01');

/*Data for the table `mo_calculation_process_algorithm` */

insert  into `mo_calculation_process_algorithm`(`calculation_process_id`,`algorithm_id`) values (4,1001),(1,2010),(2,2010),(3,2010),(1,2020),(2,2020),(3,2020),(1,2030),(2,2030),(3,2030),(1,2040),(2,2040),(3,2040);

/*Data for the table `mo_calculation_process_step` */

insert  into `mo_calculation_process_step`(`calculation_process_id`,`step`,`type`,`task_1_step`,`task_2_step`,`task_3_step`,`task_4_step`) values (1,1,0,1,2,NULL,NULL),(1,2,3,1,2,NULL,NULL),(1,3,5,1,2,NULL,NULL),(2,1,1,1,2,NULL,NULL),(2,2,3,1,2,NULL,NULL),(2,3,5,1,2,NULL,NULL),(3,1,0,1,2,NULL,NULL),(3,2,1,3,4,NULL,NULL),(3,3,4,1,2,3,4),(3,4,6,1,2,3,4),(4,1,2,NULL,1,NULL,NULL),(4,2,3,NULL,1,NULL,NULL),(4,3,5,NULL,1,NULL,NULL);

/*Data for the table `mo_calculation_process_task` */

insert  into `mo_calculation_process_task`(`calculation_process_id`,`step`,`algorithm_select`,`input_model_step`,`input_psi_step`) values (1,1,3,NULL,NULL),(1,2,1,NULL,1),(2,1,3,NULL,NULL),(2,2,2,NULL,1),(3,1,3,NULL,NULL),(3,2,1,NULL,1),(3,3,3,NULL,NULL),(3,4,2,2,3),(4,1,0,NULL,NULL);

/*Data for the table `mo_stats_global` */

insert  into `mo_stats_global`(`id`,`task_count`,`address_count_of_task`,`address_count_of_active`,`data_size`,`data_token_count`,`data_token_used`,`total_core`,`total_memory`,`total_bandwidth`) values (1,0,0,0,34947,4,0,0,0,0);
