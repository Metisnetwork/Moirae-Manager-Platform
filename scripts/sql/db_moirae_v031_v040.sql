USE `db_moirae`;

DROP TABLE `dc_meta_data_auth`;
DROP TABLE `t_project`;
DROP TABLE `t_project_member`;
DROP TABLE `t_project_temp`;
DROP TABLE `t_algorithm`;
DROP TABLE `t_algorithm_type`;
DROP TABLE `t_algorithm_auth`;
DROP TABLE `t_algorithm_code`;
DROP TABLE `t_algorithm_variable`;
DROP TABLE `t_algorithm_variable_struct`;


ALTER TABLE `dc_meta_data`
    ADD COLUMN `token_address` VARCHAR(100) NULL   COMMENT 'token合约的地址' AFTER `update_at`;

ALTER TABLE `dc_org`
    ADD COLUMN `opt_address` VARCHAR(100) NULL   COMMENT '组织操作钱包地址' AFTER `update_at`;

ALTER TABLE `dc_meta_data_column`
    DROP COLUMN `create_time`,
    DROP COLUMN `update_time`;

RENAME TABLE `t_user` TO `mo_user`;
ALTER TABLE `mo_user`
    DROP COLUMN `id`,
    CHANGE `address` `address` VARCHAR(64) CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL   COMMENT '用户钱包地址'  FIRST,
    DROP INDEX `UK_ADDRESS`,
    DROP PRIMARY KEY,
    ADD PRIMARY KEY (`address`);

DROP TABLE IF EXISTS `mo_user_login`;
CREATE TABLE `mo_user_login` (
    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '日志表id(自增长)',
    `address` varchar(64) NOT NULL COMMENT '登录地址',
    `status` tinyint(4) NOT NULL DEFAULT '1' COMMENT '登录状态: 0-登录失败, 1-登录成功',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE=InnoDB COMMENT='用户登录日志表';


DROP TABLE IF EXISTS `mo_algorithm_classify`;
CREATE TABLE `mo_algorithm_classify` (
    `id` bigint(20) NOT NULL COMMENT '分类id',
    `parent_id` bigint(20) NOT NULL COMMENT '父分类id，如果为顶级分类，则为0',
    `name` varchar(30)  DEFAULT NULL COMMENT '分类中文名称',
    `name_en` varchar(60)  DEFAULT NULL COMMENT '英文算法名称',
    `is_available` tinyint(4) NOT NULL DEFAULT '1' COMMENT '是否可用: 0-否，1-是',
    `is_algorithm` tinyint(4) NOT NULL DEFAULT '1' COMMENT '是否算法: 0-否，1-是',
    `is_exist_algorithm` tinyint(4) NOT NULL DEFAULT '1' COMMENT '是否存在对应算法: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `UK_ALG_NAME` (`name`)
) ENGINE=InnoDB COMMENT='算法分类表';
INSERT INTO `mo_algorithm_classify` (`id`, `parent_id`, `name`, `name_en`, `is_available`, `is_algorithm`, `is_exist_algorithm` ) VALUES
    (1, 0, '隐私计算', 'Privacy Computing',  1, 0, 0),
    (1000, 1, '隐私统计分析', 'Privacy Statistics',  1, 0, 0),
    (1001, 1000, '隐私求交集（PSI）', 'Private Set Intersection',  1, 1, 1),
    (2000, 1, '隐私AI计算', 'Privacy AI Computing',  1, 0, 0),
    (2010, 2000, '隐私线性回归', 'Private Linear Regression',  1, 1, 0),
    (2011, 2010, '隐私线性回归训练', 'Private Linear Regression Training',  1, 1, 1),
    (2012, 2010, '隐私线性回归预测', 'Private Linear Regression Prediction',  1, 1, 1),
    (2020, 2000, '隐私逻辑回归', 'Private Logistic Regression',  1, 1, 0),
    (2021, 2020, '隐私逻辑回归训练', 'Private Logistic Regression Training',  1, 1, 1),
    (2022, 2020, '隐私逻辑回归预测', 'Private Logistic Regression Prediction',  1, 1, 1),
    (2030, 2000, '隐私DNN（深度神经网络）', 'Private DNN',  1, 1, 0),
    (2031, 2030, '隐私DNN训练', 'Private DNN Training',  1, 1, 1),
    (2032, 2030, '隐私DNN预测', 'Private DNN Prediction',  1, 1, 1),
    (2040, 2000, '隐私XGBoost', 'Private XGBoost',  1, 1, 0),
    (2041, 2040, '隐私XGBoost训练', 'Private XGBoost Training',  1, 1, 1),
    (2042, 2040, '隐私XGBoost预测', 'Private XGBoost Prediction',  1, 1, 1),
    (2, 0, '非隐私计算', 'Non-Privacy Computing',  1, 0, 0);


DROP TABLE IF EXISTS `mo_algorithm`;
CREATE TABLE `mo_algorithm` (
    `algorithm_id` bigint(20) NOT NULL COMMENT '算法表ID',
    `algorithm_desc` varchar(200)  DEFAULT NULL COMMENT '中文算法描述',
    `algorithm_desc_en` varchar(200)  DEFAULT NULL COMMENT '英文算法描述',
    `author` varchar(30)  DEFAULT NULL COMMENT '算法作者',
    `max_numbers` bigint(20) DEFAULT NULL COMMENT '支持协同方最大数量',
    `min_numbers` bigint(20) DEFAULT NULL COMMENT '支持协同方最小数量',
    `support_language` varchar(64)  DEFAULT NULL COMMENT '支持语言,多个以","进行分隔',
    `support_os_system` varchar(64)  DEFAULT NULL COMMENT '支持操作系统,多个以","进行分隔',
    `cost_mem` bigint(20) DEFAULT NULL COMMENT '所需的内存 (单位: byte)',
    `cost_cpu` int(11) DEFAULT NULL COMMENT '所需的核数 (单位: 个)',
    `cost_gpu` int(11) DEFAULT NULL COMMENT 'GPU核数(单位：核)',
    `cost_bandwidth` bigint(20) DEFAULT '0' COMMENT '所需的带宽 (单位: bps)',
    `run_time` bigint(20) NOT NULL DEFAULT '3600000' COMMENT '所需的运行时长,默认1小时 (单位: ms)',
    `input_model` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否需要输入模型: 0-否，1:是',
    `output_model` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否产生模型: 0-否，1:是',
    `store_pattern` tinyint(4) NOT NULL DEFAULT '1' COMMENT '输出存储形式: 1-明文，2:密文',
    `data_rows_flag` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否判断数据行数: 0-否，1-是',
    `data_columns_flag` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否判断数据列数: 0-否，1-是',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB COMMENT='算法表';

insert  into `mo_algorithm`(`algorithm_id`,`algorithm_desc`,`algorithm_desc_en`,`author`,`max_numbers`,`min_numbers`,`support_language`,`support_os_system`,`cost_mem`,`cost_cpu`,`cost_gpu`,`cost_bandwidth`,`run_time`,`input_model`,`output_model`,`store_pattern`,`data_rows_flag`,`data_columns_flag`) values
    (2021,'用于跨组织逻辑回归训练','Used for cross-organization logistic regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,1,0),
    (2022,'用于跨组织逻辑回归预测','Used for cross-organization logistic regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,1,0),
    (1001,'用于跨组织的数据交集查询','Used for cross-organization data intersection query','Rosetta',3,2,'SQL,Python','window,linux,mac',1073741824,1,2,3145728,180000,0,0,1,1,0),
    (2011,'用于跨组织线性回归训练','Used for cross-organization linear regression training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,1,0),
    (2012,'用于跨组织线性回归的预测','Used for cross-organization linear regression prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,1,0),
    (2031,'用于跨组织DNN训练','Used for cross-organization DNN training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,0,1,1,1,0),
    (2032,'用于跨组织DNN预测算法','Used for cross-organization DNN prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,300000,1,0,1,1,0),
    (2041,'用于跨组织XGBoost训练','Used for cross-organization XGBoost training','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,0,1,1,1,0),
    (2042,'用于跨组织XGBoost预测','Used for cross-organization XGBoost prediction','Rosetta',3,2,'Python','window,linux,mac',1073741824,1,2,3145728,180000,1,0,1,1,0);


DROP TABLE IF EXISTS `mo_algorithm_code`;
CREATE TABLE `mo_algorithm_code` (
    `algorithm_id` bigint(20) NOT NULL COMMENT '算法代码表ID',
    `edit_type` tinyint(4) DEFAULT NULL COMMENT '编辑类型:1-sql,2-noteBook',
    `calculate_contract_struct` varchar(1024)  DEFAULT NULL COMMENT '计算合约变量模板json格式结构',
    `calculate_contract_code` text  COMMENT '计算合约',
    `data_split_contract_code` text  COMMENT '数据分片合约',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`)
) ENGINE=InnoDB COMMENT='算法代码表';

insert  into `mo_algorithm_code`(`algorithm_id`,`edit_type`,`calculate_contract_struct`,`calculate_contract_code`,`data_split_contract_code`) values
    (1001,2,NULL,'',NULL),
    (2011,2,'{"label_owner":"p0","label_column":"Y","algorithm_parameter":{"epochs":10,"batch_size":256,"learning_rate":0.1,"use_validation_set":true,"validation_set_rate":0.2,"predict_threshold":0.5}}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLinearRegTrain(object):\n    \'\'\'\n    Privacy linear regression train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.001)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert self.epochs > 0, \"epochs must be greater 0\"\n        assert self.batch_size > 0, \"batch size must be greater 0\"\n        assert self.learning_rate > 0, \"learning rate must be greater 0\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        Linear regression training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num])\n            Y = tf.placeholder(tf.float64, [None, 1])\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n            pred_Y = tf.matmul(X, W) + b\n            loss = tf.square(Y - pred_Y)\n            loss = tf.reduce_mean(loss)\n            # optimizer\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            \n            reveal_Y = rtt.SecureReveal(pred_Y)\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                \n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            from sklearn.metrics import r2_score, mean_squared_error\n            Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n            r2 = r2_score(Y_true, Y_pred)\n            rmse = np.sqrt(mean_squared_error(Y_true, Y_pred))\n            log.info(\"********************\")\n            log.info(f\"R Squared: {round(r2, 6)}\")\n            log.info(f\"RMSE: {round(rmse, 6)}\")\n            log.info(\"********************\")\n        log.info(\"train finish.\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = os.path.dirname(self.output_file)\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_linear_reg = PrivacyLinearRegTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_linear_reg.train()',NULL),
    (2012,2,'{"model_restore_party":"p0","model_path":"file_path","predict_threshold":0.5}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLinearRegPredict(object):\n    \'\'\'\n    Privacy linear regression predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        Linear regression predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        X = tf.placeholder(tf.float64, [None, column_total_num])\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n        init = tf.global_variables_initializer()\n        # predict\n        pred_Y = tf.matmul(X, W) + b\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n        with tf.Session() as sess:\n            log.info(\"session init.\")\n            sess.run(init)\n            log.info(\"start restore model.\")\n            if self.party_id == self.model_restore_party:\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                    log.info(f\"model restore from: {self.model_file}.\")\n                    saver.restore(sess, self.model_file)\n                else:\n                    raise Exception(\"model not found or model damaged\")\n            else:\n                log.info(\"restore model...\")\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                with open(temp_file, \"w\") as f:\n                    pass\n                saver.restore(sess, temp_file)\n            log.info(\"finish restore model.\")\n            \n            # predict\n            log.info(\"predict start.\")\n            predict_start_time = time.time()\n            Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\n            log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\n            predict_use_time = round(time.time() - predict_start_time, 3)\n            log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred = Y_pred.astype(\"float\")\n            Y_result = pd.DataFrame(Y_pred, columns=[\"result\"])\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_linear_reg = PrivacyLinearRegPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_linear_reg.predict()\n',NULL),
    (2021,2,'{"label_owner":"p0","label_column":"Y","algorithm_parameter":{"epochs":10,"batch_size":256,"learning_rate":0.1,"use_validation_set":true,"validation_set_rate":0.2,"predict_threshold":0.5}}','# coding:utf-8\n\nimport sys\nsys.path.append(\"..\")\nimport os\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLRTrain(object):\n    \'\'\'\n    Privacy logistic regression train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.001)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert self.epochs > 0, \"epochs must be greater 0\"\n        assert self.batch_size > 0, \"batch size must be greater 0\"\n        assert self.learning_rate > 0, \"learning rate must be greater 0\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        Logistic regression training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, label_owner=self.label_owner).load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num])\n            Y = tf.placeholder(tf.float64, [None, 1])\n            W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n            b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n            logits = tf.matmul(X, W) + b\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)\n            loss = tf.reduce_mean(loss)\n            # optimizer\n            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            \n            pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\n            reveal_Y = rtt.SecureReveal(pred_Y)\n            actual_Y = tf.placeholder(tf.float64, [None, 1])\n            reveal_Y_actual = rtt.SecureReveal(actual_Y)\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                \n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_Y_actual, feed_dict={actual_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n            Y_pred_prob = Y_pred.astype(\"float\").reshape([-1, ])\n            Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n            auc_score = roc_auc_score(Y_true, Y_pred_prob)\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            Y_pred_class = (Y_pred_prob > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n            accuracy = accuracy_score(Y_true, Y_pred_class)\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            f1_score = f1_score(Y_true, Y_pred_class)\n            precision = precision_score(Y_true, Y_pred_class)\n            recall = recall_score(Y_true, Y_pred_class)\n            log.info(\"********************\")\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n            log.info(f\"PRECISION: {round(precision, 6)}\")\n            log.info(f\"RECALL: {round(recall, 6)}\")\n            log.info(\"********************\")\n        log.info(\"train finish.\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    y_data.iloc[:split_point].to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        y_data.iloc[split_point:].to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = os.path.dirname(self.output_file)\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_lr = PrivacyLRTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_lr.train()\n',NULL),
    (2022,2,'{"model_restore_party":"p0","model_path":"file_path","predict_threshold":0.5}','# coding:utf-8\n\nimport sys\nsys.path.append(\"..\")\nimport os\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyLRPredict(object):\n    \'\'\'\n    Privacy logistic regression predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        self.predict_threshold = dynamic_parameter.get(\"predict_threshold\", 0.5)        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        Logistic regression predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        X = tf.placeholder(tf.float64, [None, column_total_num])\n        W = tf.Variable(tf.zeros([column_total_num, 1], dtype=tf.float64))\n        b = tf.Variable(tf.zeros([1], dtype=tf.float64))\n        saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n        init = tf.global_variables_initializer()\n        # predict\n        pred_Y = tf.sigmoid(tf.matmul(X, W) + b)\n        reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n        with tf.Session() as sess:\n            log.info(\"session init.\")\n            sess.run(init)\n            log.info(\"start restore model.\")\n            if self.party_id == self.model_restore_party:\n                if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                    log.info(f\"model restore from: {self.model_file}.\")\n                    saver.restore(sess, self.model_file)\n                else:\n                    raise Exception(\"model not found or model damaged\")\n            else:\n                log.info(\"restore model...\")\n                temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                with open(temp_file, \"w\") as f:\n                    pass\n                saver.restore(sess, temp_file)\n            log.info(\"finish restore model.\")\n            \n            # predict\n            log.info(\"predict start.\")\n            predict_start_time = time.time()\n            Y_pred_prob = sess.run(reveal_Y, feed_dict={X: shard_x})\n            log.debug(f\"Y_pred_prob:\\n {Y_pred_prob[:10]}\")\n            predict_use_time = round(time.time() - predict_start_time, 3)\n            log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred_prob = Y_pred_prob.astype(\"float\")\n            Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\n            Y_class = (Y_pred_prob > self.predict_threshold) * 1\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_lr = PrivacyLRPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_lr.predict()\n',NULL),
    (2031,2,'{"label_owner":"p0","label_column":"Y","algorithm_parameter":{"epochs":5,"batch_size":256,"learning_rate":0.1,"use_validation_set":true,"validation_set_rate":0.2,"predict_threshold":0.5}}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport copy\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyDnnTrain(object):\n    \'\'\'\n    Privacy DNN train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"label_owner\": \"p1\",\n                \"label_column\": \"Y\",\n                \"algorithm_parameter\": {\n                    \"epochs\": 50,\n                    \"batch_size\": 256,\n                    \"learning_rate\": 0.1,\n                    \"layer_units\": [32, 128, 32, 1],\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"],\n                    \"init_method\": \"random_normal\",\n                    \"use_intercept\": true,\n                    \"optimizer\": \"sgd\",\n                    \"use_validation_set\": True,\n                    \"validation_set_rate\": 0.2,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.results_dir = results_dir\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 50)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.1)\n        self.layer_units = algorithm_parameter.get(\"layer_units\", [32, 128, 32, 1])\n        self.layer_activation = algorithm_parameter.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"])\n        self.init_method = algorithm_parameter.get(\"init_method\", \"random_normal\")  # \'random_normal\', \'random_uniform\', \'zeros\', \'ones\'\n        self.use_intercept = algorithm_parameter.get(\"use_intercept\", True)  # True: use b, False: not use b\n        self.optimizer = algorithm_parameter.get(\"optimizer\", \"sgd\")\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.output_file = os.path.join(self.results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")        \n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\n        for i in self.layer_units:\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\n        for i in self.layer_activation:\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\n        if self.layer_activation[-1] == \'sigmoid\':\n            if self.layer_units[-1] != 1:\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\n        \n        assert isinstance(self.init_method, str), \"init_method must be type(str)\"\n        if self.init_method == \'random_normal\':\n            self.init_method = tf.random_normal\n        elif self.init_method == \'random_uniform\':\n            self.init_method = tf.random_uniform\n        elif self.init_method == \'zeros\':  # if len(self.layer_units) != 1, init_method not use zeros, because it can not work well.\n            self.init_method = tf.zeros\n        elif self.init_method == \'ones\':\n            self.init_method = tf.ones\n        else:\n            raise Exception(f\"init_method only can be random_normal/random_uniform/zeros/ones], not {self.init_method}\")\n        assert isinstance(self.optimizer, str), \"optimizer must be type(str)\"\n        if self.optimizer == \'sgd\':\n            self.optimizer = tf.train.GradientDescentOptimizer\n        else:\n            raise Exception(f\"optimizer only can be sgd, not {self.optimizer}\")\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"\n        assert isinstance(self.use_validation_set, bool), \"use_validation_set must be type(bool), true or false\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y = rtt.PrivateDataset(data_owner=self.data_party, \n                                              label_owner=self.label_owner)\\\n                                .load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val = rtt.PrivateDataset(data_owner=self.data_party, \n                                                          label_owner=self.label_owner)\\\n                                            .load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\n            Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'Y\')\n            val_Y = tf.placeholder(tf.float64, [None, self.layer_units[-1]], name=\'val_Y\')\n                        \n            output = self.dnn(X, column_total_num)\n            \n            output_layer_activation = self.layer_activation[-1]\n            with tf.name_scope(\'output\'):\n                if not output_layer_activation:\n                    pred_Y = output\n                elif output_layer_activation == \'sigmoid\':\n                    pred_Y = tf.sigmoid(output)\n                elif output_layer_activation == \'relu\':\n                    pred_Y = tf.nn.relu(output)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            with tf.name_scope(\'loss\'):\n                if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n                    loss = tf.square(Y - pred_Y)\n                    loss = tf.reduce_mean(loss)\n                elif output_layer_activation == \'sigmoid\':\n                    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=output)\n                    loss = tf.reduce_mean(loss)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            \n            # optimizer\n            with tf.name_scope(\'optimizer\'):\n                optimizer = self.optimizer(self.learning_rate).minimize(loss)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'saver\')\n                        \n            reveal_loss = rtt.SecureReveal(loss) # only reveal to the result party\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to the result party\n            reveal_val_Y = rtt.SecureReveal(val_Y) # only reveal to the result party\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                summary_writer = tf.summary.FileWriter(self.get_temp_dir(), sess.graph)\n                # train\n                log.info(\"train start.\")\n                train_start_time = time.time()\n                batch_num = math.ceil(len(shard_x) / self.batch_size)\n                loss_history_train, loss_history_val = [], []\n                for e in range(self.epochs):\n                    for i in range(batch_num):\n                        bX = shard_x[(i * self.batch_size): (i + 1) * self.batch_size]\n                        bY = shard_y[(i * self.batch_size): (i + 1) * self.batch_size]\n                        sess.run(optimizer, feed_dict={X: bX, Y: bY})\n                        if (i % 50 == 0) or (i == batch_num - 1):\n                            log.info(f\"epoch:{e + 1}/{self.epochs}, batch:{i + 1}/{batch_num}\")\n                    train_loss = sess.run(reveal_loss, feed_dict={X: shard_x, Y: shard_y})\n                    # collect loss\n                    loss_history_train.append(float(train_loss))\n                    if self.use_validation_set:\n                        val_loss = sess.run(reveal_loss, feed_dict={X: shard_x_val, Y: shard_y_val})\n                        loss_history_val.append(float(val_loss))\n                log.info(f\"model save to: {self.output_file}\")\n                saver.save(sess, self.output_file)\n                train_use_time = round(time.time()-train_start_time, 3)\n                log.info(f\"save model success. train_use_time={train_use_time}s\")\n                if self.use_validation_set:\n                    Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x_val})\n                    log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                    Y_actual = sess.run(reveal_val_Y, feed_dict={val_Y: shard_y_val})\n                    log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n        \n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n     \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        \n        if self.party_id in self.result_party:\n            log.info(f\"result_party evaluation the model.\")\n            if self.use_validation_set:\n                log.info(\"result_party evaluate model.\")\n                Y_pred = Y_pred.astype(\"float\").reshape([-1, ])\n                Y_true = Y_actual.astype(\"float\").reshape([-1, ])\n                self.model_evaluation(Y_true, Y_pred, output_layer_activation)\n            # self.show_train_history(loss_history_train, loss_history_val, self.epochs)\n            # log.info(f\"result_party show train history finish.\")\n        log.info(\"train finish.\")\n    \n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\n        with tf.name_scope(layer_name):\n            W = tf.Variable(self.init_method([input_dim, output_dim], dtype=tf.float64), name=\'W\')\n            if self.use_intercept:\n                b = tf.Variable(self.init_method([output_dim], dtype=tf.float64), name=\'b\')\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W) + b\n            else:\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W)\n            if not activation:\n                one_layer = logits\n            elif activation == \'sigmoid\':\n                one_layer = tf.sigmoid(logits)\n            elif activation == \'relu\':\n                one_layer = tf.nn.relu(logits)\n            else:\n                raise Exception(f\'not support {activation} activation.\')\n            return one_layer\n    \n    def dnn(self, input_X, input_dim):\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\n        layer_activation.append(\"\")\n        for i in range(len(self.layer_units)):\n            if i == 0:\n                input_units = input_dim\n                previous_output = input_X\n            else:\n                input_units = self.layer_units[i-1]\n                previous_output = output\n            output = self.layer(previous_output, \n                                input_units, \n                                self.layer_units[i], \n                                layer_activation[i], \n                                layer_name=f\"Dense_{i}\")\n        return output\n    \n    def model_evaluation(self, Y_true, Y_pred, output_layer_activation):\n        if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n            r2 = r2_score(Y_true, Y_pred)\n            rmse = mean_squared_error(Y_true, Y_pred, squared=False)\n            mse = mean_squared_error(Y_true, Y_pred, squared=True)\n            mae = mean_absolute_error(Y_true, Y_pred)\n            log.info(\"********************\")\n            log.info(f\"R Squared: {round(r2, 6)}\")\n            log.info(f\"RMSE: {round(rmse, 6)}\")\n            log.info(f\"MSE: {round(mse, 6)}\")\n            log.info(f\"MAE: {round(mae, 6)}\")\n            log.info(\"********************\")\n        elif output_layer_activation == \'sigmoid\':\n            from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n            auc_score = roc_auc_score(Y_true, Y_pred)\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n            accuracy = accuracy_score(Y_true, Y_pred_class)\n            f1_score = f1_score(Y_true, Y_pred_class)\n            precision = precision_score(Y_true, Y_pred_class)\n            recall = recall_score(Y_true, Y_pred_class)\n            log.info(\"********************\")\n            log.info(f\"AUC: {round(auc_score, 6)}\")\n            log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n            log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n            log.info(f\"PRECISION: {round(precision, 6)}\")\n            log.info(f\"RECALL: {round(recall, 6)}\")\n            log.info(\"********************\")\n    \n    def show_train_history(self, train_history, val_history, epochs, name=\'loss\'):\n        log.info(\"start show_train_history\")\n        assert all([isinstance(ele, float) for ele in train_history]), \'element of train_history must be float.\'\n        import matplotlib.pyplot as plt\n        plt.figure()\n        y_min = min(train_history)\n        y_max = max(train_history)\n        y_ticks = np.linspace(y_min, y_max, 10)\n        plt.scatter(list(range(1, epochs+1)), train_history, label=\'train\')\n        if self.use_validation_set:\n            plt.scatter(list(range(1, epochs+1)), val_history, label=\'val\')\n        plt.xlabel(\'epochs\') \n        plt.ylabel(name)\n        plt.yticks(y_ticks)\n        plt.title(f\'{name} with epochs\')\n        plt.legend()\n        figure_path = os.path.join(self.results_dir, f\'{name}.jpg\')\n        plt.savefig(figure_path)\n        \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_class_num = train_y_data.unique().shape[0]\n                    assert train_class_num == 2, f\"train set must be 2 class, not {train_class_num} class.\"\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_class_num = val_y_data.unique().shape[0]\n                        assert val_class_num == 2, f\"validation set must be 2 class, not {val_class_num} class.\"\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        path = self.results_dir\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_dnn = PrivacyDnnTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_dnn.train()\n',NULL),
    (2032,2,'{"model_restore_party":"p0","model_path":"file_path","algorithm_parameter":{"layer_units":[32,128,32,1],"layer_activation":["sigmoid","sigmoid","sigmoid","sigmoid"],"use_intercept":true,"predict_threshold":0.5}}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport copy\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nrtt.set_backend_loglevel(5)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyDnnPredict(object):\n    \'\'\'\n    Privacy Dnn predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"model_restore_party\": \"p3\",\n                \"model_path\": \"/absoulte_path/to/model_dir\",\n                \"algorithm_parameter\": {\n                    \"layer_units\": [32, 128, 32, 1],\n                    \"layer_activation\": [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"],\n                    \"use_intercept\": true,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        self.model_file = os.path.join(self.model_path, \"model\")\n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.layer_units = algorithm_parameter.get(\"layer_units\", [32, 128, 32, 1])\n        self.layer_activation = algorithm_parameter.get(\"layer_activation\", [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"])\n        self.use_intercept = algorithm_parameter.get(\"use_intercept\", True)  # True: use b, False: not use b        \n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.results_dir = results_dir\n        self.output_file = os.path.join(self.results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")\n        assert isinstance(self.layer_units, list) and self.layer_units, \"layer_units must be type(list) and not empty\"\n        assert isinstance(self.layer_activation, list) and self.layer_activation, \"layer_activation must be type(list) and not empty\"\n        assert len(self.layer_units) == len(self.layer_activation), \"the length of layer_units and layer_activation must be the same\"\n        for i in self.layer_units:\n            assert isinstance(i, int) and i > 0, f\'layer_units can only be type(int) and greater 0\'\n        for i in self.layer_activation:\n            if i not in [\"\", \"sigmoid\", \"relu\", None]:\n                raise Exception(f\'layer_activation can only be \"\"/\"sigmoid\"/\"relu\"/None, not {i}\')\n        if self.layer_activation[-1] == \'sigmoid\':\n            if self.layer_units[-1] != 1:\n                raise Exception(f\"output layer activation is sigmoid, output layer units must be 1, not {self.layer_units[-1]}\")\n        assert isinstance(self.use_intercept, bool), \"use_intercept must be type(bool), true or false\"     \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party).load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n\n        if self.party_id not in self.data_party:  \n            # mean the compute party and result party\n            log.info(\"compute start.\")\n            X = tf.placeholder(tf.float64, [None, column_total_num], name=\'X\')\n            output = self.dnn(X, column_total_num)\n            output_layer_activation = self.layer_activation[-1]\n            with tf.name_scope(\'output\'):\n                if not output_layer_activation:\n                    pred_Y = output\n                elif output_layer_activation == \'sigmoid\':\n                    pred_Y = tf.sigmoid(output)\n                elif output_layer_activation == \'relu\':\n                    pred_Y = tf.nn.relu(output)\n                else:\n                    raise Exception(\'output layer not support {output_layer_activation} activation.\')\n            saver = tf.train.Saver(var_list=None, max_to_keep=5, name=\'v2\')\n            init = tf.global_variables_initializer()\n            reveal_Y = rtt.SecureReveal(pred_Y)  # only reveal to result party\n\n            with tf.Session() as sess:\n                log.info(\"session init.\")\n                sess.run(init)\n                log.info(\"start restore model.\")\n                if self.party_id == self.model_restore_party:\n                    if os.path.exists(os.path.join(self.model_path, \"checkpoint\")):\n                        log.info(f\"model restore from: {self.model_file}.\")\n                        saver.restore(sess, self.model_file)\n                    else:\n                        raise Exception(\"model not found or model damaged\")\n                else:\n                    log.info(\"restore model...\")\n                    temp_file = os.path.join(self.get_temp_dir(), \'ckpt_temp_file\')\n                    with open(temp_file, \"w\") as f:\n                        pass\n                    saver.restore(sess, temp_file)\n                log.info(\"finish restore model.\")\n                \n                # predict\n                log.info(\"predict start.\")\n                predict_start_time = time.time()\n                Y_pred = sess.run(reveal_Y, feed_dict={X: shard_x})\n                log.debug(f\"Y_pred:\\n {Y_pred[:10]}\")\n                predict_use_time = round(time.time() - predict_start_time, 3)\n                log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred = Y_pred.astype(\"float\")\n            if (not output_layer_activation) or (output_layer_activation == \'relu\'):\n                Y_result = pd.DataFrame(Y_pred, columns=[\"Y_pred\"])\n            elif output_layer_activation == \'sigmoid\':\n                Y_prob = pd.DataFrame(Y_pred, columns=[\"Y_prob\"])\n                Y_class = (Y_pred > self.predict_threshold) * 1\n                Y_class = pd.DataFrame(Y_class, columns=[f\"Y_class(>{self.predict_threshold})\"])\n                Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def layer(self, input_tensor, input_dim, output_dim, activation, layer_name=\'Dense\'):\n        with tf.name_scope(layer_name):\n            W = tf.Variable(tf.random_normal([input_dim, output_dim], dtype=tf.float64), name=\'W\')\n            if self.use_intercept:\n                b = tf.Variable(tf.random_normal([output_dim], dtype=tf.float64), name=\'b\')\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W) + b\n            else:\n                with tf.name_scope(\'logits\'):\n                    logits = tf.matmul(input_tensor, W)\n            if not activation:\n                one_layer = logits\n            elif activation == \'sigmoid\':\n                one_layer = tf.sigmoid(logits)\n            elif activation == \'relu\':\n                one_layer = tf.nn.relu(logits)\n            else:\n                raise Exception(f\'not support {activation} activation.\')\n            return one_layer\n    \n    def dnn(self, input_X, input_dim):\n        layer_activation = copy.deepcopy(self.layer_activation[:-1])\n        layer_activation.append(\"\")\n        for i in range(len(self.layer_units)):\n            if i == 0:\n                input_units = input_dim\n                previous_output = input_X\n            else:\n                input_units = self.layer_units[i-1]\n                previous_output = output\n            output = self.layer(previous_output, \n                                input_units, \n                                self.layer_units[i], \n                                layer_activation[i], \n                                layer_name=f\"Dense_{i}\")\n        return output\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_dnn = PrivacyDnnPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_dnn.predict()\n',NULL),
    (2041,2,'{"label_owner":"p0","label_column":"Y","algorithm_parameter":{"epochs":10,"batch_size":256,"learning_rate":0.01,"num_trees":3,"max_depth":4,"num_bins":5,"num_class":2,"lambd":1.0,"gamma":0.0,"use_validation_set":true,"validation_set_rate":0.2,"predict_threshold":0.5}}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyXgbTrain(object):\n    \'\'\'\n    Privacy XGBoost train base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"label_owner\": \"p1\",\n                \"label_column\": \"Y\",\n                \"algorithm_parameter\": {\n                    \"epochs\": 10,\n                    \"batch_size\": 256,\n                    \"learning_rate\": 0.01,\n                    \"num_trees\": 3,   # num of trees\n                    \"max_depth\": 4,   # max depth of per tree\n                    \"num_bins\": 5,    # num of bins of feature\n                    \"num_class\": 2,   # num of class of label\n                    \"lambd\": 1.0,     # L2 regular coefficient, [0, +∞)\n                    \"gamma\": 0.0,     # Gamma, also known as \"complexity control\", is an important parameter we use to prevent over fitting\n                    \"use_validation_set\": True,\n                    \"validation_set_rate\": 0.2,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.results_dir = results_dir\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.label_owner = dynamic_parameter.get(\"label_owner\")\n        if self.party_id == self.label_owner:\n            self.label_column = dynamic_parameter.get(\"label_column\")\n            self.data_with_label = True\n        else:\n            self.label_column = \"\"\n            self.data_with_label = False\n                        \n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.epochs = algorithm_parameter.get(\"epochs\", 10)\n        self.batch_size = algorithm_parameter.get(\"batch_size\", 256)\n        self.learning_rate = algorithm_parameter.get(\"learning_rate\", 0.1)\n        self.num_trees = algorithm_parameter.get(\"num_trees\", 3)\n        self.max_depth = algorithm_parameter.get(\"max_depth\", 4)\n        self.num_bins = algorithm_parameter.get(\"num_bins\", 5)\n        self.num_class = algorithm_parameter.get(\"num_class\", 2)\n        self.lambd = algorithm_parameter.get(\"lambd\", 1.0)\n        self.gamma = algorithm_parameter.get(\"gamma\", 0.0)\n        self.use_validation_set = algorithm_parameter.get(\"use_validation_set\", True)\n        self.validation_set_rate = algorithm_parameter.get(\"validation_set_rate\", 0.2)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n\n        self.output_file = os.path.join(self.results_dir, \"model\")\n        \n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")\n        assert isinstance(self.epochs, int) and self.epochs > 0, \"epochs must be type(int) and greater 0\"\n        assert isinstance(self.batch_size, int) and self.batch_size > 0, \"batch_size must be type(int) and greater 0\"\n        assert isinstance(self.learning_rate, float) and self.learning_rate > 0, \"learning rate must be type(float) and greater 0\"       \n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\"\n        assert 0 < self.validation_set_rate < 1, \"validattion set rate must be between (0,1)\"\n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n                if self.label_column:\n                    assert self.label_column in input_columns, f\"label_column:{self.label_column} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        log.info(f\"check parameter finish.\")\n                        \n        \n    def train(self):\n        \'\'\'\n        training algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or label.\")\n        train_x, train_y, val_x, val_y = self.extract_feature_or_label(with_label=self.data_with_label)\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set save model. save to party: {self.result_party}\")\n        rtt.set_saver_model(False, plain_model=self.result_party)\n        # sharing data\n        log.info(f\"start sharing train data. data_owner={self.data_party}, label_owner={self.label_owner}\")\n        shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx\\\n            = rtt.PrivateDatasetEx(data_owner=self.data_party, \n                                    label_owner=self.label_owner,\n                                    dataset_type=rtt.DatasetType.SampleAligned,\n                                    num_classes=self.num_class)\\\n                    .load_data(train_x, train_y, header=0)\n        log.info(\"finish sharing train data.\")\n        column_total_num = shard_x.shape[1]\n        log.info(f\"column_total_num = {column_total_num}.\")\n        \n        if self.use_validation_set:\n            log.info(\"start sharing validation data.\")\n            shard_x_val, shard_y_val\\\n                = rtt.PrivateDataset(data_owner=self.data_party,\n                                     label_owner=self.label_owner,\n                                    dataset_type=rtt.DatasetType.SampleAligned,\n                                    num_classes=self.num_class)\\\n                    .load_data(val_x, val_y, header=0)\n            log.info(\"finish sharing validation data.\")\n\n        if self.party_id not in self.data_party:\n            log.info(\"start build SecureXGBClassifier.\")\n            xgb = rtt.SecureXGBClassifier(epochs=self.epochs,\n                                        batch_size=self.batch_size,\n                                        learning_rate=self.learning_rate,\n                                        max_depth=self.max_depth,\n                                        num_trees=self.num_trees,\n                                        num_class=self.num_class,\n                                        num_bins=self.num_bins,\n                                        lambd=self.lambd,\n                                        gamma=self.gamma)\n            log.info(\"start train XGBoost.\")\n            xgb.FitEx(shard_x, shard_y, x_pmt_idx, x_inv_pmt_idx)\n            log.info(\"start save model.\")\n            xgb.SaveModel(self.output_file)\n            log.info(\"save model success.\")    \n            if self.use_validation_set:\n                # predict Y\n                rv_pred = xgb.Reveal(xgb.Predict(shard_x_val), self.result_party)\n                y_shape = rv_pred.shape\n                log.info(f\"y_shape: {y_shape}, rv_pred: \\n {rv_pred[:10]}\")\n                pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\n                log.info(f\"pred_y: \\n {pred_y[:10]}\")\n                pred_y = np.array(pred_y)\n                pred_y.reshape(y_shape)\n                Y_pred = np.squeeze(pred_y, 1)\n                log.info(f\"Y_pred:\\n {Y_pred[:10]}\")\n                # actual Y\n                Y_actual = xgb.Reveal(shard_y_val, self.result_party)\n                log.info(f\"Y_actual:\\n {Y_actual[:10]}\")\n\n            running_stats = str(rtt.get_perf_stats(True)).replace(\'\\n\', \'\').replace(\' \', \'\')\n            log.info(f\"running stats: {running_stats}\")\n        else:\n            log.info(\"computing, please waiting for compute finish...\")\n        rtt.deactivate()\n        log.info(\"rtt deactivate success.\")\n             \n        if (self.party_id in self.result_party) and self.use_validation_set:\n            log.info(\"result_party evaluate model.\")\n            Y_pred = np.squeeze(Y_pred.astype(\"float\"))\n            Y_true = np.squeeze(Y_actual.astype(\"float\"))\n            self.model_evaluation(Y_true, Y_pred)\n        \n        log.info(\"remove temp dir.\")\n        if self.party_id in (self.data_party + self.result_party):\n            # self.remove_temp_dir()\n            pass\n        else:\n            # delete the model in the compute party.\n            self.remove_output_dir()\n        log.info(\"train success.\")\n    \n    def model_evaluation(self, Y_true, Y_pred):\n        from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n        if self.num_class == 2:\n            average = \'binary\'\n            multi_class = \'raise\'\n            Y_pred_class = (Y_pred > self.predict_threshold).astype(\'int64\')  # default threshold=0.5\n        else:\n            average = \'weighted\'\n            multi_class = \'ovr\'\n            Y_pred_class = np.argmax(Y_pred, axis=1)\n        auc_score = roc_auc_score(Y_true, Y_pred, multi_class=multi_class)\n        accuracy = accuracy_score(Y_true, Y_pred_class)\n        f1_score = f1_score(Y_true, Y_pred_class, average=average)\n        precision = precision_score(Y_true, Y_pred_class, average=average)\n        recall = recall_score(Y_true, Y_pred_class, average=average)\n        log.info(\"********************\")\n        log.info(f\"AUC: {round(auc_score, 6)}\")\n        log.info(f\"ACCURACY: {round(accuracy, 6)}\")\n        log.info(f\"F1_SCORE: {round(f1_score, 6)}\")\n        log.info(f\"PRECISION: {round(precision, 6)}\")\n        log.info(f\"RECALL: {round(recall, 6)}\")\n        log.info(\"********************\")\n    \n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n    \n    def extract_feature_or_label(self, with_label: bool=False):\n        \'\'\'\n        Extract feature columns or label column from input file,\n        and then divide them into train set and validation set.\n        \'\'\'\n        train_x = \"\"\n        train_y = \"\"\n        val_x = \"\"\n        val_y = \"\"\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                if with_label:\n                    usecols = self.selected_columns + [self.label_column]\n                else:\n                    usecols = self.selected_columns\n                \n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                # only if self.validation_set_rate==0, split_point==input_data.shape[0]\n                split_point = int(input_data.shape[0] * (1 - self.validation_set_rate))\n                assert split_point > 0, f\"train set is empty, because validation_set_rate:{self.validation_set_rate} is too big\"\n                \n                if with_label:\n                    y_data = input_data[self.label_column]\n                    train_y_data = y_data.iloc[:split_point]\n                    train_y = os.path.join(temp_dir, f\"train_y_{self.party_id}.csv\")\n                    train_y_data.to_csv(train_y, header=True, index=False)\n                    if self.use_validation_set:\n                        assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small\"\n                        val_y_data = y_data.iloc[split_point:]\n                        val_y = os.path.join(temp_dir, f\"val_y_{self.party_id}.csv\")\n                        val_y_data.to_csv(val_y, header=True, index=False)\n                    del input_data[self.label_column]\n                \n                x_data = input_data\n                train_x = os.path.join(temp_dir, f\"train_x_{self.party_id}.csv\")\n                x_data.iloc[:split_point].to_csv(train_x, header=True, index=False)\n                if self.use_validation_set:\n                    assert split_point < input_data.shape[0], \\\n                            f\"validation set is empty, because validation_set_rate:{self.validation_set_rate} is too small.\"\n                    val_x = os.path.join(temp_dir, f\"val_x_{self.party_id}.csv\")\n                    x_data.iloc[split_point:].to_csv(val_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_node {self.party_id} not have data. input_file:{self.input_file}\")\n        return train_x, train_y, val_x, val_y\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(self.results_dir, \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n    \n    def remove_output_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        This is used to delete all output files of the non-resulting party\n        \'\'\'\n        temp_dir = self.results_dir\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_xgb = PrivacyXgbTrain(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_xgb.train()\n',NULL),
    (2042,2,'{"model_restore_party":"p0","model_path":"file_path","algorithm_parameter":{"num_trees":3,"max_depth":4,"num_bins":5,"num_class":2,"lambd":1.0,"gamma":0.0,"predict_threshold":0.5}}','# coding:utf-8\n\nimport os\nimport sys\nimport math\nimport json\nimport time\nimport logging\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport latticex.rosetta as rtt\nimport channel_sdk\n\n\nnp.set_printoptions(suppress=True)\nrtt.set_backend_loglevel(3)  # All(0), Trace(1), Debug(2), Info(3), Warn(4), Error(5), Fatal(6)\nlog = logging.getLogger(__name__)\n\nclass PrivacyXgbPredict(object):\n    \'\'\'\n    Privacy XGBoost predict base on rosetta.\n    \'\'\'\n\n    def __init__(self,\n                 channel_config: str,\n                 cfg_dict: dict,\n                 data_party: list,\n                 result_party: list,\n                 results_dir: str):\n        \'\'\'\n        cfg_dict:\n        {\n            \"party_id\": \"p1\",\n            \"data_party\": {\n                \"input_file\": \"path/to/file\",\n                \"key_column\": \"col1\",\n                \"selected_columns\": [\"col2\", \"col3\"]\n            },\n            \"dynamic_parameter\": {\n                \"model_restore_party\": \"p3\",\n                \"model_path\": \"/absoulte_path/to/model_dir\",\n                \"algorithm_parameter\": {\n                    \"num_trees\": 3,\n                    \"max_depth\": 4,\n                    \"num_bins\": 5,\n                    \"num_class\": 2,\n                    \"lambd\": 1.0,\n                    \"gamma\": 0.0,\n                    \"predict_threshold\": 0.5\n                }\n            }\n\n        }\n        \'\'\'\n        log.info(f\"channel_config:{channel_config}\")\n        log.info(f\"cfg_dict:{cfg_dict}\")\n        log.info(f\"data_party:{data_party}, result_party:{result_party}, results_dir:{results_dir}\")\n        assert isinstance(channel_config, str), \"type of channel_config must be str\"\n        assert isinstance(cfg_dict, dict), \"type of cfg_dict must be dict\"\n        assert isinstance(data_party, (list, tuple)), \"type of data_party must be list or tuple\"\n        assert isinstance(result_party, (list, tuple)), \"type of result_party must be list or tuple\"\n        assert isinstance(results_dir, str), \"type of results_dir must be str\"\n        \n        self.channel_config = channel_config\n        self.data_party = list(data_party)\n        self.result_party = list(result_party)\n        self.party_id = cfg_dict[\"party_id\"]\n        self.input_file = cfg_dict[\"data_party\"].get(\"input_file\")\n        self.key_column = cfg_dict[\"data_party\"].get(\"key_column\")\n        self.selected_columns = cfg_dict[\"data_party\"].get(\"selected_columns\")\n        dynamic_parameter = cfg_dict[\"dynamic_parameter\"]\n        self.model_restore_party = dynamic_parameter.get(\"model_restore_party\")\n        self.model_path = dynamic_parameter.get(\"model_path\")\n        algorithm_parameter = dynamic_parameter[\"algorithm_parameter\"]\n        self.num_trees = algorithm_parameter.get(\"num_trees\", 3)\n        self.max_depth = algorithm_parameter.get(\"max_depth\", 4)\n        self.num_bins = algorithm_parameter.get(\"num_bins\", 5)\n        self.num_class = algorithm_parameter.get(\"num_class\", 2)\n        self.lambd = algorithm_parameter.get(\"lambd\", 1.0)\n        self.gamma = algorithm_parameter.get(\"gamma\", 0.0)\n        self.predict_threshold = algorithm_parameter.get(\"predict_threshold\", 0.5)\n        self.model_file = os.path.join(self.model_path, \"model\")        \n        self.output_file = os.path.join(results_dir, \"result\")\n        self.data_party.remove(self.model_restore_party)  # except restore party\n        self.check_parameters()\n\n    def check_parameters(self):\n        log.info(f\"check parameter start.\")      \n        assert 0 <= self.predict_threshold <= 1, \"predict threshold must be between [0,1]\"\n        assert isinstance(self.num_trees, int) and self.num_trees > 0, \"num_trees must be type(int) and greater 0\"\n        assert isinstance(self.max_depth, int) and self.max_depth > 0, \"max_depth must be type(int) and greater 0\"\n        assert isinstance(self.num_bins, int) and self.num_bins > 0, \"num_bins must be type(int) and greater 0\"\n        assert isinstance(self.num_class, int) and self.num_class > 1, \"num_class must be type(int) and greater 1\"\n        assert isinstance(self.lambd, (float, int)) and self.lambd >= 0, \"lambd must be type(float/int) and greater_equal 0\"\n        assert isinstance(self.gamma, (float, int)), \"gamma must be type(float/int)\" \n        \n        if self.input_file:\n            self.input_file = self.input_file.strip()\n        if self.party_id in self.data_party:\n            if os.path.exists(self.input_file):\n                input_columns = pd.read_csv(self.input_file, nrows=0)\n                input_columns = list(input_columns.columns)\n                if self.key_column:\n                    assert self.key_column in input_columns, f\"key_column:{self.key_column} not in input_file\"\n                if self.selected_columns:\n                    error_col = []\n                    for col in self.selected_columns:\n                        if col not in input_columns:\n                            error_col.append(col)   \n                    assert not error_col, f\"selected_columns:{error_col} not in input_file\"\n            else:\n                raise Exception(f\"input_file is not exist. input_file={self.input_file}\")\n        if self.party_id == self.model_restore_party:\n            assert os.path.exists(self.model_path), f\"model path not found. model_path={self.model_path}\"\n        log.info(f\"check parameter finish.\")\n       \n\n    def predict(self):\n        \'\'\'\n        predict algorithm implementation function\n        \'\'\'\n\n        log.info(\"extract feature or id.\")\n        file_x, id_col = self.extract_feature_or_index()\n        \n        log.info(\"start create and set channel.\")\n        self.create_set_channel()\n        log.info(\"waiting other party connect...\")\n        rtt.activate(\"SecureNN\")\n        log.info(\"protocol has been activated.\")\n        \n        log.info(f\"start set restore model. restore party={self.model_restore_party}\")\n        rtt.set_restore_model(False, plain_model=self.model_restore_party)\n        # sharing data\n        log.info(f\"start sharing data. data_owner={self.data_party}\")\n        shard_x = rtt.PrivateDataset(data_owner=self.data_party,\n                                     dataset_type=rtt.DatasetType.SampleAligned,\n                                     num_classes=self.num_class)\\\n                        .load_X(file_x, header=0)\n        log.info(\"finish sharing data .\")\n\n        xgb = rtt.SecureXGBClassifier(max_depth=self.max_depth, \n                                      num_trees=self.num_trees, \n                                      num_class=self.num_class, \n                                      num_bins=self.num_bins,\n                                      lambd=self.lambd,\n                                      gamma=self.gamma,\n                                      epochs=10,\n                                      batch_size=256,\n                                      learning_rate=0.01)\n        \n        log.info(\"start restore model.\")\n        if self.party_id == self.model_restore_party:\n            log.info(f\"model restore from: {self.model_file}.\")\n            xgb.LoadModel(self.model_file)\n        else:\n            log.info(\"restore model...\")\n            xgb.LoadModel(\"\")\n        log.info(\"finish restore model.\")\n                \n        # predict\n        predict_start_time = time.time()\n        rv_pred = xgb.Reveal(xgb.Predict(shard_x), self.result_party)\n        predict_use_time = round(time.time() - predict_start_time, 3)\n        log.info(f\"predict success. predict_use_time={predict_use_time}s\")\n        y_shape = rv_pred.shape\n        pred_y = [[float(ii_x) for ii_x in i_x] for i_x in rv_pred]\n        pred_y = np.array(pred_y)\n        pred_y.reshape(y_shape)\n        pred_y = np.squeeze(pred_y, 1)\n        rtt.deactivate()\n        log.info(\"rtt deactivate finish.\")\n        \n        if self.party_id in self.result_party:\n            log.info(\"predict result write to file.\")\n            output_file_predict_prob = os.path.splitext(self.output_file)[0] + \"_predict.csv\"\n            Y_pred_prob = pred_y.astype(\"float\")\n            if self.num_class == 2:\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=[\"Y_prob\"])\n                Y_class = (Y_pred_prob > self.predict_threshold) * 1\n            else:\n                columns = [f\"Y_prob_{i}\" for i in range(Y_pred_prob.shape[1])]\n                Y_prob = pd.DataFrame(Y_pred_prob, columns=columns)\n                Y_class = np.argmax(Y_prob, axis=1)\n            Y_class = pd.DataFrame(Y_class, columns=[\"Y_class\"])\n            Y_result = pd.concat([Y_prob, Y_class], axis=1)\n            Y_result.to_csv(output_file_predict_prob, header=True, index=False)\n        log.info(\"start remove temp dir.\")\n        self.remove_temp_dir()\n        log.info(\"predict finish.\")\n\n    def create_set_channel(self):\n        \'\'\'\n        create and set channel.\n        \'\'\'\n        io_channel = channel_sdk.grpc.APIManager()\n        log.info(\"start create channel\")\n        channel = io_channel.create_channel(self.party_id, self.channel_config)\n        log.info(\"start set channel\")\n        rtt.set_channel(\"\", channel)\n        log.info(\"set channel success.\")\n        \n    def extract_feature_or_index(self):\n        \'\'\'\n        Extract feature columns or index column from input file.\n        \'\'\'\n        file_x = \"\"\n        id_col = None\n        temp_dir = self.get_temp_dir()\n        if self.party_id in self.data_party:\n            if self.input_file:\n                usecols = [self.key_column] + self.selected_columns\n                input_data = pd.read_csv(self.input_file, usecols=usecols, dtype=\"str\")\n                input_data = input_data[usecols]\n                id_col = input_data[self.key_column]\n                file_x = os.path.join(temp_dir, f\"file_x_{self.party_id}.csv\")\n                x_data = input_data.drop(labels=self.key_column, axis=1)\n                x_data.to_csv(file_x, header=True, index=False)\n            else:\n                raise Exception(f\"data_party:{self.party_id} not have data. input_file:{self.input_file}\")\n        return file_x, id_col\n    \n    def get_temp_dir(self):\n        \'\'\'\n        Get the directory for temporarily saving files\n        \'\'\'\n        temp_dir = os.path.join(os.path.dirname(self.output_file), \'temp\')\n        if not os.path.exists(temp_dir):\n            os.makedirs(temp_dir, exist_ok=True)\n        return temp_dir\n\n    def remove_temp_dir(self):\n        \'\'\'\n        Delete all files in the temporary directory, these files are some temporary data.\n        Only delete temp file.\n        \'\'\'\n        temp_dir = self.get_temp_dir()\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n\n\ndef main(channel_config: str, cfg_dict: dict, data_party: list, result_party: list, results_dir: str):\n    \'\'\'\n    This is the entrance to this module\n    \'\'\'\n    privacy_xgb = PrivacyXgbPredict(channel_config, cfg_dict, data_party, result_party, results_dir)\n    privacy_xgb.predict()\n',NULL);



DROP TABLE IF EXISTS `mo_algorithm_variable`;
CREATE TABLE `mo_algorithm_variable` (
    `algorithm_id` bigint(20) NOT NULL COMMENT '算法id',
    `var_key` varchar(128)  NOT NULL COMMENT '变量key',
    `var_type` tinyint(4)  NOT NULL COMMENT '变量类型. 1-boolean, 2-number, 3-string',
    `var_value` varchar(128)  NOT NULL COMMENT '变量默认值',
    `var_desc` varchar(512)  DEFAULT NULL COMMENT '变量描述',
    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`algorithm_id`,`var_key`)
) ENGINE=InnoDB COMMENT='算法变量表';

INSERT INTO `mo_algorithm_variable` (`algorithm_id`, `var_key`, `var_type`, `var_value`, `var_desc`) VALUES
    ( 2011, 'epochs', 2, '10', NULL),
    ( 2011, 'batch_size', 2, '256', NULL),
    ( 2011, 'learning_rate', 2, '0.1', NULL),
    ( 2011, 'use_validation_set', 1, 'true', NULL),
    ( 2011, 'validation_set_rate', 2, '0.2', NULL),
    ( 2011, 'predict_threshold', 2, '0.5', NULL),
    ( 2021, 'epochs', 2, '10', NULL),
    ( 2021, 'batch_size', 2, '256', NULL),
    ( 2021, 'learning_rate', 2, '0.1', NULL),
    ( 2021, 'use_validation_set', 1, 'true', NULL),
    ( 2021, 'validation_set_rate', 2, '0.2', NULL),
    ( 2021, 'predict_threshold', 2, '0.5', NULL),
    ( 2031, 'epochs', 2, '5', NULL),
    ( 2031, 'batch_size', 2, '256', NULL),
    ( 2031, 'learning_rate', 2, '0.1', NULL),
    ( 2031, 'use_validation_set', 1, 'true', NULL),
    ( 2031, 'validation_set_rate', 2, '0.2', NULL),
    ( 2031, 'predict_threshold', 2, '0.5', NULL),



    ( 2032, '', 1, '', NULL),
    ( 2041, '', 1, '', NULL),
    ( 2042, '', 1, '', NULL)

    ;





